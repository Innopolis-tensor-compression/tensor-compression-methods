{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorly as tl\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tensorly.decomposition import tucker, parafac\n",
    "import warnings\n",
    "import gc\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from triton.language import dtype\n",
    "\n",
    "tl.set_backend(\"pytorch\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "f93051c43ce64e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "in_channels = 64\n",
    "out_channels = 128\n",
    "kernel_size = (4, 4)\n",
    "tensor_size = 7\n",
    "number_of_images = 16"
   ],
   "id": "fb2ee567bb7c0d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=device)\n",
    "model_compressed = copy.deepcopy(model)\n",
    "model_replaced = copy.deepcopy(model)"
   ],
   "id": "de46896e4ce1e0a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "testConvTranspose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, dtype=torch.float32).to(device)\n",
    "random_tensor = torch.randn(number_of_images, in_channels, 1, 1, dtype=torch.float32).to(device)"
   ],
   "id": "d4ad1fb262b7d970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# testConvTranspose(random_tensor)"
   ],
   "id": "ecb37bb77c24eb55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def SVD_conv(conv_layer: torch.nn.ConvTranspose2d, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    stride = conv_layer.stride\n",
    "    bias = conv_layer.bias is not None\n",
    "    matrix = conv_layer.weight.squeeze().squeeze()\n",
    "    if rank_CPD is None:\n",
    "        rank_CPD = min(matrix.shape)\n",
    "\n",
    "    core, factors = parafac(matrix, rank_CPD, init=\"random\")\n",
    "    norm = tl.norm(matrix - tl.cp_to_tensor((core, factors))) / tl.norm(matrix)\n",
    "    print(f\"SVD ({in_channels}, {out_channels}, (1, 1)): {norm}\")\n",
    "\n",
    "    factor_CPD_input = factors[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "    factor_CPD_output = factors[0].unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "    conv1 = torch.nn.ConvTranspose2d(in_channels, rank_CPD, 1, stride=stride, dtype=torch.float32, bias=bias)\n",
    "    conv2 = torch.nn.ConvTranspose2d(rank_CPD, out_channels, 1, dtype=torch.float32, bias=bias)\n",
    "    conv1.weight = torch.nn.parameter.Parameter(factor_CPD_input)\n",
    "    conv2.weight = torch.nn.parameter.Parameter(factor_CPD_output)\n",
    "    return torch.nn.Sequential(conv1, conv2), norm"
   ],
   "id": "5a4072a3d02144d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def CPD_conv(conv_layer: torch.nn.ConvTranspose2d, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, rank_CPD)\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    bias = conv_layer.bias is not None\n",
    "    conv_weight = conv_layer.weight.reshape(in_channels, out_channels, kernel_size_x * kernel_size_y)\n",
    "\n",
    "    if rank_CPD is None:\n",
    "        rank_CPD = sorted(conv_weight.size())[0]\n",
    "\n",
    "    core_CPD, factors_CPD = parafac(conv_weight, rank_CPD, verbose=0)\n",
    "    norm = tl.norm(conv_weight - tl.cp_to_tensor((core_CPD, factors_CPD))) / tl.norm(conv_weight)\n",
    "    print(f\"CPD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_CPD_input = factors_CPD[0].unsqueeze(2).unsqueeze(3)\n",
    "    factor_CPD_hidden = factors_CPD[2].permute([1, 0]).unsqueeze(1).reshape(rank_CPD, 1, kernel_size_x, kernel_size_y)\n",
    "    factor_CPD_output = factors_CPD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "    conv1_CPD = torch.nn.ConvTranspose2d(in_channels, rank_CPD, 1, dtype=torch.float32, bias=bias)\n",
    "    conv2_CPD = torch.nn.ConvTranspose2d(rank_CPD, rank_CPD, (kernel_size_x, kernel_size_y), groups=rank_CPD, stride=stride, padding=padding, dilation=dilation, dtype=torch.float32, bias=bias)\n",
    "    conv3_CPD = torch.nn.ConvTranspose2d(rank_CPD, out_channels, 1, dtype=torch.float32, bias=bias)\n",
    "    conv1_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_input)\n",
    "    conv2_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_hidden)\n",
    "    conv3_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_output)\n",
    "\n",
    "    return torch.nn.Sequential(conv1_CPD, conv2_CPD, conv3_CPD), norm"
   ],
   "id": "57f3493dd9ef8b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "CPD = CPD_conv(testConvTranspose)[0].to(device)",
   "id": "ceaaaac71fa9b4a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# CPD(random_tensor)"
   ],
   "id": "7d875606658cd576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def TKD_conv(conv_layer: torch.nn.ConvTranspose2d, rank_TKD: list[int]=None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, min(rank_TKD))\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    bias = conv_layer.bias is not None\n",
    "    conv_weight = conv_layer.weight.reshape(in_channels, out_channels, kernel_size_x * kernel_size_y)\n",
    "\n",
    "    if rank_TKD is None:\n",
    "        rank_TKD = [in_channels, out_channels]\n",
    "    else:\n",
    "        if rank_TKD[0] > in_channels:\n",
    "            rank_TKD = (in_channels, rank_TKD[1], rank_TKD[2])\n",
    "            warnings.warn(\"rank_TKD[0] is bigger then in_channels\")\n",
    "        if rank_TKD[1] > out_channels:\n",
    "            rank_TKD = (rank_TKD[0], out_channels, rank_TKD[2])\n",
    "            warnings.warn(\"rank_TKD[1] is bigger then out_channels\")\n",
    "\n",
    "    core_TKD, factors_TKD = tucker(conv_weight, rank_TKD + [kernel_size_y * kernel_size_x], verbose=0)\n",
    "    norm = tl.norm(conv_weight - tl.tucker_to_tensor((core_TKD, factors_TKD))) / tl.norm(conv_weight)\n",
    "    print(f\"TKD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_TKD_input = factors_TKD[0].unsqueeze(2).unsqueeze(3)\n",
    "    factor_TKD_hidden = torch.tensordot(core_TKD, factors_TKD[2], dims=([2], [1])).reshape(rank_TKD[0], rank_TKD[1], kernel_size_x, kernel_size_y)\n",
    "    factor_TKD_output = factors_TKD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    conv1_TKD = torch.nn.ConvTranspose2d(in_channels, rank_TKD[0], 1, dtype=torch.float32, bias=bias)\n",
    "    conv2_TKD = torch.nn.ConvTranspose2d(rank_TKD[0], rank_TKD[1], (kernel_size_x, kernel_size_y), stride=stride, padding=padding, dilation=dilation, dtype=torch.float32, bias=bias)\n",
    "    conv3_TKD = torch.nn.ConvTranspose2d(rank_TKD[1], out_channels, 1, dtype=torch.float32, bias=bias)\n",
    "    conv1_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_input)\n",
    "    conv2_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_hidden)\n",
    "    conv3_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_output)\n",
    "\n",
    "    return torch.nn.Sequential(conv1_TKD, conv2_TKD, conv3_TKD), norm"
   ],
   "id": "b4ad38d7f3cbf9dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TKD = TKD_conv(testConvTranspose, [32, 64])[0].to(device)",
   "id": "bbd1d738f4b02fe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# TKD(random_tensor)"
   ],
   "id": "dad1ec8c99b22102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def TKDCPD_conv(conv_layer: torch.nn.ConvTranspose2d, rank_TKD: list[int] = None, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, rank_CPD)\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    bias = conv_layer.bias is not None\n",
    "    conv_weight = conv_layer.weight.reshape(in_channels, out_channels, kernel_size_x * kernel_size_y)\n",
    "\n",
    "    if rank_TKD is None:\n",
    "        rank_TKD = [in_channels, out_channels]\n",
    "    else:\n",
    "        if rank_TKD[0] > in_channels:\n",
    "            rank_TKD = (in_channels, rank_TKD[1], rank_TKD[2])\n",
    "            warnings.warn(f\"rank_TKD[0] is bigger then in_channels\\n\\nrank_TKD[0]={rank_TKD[0]}\\nin_channels={in_channels}\")\n",
    "        if rank_TKD[1] > out_channels:\n",
    "            rank_TKD = (rank_TKD[0], out_channels, rank_TKD[2])\n",
    "            warnings.warn(f\"rank_TKD[1] is bigger then out_channels\\n\\nrank_TKD[1]={rank_TKD[1]}\\nout_channels={in_channels}\")\n",
    "\n",
    "    core_TKD, factors_TKD = tucker(conv_weight, rank_TKD + [kernel_size_x * kernel_size_y], verbose=0)\n",
    "    norm = tl.norm(conv_weight - tl.tucker_to_tensor((core_TKD, factors_TKD))) / tl.norm(conv_weight)\n",
    "    print(f\"TKDCPD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_TKD_input = factors_TKD[0].unsqueeze(2).unsqueeze(3)\n",
    "    factor_TKD_hidden = torch.tensordot(core_TKD, factors_TKD[2], dims=([2], [1])).permute([1, 2, 0]).reshape(rank_TKD[0], rank_TKD[1], kernel_size_x, kernel_size_y)\n",
    "    factor_TKD_output = factors_TKD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    conv2_TKD = torch.nn.ConvTranspose2d(rank_TKD[0], rank_TKD[1], (kernel_size_x, kernel_size_y), stride=stride, padding=padding, dilation=dilation, dtype=torch.float32, bias=bias)\n",
    "    conv2_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_hidden)\n",
    "    conv2_TKD = CPD_conv(conv2_TKD, rank_CPD=rank_CPD)\n",
    "    norm = conv2_TKD[1]\n",
    "    conv2_TKD = conv2_TKD[0]\n",
    "\n",
    "    conv1_TKD = torch.nn.ConvTranspose2d(in_channels, rank_TKD[0], 1, dtype=torch.float32, bias=bias)\n",
    "    conv3_TKD = torch.nn.ConvTranspose2d(rank_TKD[1], out_channels, 1, dtype=torch.float32, bias=bias)\n",
    "    conv1_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_input)\n",
    "    conv3_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_output)\n",
    "\n",
    "    return torch.nn.Sequential(conv1_TKD, conv2_TKD, conv3_TKD), norm"
   ],
   "id": "9f4f2b6e2ea34d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TKDCPD = TKDCPD_conv(testConvTranspose)[0].to(device)",
   "id": "ef6d1e0b384a3d1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# TKDCPD(random_tensor)"
   ],
   "id": "d03e52f8512bcd7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compress",
   "id": "8e97069043b79be1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def replace_deconv_layers(module, conv_func):\n",
    "    for name, child in module.named_children():\n",
    "        # If child is a ConvTranspose2d, replace it with a new layer\n",
    "        if isinstance(child, nn.ConvTranspose2d):\n",
    "            weight = child.weight.size()\n",
    "            if weight[1] == 3:\n",
    "                continue\n",
    "            TKD, _ = conv_func(child, [weight[0] // 4, weight[1] // 4, weight[2] * weight[3]])\n",
    "            setattr(module, name, TKD)\n",
    "        else:\n",
    "            # Recursively process nested submodules (for custom blocks)\n",
    "            replace_deconv_layers(child, conv_func)"
   ],
   "id": "ba228b146871c517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "replace_deconv_layers(model_compressed.netG, TKDCPD_conv)",
   "id": "907e54d4d3894a50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Photo 1",
   "id": "dc7dba4344754ec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_images = 64\n",
    "clean_noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(clean_noise)\n",
    "\n",
    "# let's plot these images using torchvision and matplotlib\n",
    "\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "4b209825d234e7ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_images = 64\n",
    "with torch.no_grad():\n",
    "    generated_images = model_compressed.test(clean_noise)\n",
    "\n",
    "# let's plot these images using torchvision and matplotlib\n",
    "\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "426e1d6585d3cf1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TRAIN",
   "id": "21c8aa61dcce2dfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "latent_dim = 120\n",
    "num_samples = 10000\n",
    "batch_size = 64\n",
    "\n",
    "# Generate random noise as inputs\n",
    "def generate_random_noise(batch_size, latent_dim):\n",
    "    return torch.randn(batch_size, latent_dim, 1, 1).to(device)\n"
   ],
   "id": "39178a6541e9feef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the original model (teacher)\n",
    "teacher = model\n",
    "teacher.netG.eval()\n",
    "\n",
    "# Load your modified model (student)\n",
    "student = model_compressed  # Replace with your modified model definition\n",
    "student.netG.train()"
   ],
   "id": "2c05e4af1ebb00a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fn = nn.MSELoss()  # Loss function to match the outputs\n",
    "\n",
    "optimizer = torch.optim.Adam(student.netG.parameters())"
   ],
   "id": "8654e921e54e0c17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in tqdm.tqdm(range(num_samples // batch_size)):\n",
    "        # Generate random noise\n",
    "        noise = generate_random_noise(batch_size, latent_dim)\n",
    "\n",
    "        # Get teacher outputs\n",
    "        with torch.no_grad():  # No gradients needed for the teacher\n",
    "            teacher_outputs = teacher.netG(noise)\n",
    "\n",
    "        # Get student outputs\n",
    "        student_outputs = student.netG(noise)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(student_outputs, teacher_outputs)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ],
   "id": "2330a204b8d3feaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Photo 2",
   "id": "805bc11fe6a4a1f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_compressed = student\n",
    "num_images = 64\n",
    "with torch.no_grad():\n",
    "    generated_images = model_compressed.test(clean_noise)\n",
    "\n",
    "# let's plot these images using torchvision and matplotlib\n",
    "\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "217e1033925dcd17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Replace",
   "id": "636c5bff6069257d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Convert RESNET\n",
    "def replace_model(resnet):\n",
    "    layer_norms = {}\n",
    "    for name, module in resnet.named_modules():\n",
    "        if isinstance(module, torch.nn.ConvTranspose2d):\n",
    "            parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "            attr_name = name.split(\".\")[-1]\n",
    "            print(parent_name, end=\": \")\n",
    "            # Access the parent module\n",
    "            parent_module = resnet\n",
    "            if parent_name:\n",
    "                parent_module = dict(resnet.named_modules())[parent_name]\n",
    "\n",
    "            # Replace the old layer with the new one\n",
    "            weights = module.weight.size()\n",
    "            if weights[1] == 3:\n",
    "                continue\n",
    "            result = nn.Sequential(nn.ConvTranspose2d(weights[0], weights[0] // 2, kernel_size=1, bias=False),\n",
    "                                   nn.ConvTranspose2d(weights[0] // 2, weights[1] // 2, kernel_size=(weights[2], weights[3]), stride=module.stride, padding=module.padding, bias=False),\n",
    "                                   nn.ConvTranspose2d(weights[1] // 2, weights[1], kernel_size=1, bias=False))\n",
    "            setattr(parent_module, attr_name, result)\n",
    "            del result\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return resnet"
   ],
   "id": "90f457d668e2fd3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_replaced_G = replace_model(model_replaced.getNetG())",
   "id": "60dbf90db4207882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_replaced.netG = nn.DataParallel(model_replaced_G)\n",
    "num_images = 64\n",
    "with torch.no_grad():\n",
    "    generated_images = model_replaced.test(clean_noise)\n",
    "\n",
    "# let's plot these images using torchvision and matplotlib\n",
    "\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "22223c04b3357eda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TRAIN",
   "id": "f3f41578c2475fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "latent_dim = 120\n",
    "num_samples = 10000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Generate random noise as inputs\n",
    "def generate_random_noise(batch_size, latent_dim):\n",
    "    return torch.randn(batch_size, latent_dim, 1, 1).to(device)"
   ],
   "id": "331d69a8e9302108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the original model (teacher)\n",
    "teacher = model\n",
    "teacher.netG.eval()\n",
    "\n",
    "# Load your modified model (student)\n",
    "student = model_replaced  # Replace with your modified model definition\n",
    "student.netG.train()"
   ],
   "id": "43eb20f9d421f9df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fn = nn.MSELoss()  # Loss function to match the outputs\n",
    "\n",
    "optimizer = torch.optim.Adam(student.netG.parameters())"
   ],
   "id": "b330c1d8d46280e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for epoch in range(epochs):\n",
    "    for _ in tqdm.tqdm(range(num_samples // batch_size)):\n",
    "        # Generate random noise\n",
    "        noise = generate_random_noise(batch_size, latent_dim)\n",
    "\n",
    "        # Get teacher outputs\n",
    "        with torch.no_grad():  # No gradients needed for the teacher\n",
    "            teacher_outputs = teacher.netG(noise)\n",
    "\n",
    "        # Get student outputs\n",
    "        student_outputs = student.netG(noise)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(student_outputs, teacher_outputs)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ],
   "id": "bc7b87fe04cf8888"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Photo 3",
   "id": "256f85fcd6378277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_replaced = student\n",
    "num_images = 64\n",
    "with torch.no_grad():\n",
    "    generated_images = model_replaced.test(clean_noise)\n",
    "\n",
    "# let's plot these images using torchvision and matplotlib\n",
    "\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "954460d52ecd9beb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_replaced.netG",
   "id": "42407c46f562e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# model_compressed.netG(clean_noise)"
   ],
   "id": "a3b6d5e8452a315c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%timeit -r 10 -n 1000\n",
    "# model.netG(clean_noise)"
   ],
   "id": "1342fd85832959ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TEST",
   "id": "52a86649da0212d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Tensor",
   "id": "de1cbba442c4463e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "random_tensor = torch.randn(100, 50, 25, 10).to(device)",
   "id": "d88f064e8567de1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decomposition",
   "id": "e72b6da81349099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "core, factors = tucker(random_tensor, rank=[100, 50, 25, 10])",
   "id": "ab14c14e12a381ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "factors[0].size()",
   "id": "7514a02ea7517b46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Manual Restore",
   "id": "70d6d39ca2de7f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "factor1 = torch.tensordot(core, factors[0], dims=([0], [1]))\n",
    "print(factor1.shape)\n",
    "factor2 = torch.tensordot(factor1, factors[1], dims=([0], [1]))\n",
    "print(factor2.shape)\n",
    "factor3 = torch.tensordot(factor2, factors[2], dims=([0], [1]))\n",
    "print(factor3.shape)\n",
    "factor4 = torch.tensordot(factor3, factors[3], dims=([0], [1]))\n",
    "print(factor4.shape)"
   ],
   "id": "4655db812e0bef0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare manual restore vs built-it restore",
   "id": "36f113e15bb4eb51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(tl.norm(random_tensor - factor4) / tl.norm(random_tensor))\n",
    "print(tl.norm(random_tensor - tl.tucker_to_tensor((core, factors))) / tl.norm(random_tensor))\n",
    "print(torch.sum(factor4 - tl.tucker_to_tensor((core, factors))))"
   ],
   "id": "559888266a8deba4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test small conv",
   "id": "4e486953d5933ecf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def replace_deconv_layers(module):\n",
    "    for name, child in module.named_children():\n",
    "        # If child is a ConvTranspose2d, replace it with a new layer\n",
    "        if isinstance(child, nn.ConvTranspose2d):\n",
    "            weight = child.weight.size()\n",
    "            if weight[1] == 3:\n",
    "                continue\n",
    "            TKD, _ = TKD_conv(child, [weight[0] // 4, weight[1] // 4, weight[2] * weight[3]])\n",
    "            setattr(module, name, TKD)\n",
    "        else:\n",
    "            # Recursively process nested submodules (for custom blocks)\n",
    "            replace_deconv_layers(child)\n",
    "\n",
    "model_compressed = deepcopy(model.netG)\n",
    "replace_deconv_layers(model_compressed)"
   ],
   "id": "cb90301cb8fea786"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tensor = torch.randn(64, 120, 1, 1).to(device)\n",
    "tensor_decomposed = deepcopy(tensor)"
   ],
   "id": "8acf16291adc6163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tensor = model.netG(tensor)\n",
    "tensor_decomposed = model_compressed(tensor_decomposed)"
   ],
   "id": "3fba4be5bdc12f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.netG",
   "id": "f70a2176242c582c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grid = torchvision.utils.make_grid(tensor.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "aeda8bb8b547ce08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grid = torchvision.utils.make_grid(tensor_decomposed.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "23b3e62e8a4aaf24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.mean(tensor - tensor_decomposed)",
   "id": "bb33d9cd7ac3a6cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.utils.model_compressor import *\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "59365b7c9bf4e54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=False)",
   "id": "bc8160a0226f04bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = deepcopy(gan.netG)",
   "id": "59fe1b50f6f8d24b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "compress_model(model, conv_transpose_compression_method=\"TKD\")",
   "id": "ee790e93e8d4131a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "random_tensor = torch.randn(64, 120, 1, 1)",
   "id": "ce7a5616dd5eda36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "out1 = model(random_tensor)",
   "id": "a4406361f02b00a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "out2 = gan.netG(random_tensor)",
   "id": "ab8bef79447953c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grid = torchvision.utils.make_grid(out1.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "33ced1029570b4a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grid = torchvision.utils.make_grid(out2.clamp(min=-1, max=1), scale_each=False, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ],
   "id": "5d251d9b93e5271c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model",
   "id": "540787ece53df244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gan.netG",
   "id": "b8005f25b9c3126b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b14d8abeccf6a8ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
