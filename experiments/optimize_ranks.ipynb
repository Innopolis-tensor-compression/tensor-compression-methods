{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "415b86a58f1eefa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:17.102824Z",
     "start_time": "2025-06-05T20:02:17.015821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "modules_to_reload = [\n",
    "    \"src.utils.method_loggers\",\n",
    "    \"src.utils.method_runners\",\n",
    "    \"src.utils.metrics_calculators\",\n",
    "    \"src.utils.tensor_handlers\",\n",
    "    \"src.utils.trackers\",\n",
    "    \"src.utils.video_controller\",\n",
    "    \"src.utils.optimal_rank_finders\",\n",
    "]\n",
    "\n",
    "for module in modules_to_reload:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import argrelextrema\n",
    "from functools import partial\n",
    "from typing import Callable\n",
    "from torch.nn import Conv2d, ConvTranspose2d\n",
    "import re\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import tensorly as tl\n",
    "from dotenv import load_dotenv\n",
    "from scipy.optimize import OptimizeResult\n",
    "from scipy.optimize import minimize, differential_evolution, shgo\n",
    "\n",
    "from src.utils.image_controller import download_image, extract_image_frames\n",
    "from src.utils.metrics_calculators import IMetricCalculator\n",
    "from src.utils.optimal_rank_finders import (\n",
    "    find_optimal_rank_tensor_train_by_compression_ratio,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "93fd96fe374eb7ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:17.441376Z",
     "start_time": "2025-06-05T20:02:17.379441Z"
    }
   },
   "cell_type": "code",
   "source": "tensor_train_args = {\"svd\": \"truncated_svd\"}",
   "id": "5fa3f3a3df6b5e1d",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# get tensor",
   "id": "626b5425644480c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:17.570113Z",
     "start_time": "2025-06-05T20:02:17.488648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cache_dir_image = \"../.cache/image\"\n",
    "\n",
    "image_urls = [\n",
    "    \"https://i.pinimg.com/564x/04/b2/68/04b26838bdd5e2ba54d0144558685bae.jpg\",\n",
    "    \"https://cdnstatic.rg.ru/crop620x412/uploads/images/187/94/47/iStock-644032024.jpg\",\n",
    "    \"https://i.sstatic.net/uQggz.png\",\n",
    "]\n",
    "\n",
    "images = {}"
   ],
   "id": "a2d129f57bead309",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:17.654618Z",
     "start_time": "2025-06-05T20:02:17.609696Z"
    }
   },
   "cell_type": "code",
   "source": "image_paths = [download_image(image_url, cache_dir_image) for image_url in image_urls]",
   "id": "edd35419372de37f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изображение уже загружено и закешировано: ../.cache/image/04b26838bdd5e2ba54d0144558685bae.jpg\n",
      "Изображение уже загружено и закешировано: ../.cache/image/iStock-644032024.jpg\n",
      "Изображение уже загружено и закешировано: ../.cache/image/uQggz.png\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:17.817881Z",
     "start_time": "2025-06-05T20:02:17.751585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for image_index, image_path in enumerate(image_paths):\n",
    "    image_frames = extract_image_frames(image_path)\n",
    "\n",
    "    images[f\"image-{image_index}\"] = {\n",
    "        \"image_url\": image_urls[image_index],\n",
    "        \"image_path\": image_path,\n",
    "        \"frames\": image_frames,\n",
    "    }\n",
    "\n",
    "    print(f\"image-{image_index} - {image_frames.shape}\")"
   ],
   "id": "5c010beb08c35ab5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image-0 - (564, 564, 3)\n",
      "image-1 - (412, 620, 3)\n",
      "image-2 - (689, 1195, 3)\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.027557Z",
     "start_time": "2025-06-05T20:02:17.987642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_name = \"image-0\"\n",
    "example_tensor = images[tensor_name][\"frames\"].copy().astype(np.float32)"
   ],
   "id": "66e55dddbf0a887d",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# get tensor - layer of NN",
   "id": "5cff8f20e16d1455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.120979Z",
     "start_time": "2025-06-05T20:02:18.073015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=False)\n",
    "# \n",
    "# def SingleLayer(model):\n",
    "#     for name, child in model.named_children():\n",
    "#         if isinstance(child, Conv2d):\n",
    "#             return child\n",
    "#         elif isinstance(child, ConvTranspose2d):\n",
    "#             return child\n",
    "#         else:\n",
    "#             return SingleLayer(child)\n",
    "# \n",
    "# layer_weights_nn_in_array = SingleLayer(gan.netG).weight.detach().numpy()\n",
    "# size = layer_weights_nn_in_array.shape\n",
    "# layer_weights_nn_in_array = layer_weights_nn_in_array.reshape(size[0], size[1], size[2] * size[3])"
   ],
   "id": "b7681bd8aefb0b6d",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.295200Z",
     "start_time": "2025-06-05T20:02:18.237834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example_tensor = layer_weights_nn_in_array\n",
    "# tensor_name = \"random_reshaped_layer_from_NN\""
   ],
   "id": "899cd32e6a99dc32",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# func for calculate bounds for tensor train factors",
   "id": "51b1f4234acc1853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.378519Z",
     "start_time": "2025-06-05T20:02:18.331019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_tt_bounds(shape: tuple | list) -> list:\n",
    "    \"\"\"\n",
    "    Calculate the bounds for TT-ranks of a tensor based on its shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : tuple[int, ...] | list[int]\n",
    "        The shape of the tensor as a list or tuple of integers.\n",
    "        Each element represents the size of the tensor along a corresponding dimension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[int, int]]\n",
    "        A list of rank bounds for the Tensor Train (TT) decomposition.\n",
    "        Each element is a tuple (r_min, r_max), where:\n",
    "        - r_min is always 1.\n",
    "        - r_max is the upper bound for the TT-rank at the corresponding position.\n",
    "\n",
    "    Examples\n",
    "    -------\n",
    "    >>> calculate_tt_bounds((3, 4, 5))\n",
    "    [(1, 1), (1, 3), (1, 12), (1, 1)]\n",
    "    \"\"\"\n",
    "    d = len(shape)\n",
    "    bounds = [(1, 1)]\n",
    "\n",
    "    for k in range(1, d):\n",
    "        prod_left = 1\n",
    "        for i in range(k):\n",
    "            prod_left *= shape[i]\n",
    "\n",
    "        prod_right = 1\n",
    "        for j in range(k, d):\n",
    "            prod_right *= shape[j]\n",
    "\n",
    "        rk_max = min(prod_left, prod_right)\n",
    "        bounds.append((1, rk_max))\n",
    "\n",
    "    bounds.append((1, 1))\n",
    "    return bounds"
   ],
   "id": "5f8a383ad76fde04",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# func for calculate optimal initial rank of tensor train",
   "id": "f27f6565d8fba675"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.458051Z",
     "start_time": "2025-06-05T20:02:18.415979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Выходит локальный минимум ;c\n",
    "# def calculate_tensor_train_initial_rank(bounds: tuple) -> list[int]:\n",
    "#     return [max(1, round(max_bound / 2)) for min_bound, max_bound in bounds]\n",
    "\n",
    "def calculate_tensor_train_initial_rank(bounds: tuple | list) -> list[int]:\n",
    "    return [min_bound for min_bound, max_bound in bounds]"
   ],
   "id": "968a81a8e244868",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# calculate search area",
   "id": "427f7a98321f8757"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:02:18.547830Z",
     "start_time": "2025-06-05T20:02:18.498528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tt_bounds_example_tensor = calculate_tt_bounds(example_tensor.shape)\n",
    "example_tensor_initial_rank = calculate_tensor_train_initial_rank(tt_bounds_example_tensor)\n",
    "\n",
    "print(\n",
    "    tt_bounds_example_tensor,\n",
    "    example_tensor_initial_rank,\n",
    "    sep='\\n'\n",
    ")"
   ],
   "id": "bf32a2ca0f025122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 564), (1, 3), (1, 1)]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:09:49.649700Z",
     "start_time": "2025-06-05T20:02:18.648235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_compression_ratio_for_graphs = 50.0\n",
    "frobenius_error_coef_for_graphs = 1.0\n",
    "compression_ratio_coef_for_graphs = 10.0\n",
    "\n",
    "rank_ranges = [range(bound[0], bound[1] + 1) for bound in tt_bounds_example_tensor]\n",
    "\n",
    "tqdm_iterable = product(*rank_ranges)\n",
    "tqdm_total = np.prod([len(r) for r in rank_ranges])\n",
    "\n",
    "search_area_example_results = {}\n",
    "\n",
    "with tl.backend_context(\"pytorch\"):\n",
    "    example_tensor_cuda = tl.tensor(example_tensor).to(\"cuda\")\n",
    "\n",
    "    for rank_combination in tqdm(\n",
    "            iterable=tqdm_iterable, total=tqdm_total, desc=\"Processing Ranks\"\n",
    "    ):\n",
    "        test_rank = list(rank_combination)\n",
    "        internal_indices = test_rank[1:-1]\n",
    "\n",
    "        try:\n",
    "            method_result = tl.decomposition.tensor_train(example_tensor_cuda, rank=test_rank, **tensor_train_args)\n",
    "            tt_factors = method_result\n",
    "            reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "\n",
    "            frobenius_error = (\n",
    "                    tl.norm(reconstructed_tensor - example_tensor_cuda) / tl.norm(example_tensor_cuda)\n",
    "            ).item()\n",
    "            compression_ratio = IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(\n",
    "                example_tensor_cuda\n",
    "            )\n",
    "            compression_penalty = (target_compression_ratio_for_graphs / 100 - compression_ratio) ** 2\n",
    "            loss_function_result = (\n",
    "                    frobenius_error_coef_for_graphs * frobenius_error\n",
    "                    + compression_ratio_coef_for_graphs * compression_penalty\n",
    "            )\n",
    "\n",
    "            search_area_example_results[tuple(internal_indices)] = {\n",
    "                \"rank\": test_rank,\n",
    "                \"frobenius_error\": frobenius_error,\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"compression_penalty\": compression_penalty,\n",
    "                \"loss_function_result\": loss_function_result,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            search_area_example_results[tuple(internal_indices)] = {\"rank\": test_rank, \"error\": str(e)}\n",
    "        finally:\n",
    "            torch.cuda.synchronize()\n",
    "            del tt_factors, reconstructed_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    del example_tensor_cuda\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "id": "f339e8ccc35dccaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ranks: 100%|██████████| 1692/1692 [07:30<00:00,  3.75it/s]\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph of search area",
   "id": "7811549fbb28e032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:09:49.909014Z",
     "start_time": "2025-06-05T20:09:49.657607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "internal_indices = np.array(list(search_area_example_results.keys()))\n",
    "metrics = [\"frobenius_error\", \"compression_ratio\", \"compression_penalty\", \"loss_function_result\"]\n",
    "\n",
    "metric_dict = {tuple(idx): search_area_example_results[idx] for idx in search_area_example_results}\n",
    "figs = []\n",
    "\n",
    "for metric in metrics:\n",
    "    z_values = np.array([search_area_example_results[key].get(metric, np.nan) for key in search_area_example_results])\n",
    "    x_indices = internal_indices[:, 0]\n",
    "    y_indices = internal_indices[:, 1]\n",
    "\n",
    "    local_min_points = []\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_indices, y_indices)):\n",
    "        z = z_values[i]\n",
    "        neighbors = [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)]\n",
    "        is_local_min = all(\n",
    "            (neighbor not in metric_dict or metric_dict[neighbor].get(metric, np.inf) >= z)\n",
    "            for neighbor in neighbors\n",
    "        )\n",
    "        if is_local_min:\n",
    "            local_min_points.append((x, y, z))\n",
    "\n",
    "    x_min, y_min, z_min = zip(*local_min_points) if local_min_points else ([], [], [])\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_indices,\n",
    "        y=y_indices,\n",
    "        z=z_values,\n",
    "        mode=\"markers\",\n",
    "        marker={\"size\": 5, \"color\": z_values, \"colorscale\": \"Viridis\", \"opacity\": 0.8},\n",
    "        name=metric\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_min,\n",
    "        y=y_min,\n",
    "        z=z_min,\n",
    "        mode=\"markers+text\",\n",
    "        marker={\"size\": 6, \"color\": \"red\", \"symbol\": \"diamond\"},\n",
    "        text=[f\"min: {val:.6f}\" for val in z_min],\n",
    "        textposition=\"top center\",\n",
    "        name=\"Local Minima\"\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"Search area for example tensor {tensor_name} of {metric.replace('_', ' ').title()}\",\n",
    "        height=800,\n",
    "        scene={\n",
    "            \"xaxis_title\": \"Rank Index 2\",\n",
    "            \"yaxis_title\": \"Rank Index 3\",\n",
    "            \"zaxis_title\": metric.replace(\"_\", \" \").title(),\n",
    "            \"yaxis\": {\"tickmode\": \"array\", \"tickvals\": list(set(y_indices.astype(int)))}\n",
    "        },\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 0},\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "internal_indices = np.array(list(search_area_example_results.keys()))\n",
    "metrics = [\"frobenius_error\", \"compression_ratio\", \"compression_penalty\", \"loss_function_result\"]\n",
    "\n",
    "metric_dict = {tuple(idx): search_area_example_results[idx] for idx in search_area_example_results}\n",
    "\n",
    "figs = []\n",
    "\n",
    "for metric in metrics:\n",
    "    z_values = np.array([search_area_example_results[key].get(metric, np.nan) for key in search_area_example_results])\n",
    "    x_indices = internal_indices[:, 0]\n",
    "    y_indices = internal_indices[:, 1]\n",
    "\n",
    "    local_min_points = []\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_indices, y_indices)):\n",
    "        z = z_values[i]\n",
    "\n",
    "        neighbors = [\n",
    "            (x - 1, y), (x + 1, y),\n",
    "            (x, y - 1), (x, y + 1)\n",
    "        ]\n",
    "\n",
    "        is_local_min = all(\n",
    "            (neighbor not in metric_dict or metric_dict[neighbor].get(metric, np.inf) >= z)\n",
    "            for neighbor in neighbors\n",
    "        )\n",
    "\n",
    "        if is_local_min:\n",
    "            local_min_points.append((x, y, z))\n",
    "\n",
    "    x_min, y_min, z_min = zip(*local_min_points) if local_min_points else ([], [], [])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_indices,\n",
    "        y=y_indices,\n",
    "        z=z_values,\n",
    "        mode=\"markers\",\n",
    "        marker={\"size\": 5, \"color\": z_values, \"colorscale\": \"Viridis\", \"opacity\": 0.8},\n",
    "        name=metric\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_min,\n",
    "        y=y_min,\n",
    "        z=z_min,\n",
    "        mode=\"markers+text\",\n",
    "        marker={\"size\": 6, \"color\": \"red\", \"symbol\": \"diamond\"},\n",
    "        text=[f\"min: {val:.6f}\" for val in z_min],\n",
    "        textposition=\"top center\",\n",
    "        name=\"Local Minima\"\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Search area for example tensor {tensor_name} of {metric.replace('_', ' ').title()}\",\n",
    "        height=800,\n",
    "        scene={\n",
    "            \"xaxis_title\": \"Rank Index 1\",\n",
    "            \"yaxis_title\": \"Rank Index 2\",\n",
    "            \"zaxis_title\": metric.replace(\"_\", \" \").title(),\n",
    "            \"yaxis\": {\"tickmode\": \"array\", \"tickvals\": list(set(y_indices.astype(int)))}\n",
    "        },\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 0},\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    figs.append(fig)\n",
    "\n",
    "html_str = \"\"\n",
    "for fig in figs:\n",
    "    html_str += go.Figure(fig).to_html(full_html=False, include_plotlyjs=False)\n",
    "\n",
    "html_file = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Search area by some metrics</h1>\n",
    "{html_str}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "output_path = f\"../.cache/data_analyze/optimization_algs_for_tensor_train_search_area_{tensor_name}.html\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:  # noqa: PTH123\n",
    "    f.write(html_file)"
   ],
   "id": "85ac1d75df5d5146",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimization algs test",
   "id": "e69117e76cbef0a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## optimize with custom algorithm",
   "id": "23c74370dc54bcbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:45.186372Z",
     "start_time": "2025-06-05T20:09:49.933907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_time = time.perf_counter()\n",
    "best_rank, compression_ratio, frobenius_error, find_rank_logs = find_optimal_rank_tensor_train_by_compression_ratio(\n",
    "    tensor=example_tensor,\n",
    "    target_compression_ratio=50.0,\n",
    "    initial_rank_arg=example_tensor_initial_rank,\n",
    "    tensor_train_args=tensor_train_args,\n",
    "    search_strategy=\"custom\",\n",
    ")\n",
    "elapsed_time = time.perf_counter() - start_time"
   ],
   "id": "f5f43b23be34d57a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal rank search process for TensorTrain:\n",
      "step | rank | compression ratio (%) | frobenius error (%)\n",
      "1 | [1, 2, 1, 1] | 0.236721 % | 22.747736 %\n",
      "2 | [1, 2, 2, 1] | 0.355239 % | 19.237901 %\n",
      "3 | [1, 3, 2, 1] | 0.532544 % | 15.444270 %\n",
      "4 | [1, 4, 2, 1] | 0.709849 % | 12.865447 %\n",
      "5 | [1, 5, 2, 1] | 0.887154 % | 11.241703 %\n",
      "6 | [1, 6, 2, 1] | 1.064459 % | 9.935276 %\n",
      "7 | [1, 7, 2, 1] | 1.241763 % | 9.348312 %\n",
      "8 | [1, 8, 2, 1] | 1.419068 % | 8.872970 %\n",
      "9 | [1, 9, 2, 1] | 1.596373 % | 8.454144 %\n",
      "10 | [1, 10, 2, 1] | 1.773678 % | 8.136910 %\n",
      "11 | [1, 11, 2, 1] | 1.950983 % | 7.804918 %\n",
      "12 | [1, 12, 2, 1] | 2.128288 % | 7.554773 %\n",
      "13 | [1, 13, 2, 1] | 2.305593 % | 7.316292 %\n",
      "14 | [1, 13, 3, 1] | 3.074229 % | 7.092544 %\n",
      "15 | [1, 14, 3, 1] | 3.310636 % | 6.882996 %\n",
      "16 | [1, 15, 3, 1] | 3.547042 % | 6.697510 %\n",
      "17 | [1, 16, 3, 1] | 3.783449 % | 6.526667 %\n",
      "18 | [1, 17, 3, 1] | 4.019856 % | 6.368013 %\n",
      "19 | [1, 18, 3, 1] | 4.256262 % | 6.219484 %\n",
      "20 | [1, 19, 3, 1] | 4.492669 % | 6.086923 %\n",
      "21 | [1, 20, 3, 1] | 4.729075 % | 5.962847 %\n",
      "22 | [1, 21, 3, 1] | 4.965482 % | 5.854254 %\n",
      "23 | [1, 22, 3, 1] | 5.201889 % | 5.757741 %\n",
      "24 | [1, 23, 3, 1] | 5.438295 % | 5.665830 %\n",
      "25 | [1, 24, 3, 1] | 5.674702 % | 5.578881 %\n",
      "26 | [1, 25, 3, 1] | 5.911109 % | 5.498346 %\n",
      "27 | [1, 26, 3, 1] | 6.147515 % | 5.418858 %\n",
      "28 | [1, 27, 3, 1] | 6.383922 % | 5.340550 %\n",
      "29 | [1, 28, 3, 1] | 6.620328 % | 5.263821 %\n",
      "30 | [1, 29, 3, 1] | 6.856735 % | 5.193681 %\n",
      "31 | [1, 30, 3, 1] | 7.093142 % | 5.128356 %\n",
      "32 | [1, 31, 3, 1] | 7.329548 % | 5.066708 %\n",
      "33 | [1, 32, 3, 1] | 7.565955 % | 5.007410 %\n",
      "34 | [1, 33, 3, 1] | 7.802362 % | 4.949088 %\n",
      "35 | [1, 34, 3, 1] | 8.038768 % | 4.892294 %\n",
      "36 | [1, 35, 3, 1] | 8.275175 % | 4.836930 %\n",
      "37 | [1, 36, 3, 1] | 8.511581 % | 4.782590 %\n",
      "38 | [1, 37, 3, 1] | 8.747988 % | 4.729387 %\n",
      "39 | [1, 38, 3, 1] | 8.984395 % | 4.678412 %\n",
      "40 | [1, 39, 3, 1] | 9.220801 % | 4.628931 %\n",
      "41 | [1, 40, 3, 1] | 9.457208 % | 4.581610 %\n",
      "42 | [1, 41, 3, 1] | 9.693615 % | 4.534112 %\n",
      "43 | [1, 42, 3, 1] | 9.930021 % | 4.487386 %\n",
      "44 | [1, 43, 3, 1] | 10.166428 % | 4.440810 %\n",
      "45 | [1, 44, 3, 1] | 10.402834 % | 4.395419 %\n",
      "46 | [1, 45, 3, 1] | 10.639241 % | 4.350758 %\n",
      "47 | [1, 46, 3, 1] | 10.875648 % | 4.307636 %\n",
      "48 | [1, 47, 3, 1] | 11.112054 % | 4.264336 %\n",
      "49 | [1, 48, 3, 1] | 11.348461 % | 4.222283 %\n",
      "50 | [1, 49, 3, 1] | 11.584867 % | 4.180527 %\n",
      "51 | [1, 50, 3, 1] | 11.821274 % | 4.139886 %\n",
      "52 | [1, 51, 3, 1] | 12.057681 % | 4.099829 %\n",
      "53 | [1, 52, 3, 1] | 12.294087 % | 4.059891 %\n",
      "54 | [1, 53, 3, 1] | 12.530494 % | 4.020786 %\n",
      "55 | [1, 54, 3, 1] | 12.766901 % | 3.983251 %\n",
      "56 | [1, 55, 3, 1] | 13.003307 % | 3.945974 %\n",
      "57 | [1, 56, 3, 1] | 13.239714 % | 3.908707 %\n",
      "58 | [1, 57, 3, 1] | 13.476120 % | 3.871329 %\n",
      "59 | [1, 58, 3, 1] | 13.712527 % | 3.835641 %\n",
      "60 | [1, 59, 3, 1] | 13.948934 % | 3.801167 %\n",
      "61 | [1, 60, 3, 1] | 14.185340 % | 3.766710 %\n",
      "62 | [1, 61, 3, 1] | 14.421747 % | 3.732883 %\n",
      "63 | [1, 62, 3, 1] | 14.658154 % | 3.698935 %\n",
      "64 | [1, 63, 3, 1] | 14.894560 % | 3.665354 %\n",
      "65 | [1, 64, 3, 1] | 15.130967 % | 3.632220 %\n",
      "66 | [1, 65, 3, 1] | 15.367373 % | 3.599505 %\n",
      "67 | [1, 66, 3, 1] | 15.603780 % | 3.566965 %\n",
      "68 | [1, 67, 3, 1] | 15.840187 % | 3.534691 %\n",
      "69 | [1, 68, 3, 1] | 16.076593 % | 3.502476 %\n",
      "70 | [1, 69, 3, 1] | 16.313000 % | 3.470368 %\n",
      "71 | [1, 70, 3, 1] | 16.549406 % | 3.438357 %\n",
      "72 | [1, 71, 3, 1] | 16.785813 % | 3.406416 %\n",
      "73 | [1, 72, 3, 1] | 17.022220 % | 3.375202 %\n",
      "74 | [1, 73, 3, 1] | 17.258626 % | 3.344018 %\n",
      "75 | [1, 74, 3, 1] | 17.495033 % | 3.313259 %\n",
      "76 | [1, 75, 3, 1] | 17.731440 % | 3.282785 %\n",
      "77 | [1, 76, 3, 1] | 17.967846 % | 3.252695 %\n",
      "78 | [1, 77, 3, 1] | 18.204253 % | 3.222605 %\n",
      "79 | [1, 78, 3, 1] | 18.440659 % | 3.193513 %\n",
      "80 | [1, 79, 3, 1] | 18.677066 % | 3.164606 %\n",
      "81 | [1, 80, 3, 1] | 18.913473 % | 3.135802 %\n",
      "82 | [1, 81, 3, 1] | 19.149879 % | 3.106886 %\n",
      "83 | [1, 82, 3, 1] | 19.386286 % | 3.077947 %\n",
      "84 | [1, 83, 3, 1] | 19.622693 % | 3.049133 %\n",
      "85 | [1, 84, 3, 1] | 19.859099 % | 3.020445 %\n",
      "86 | [1, 85, 3, 1] | 20.095506 % | 2.992544 %\n",
      "87 | [1, 86, 3, 1] | 20.331912 % | 2.964550 %\n",
      "88 | [1, 87, 3, 1] | 20.568319 % | 2.937503 %\n",
      "89 | [1, 88, 3, 1] | 20.804726 % | 2.910515 %\n",
      "90 | [1, 89, 3, 1] | 21.041132 % | 2.883890 %\n",
      "91 | [1, 90, 3, 1] | 21.277539 % | 2.857371 %\n",
      "92 | [1, 91, 3, 1] | 21.513945 % | 2.831667 %\n",
      "93 | [1, 92, 3, 1] | 21.750352 % | 2.806263 %\n",
      "94 | [1, 93, 3, 1] | 21.986759 % | 2.781211 %\n",
      "95 | [1, 94, 3, 1] | 22.223165 % | 2.756270 %\n",
      "96 | [1, 95, 3, 1] | 22.459572 % | 2.731436 %\n",
      "97 | [1, 96, 3, 1] | 22.695979 % | 2.706743 %\n",
      "98 | [1, 97, 3, 1] | 22.932385 % | 2.681950 %\n",
      "99 | [1, 98, 3, 1] | 23.168792 % | 2.657675 %\n",
      "100 | [1, 99, 3, 1] | 23.405198 % | 2.633336 %\n",
      "101 | [1, 100, 3, 1] | 23.641605 % | 2.609273 %\n",
      "102 | [1, 101, 3, 1] | 23.878012 % | 2.585259 %\n",
      "103 | [1, 102, 3, 1] | 24.114418 % | 2.561382 %\n",
      "104 | [1, 103, 3, 1] | 24.350825 % | 2.537682 %\n",
      "105 | [1, 104, 3, 1] | 24.587232 % | 2.514564 %\n",
      "106 | [1, 105, 3, 1] | 24.823638 % | 2.491569 %\n",
      "107 | [1, 106, 3, 1] | 25.060045 % | 2.468972 %\n",
      "108 | [1, 107, 3, 1] | 25.296451 % | 2.446415 %\n",
      "109 | [1, 108, 3, 1] | 25.532858 % | 2.423976 %\n",
      "110 | [1, 109, 3, 1] | 25.769265 % | 2.401829 %\n",
      "111 | [1, 110, 3, 1] | 26.005671 % | 2.379549 %\n",
      "112 | [1, 111, 3, 1] | 26.242078 % | 2.357666 %\n",
      "113 | [1, 112, 3, 1] | 26.478484 % | 2.336278 %\n",
      "114 | [1, 113, 3, 1] | 26.714891 % | 2.315132 %\n",
      "115 | [1, 114, 3, 1] | 26.951298 % | 2.294185 %\n",
      "116 | [1, 115, 3, 1] | 27.187704 % | 2.273420 %\n",
      "117 | [1, 116, 3, 1] | 27.424111 % | 2.253040 %\n",
      "118 | [1, 117, 3, 1] | 27.660518 % | 2.232634 %\n",
      "119 | [1, 118, 3, 1] | 27.896924 % | 2.212378 %\n",
      "120 | [1, 119, 3, 1] | 28.133331 % | 2.192290 %\n",
      "121 | [1, 120, 3, 1] | 28.369737 % | 2.172234 %\n",
      "122 | [1, 121, 3, 1] | 28.606144 % | 2.152595 %\n",
      "123 | [1, 122, 3, 1] | 28.842551 % | 2.133006 %\n",
      "124 | [1, 123, 3, 1] | 29.078957 % | 2.113667 %\n",
      "125 | [1, 124, 3, 1] | 29.315364 % | 2.094530 %\n",
      "126 | [1, 125, 3, 1] | 29.551771 % | 2.075466 %\n",
      "127 | [1, 126, 3, 1] | 29.788177 % | 2.056860 %\n",
      "128 | [1, 127, 3, 1] | 30.024584 % | 2.038109 %\n",
      "129 | [1, 128, 3, 1] | 30.260990 % | 2.019228 %\n",
      "130 | [1, 129, 3, 1] | 30.497397 % | 2.000598 %\n",
      "131 | [1, 130, 3, 1] | 30.733804 % | 1.982317 %\n",
      "132 | [1, 131, 3, 1] | 30.970210 % | 1.964249 %\n",
      "133 | [1, 132, 3, 1] | 31.206617 % | 1.946238 %\n",
      "134 | [1, 133, 3, 1] | 31.443023 % | 1.928573 %\n",
      "135 | [1, 134, 3, 1] | 31.679430 % | 1.910972 %\n",
      "136 | [1, 135, 3, 1] | 31.915837 % | 1.893623 %\n",
      "137 | [1, 136, 3, 1] | 32.152243 % | 1.876468 %\n",
      "138 | [1, 137, 3, 1] | 32.388650 % | 1.859403 %\n",
      "139 | [1, 138, 3, 1] | 32.625057 % | 1.842381 %\n",
      "140 | [1, 139, 3, 1] | 32.861463 % | 1.825576 %\n",
      "141 | [1, 140, 3, 1] | 33.097870 % | 1.808794 %\n",
      "142 | [1, 141, 3, 1] | 33.334276 % | 1.792313 %\n",
      "143 | [1, 142, 3, 1] | 33.570683 % | 1.775810 %\n",
      "144 | [1, 143, 3, 1] | 33.807090 % | 1.759313 %\n",
      "145 | [1, 144, 3, 1] | 34.043496 % | 1.742863 %\n",
      "146 | [1, 145, 3, 1] | 34.279903 % | 1.726685 %\n",
      "147 | [1, 146, 3, 1] | 34.516310 % | 1.710595 %\n",
      "148 | [1, 147, 3, 1] | 34.752716 % | 1.694542 %\n",
      "149 | [1, 148, 3, 1] | 34.989123 % | 1.678698 %\n",
      "150 | [1, 149, 3, 1] | 35.225529 % | 1.663112 %\n",
      "151 | [1, 150, 3, 1] | 35.461936 % | 1.647798 %\n",
      "152 | [1, 151, 3, 1] | 35.698343 % | 1.632506 %\n",
      "153 | [1, 152, 3, 1] | 35.934749 % | 1.617283 %\n",
      "154 | [1, 153, 3, 1] | 36.171156 % | 1.602119 %\n",
      "155 | [1, 154, 3, 1] | 36.407562 % | 1.586968 %\n",
      "156 | [1, 155, 3, 1] | 36.643969 % | 1.572134 %\n",
      "157 | [1, 156, 3, 1] | 36.880376 % | 1.557371 %\n",
      "158 | [1, 157, 3, 1] | 37.116782 % | 1.542820 %\n",
      "159 | [1, 158, 3, 1] | 37.353189 % | 1.528389 %\n",
      "160 | [1, 159, 3, 1] | 37.589596 % | 1.513965 %\n",
      "161 | [1, 160, 3, 1] | 37.826002 % | 1.499566 %\n",
      "162 | [1, 161, 3, 1] | 38.062409 % | 1.485373 %\n",
      "163 | [1, 162, 3, 1] | 38.298815 % | 1.471258 %\n",
      "164 | [1, 163, 3, 1] | 38.535222 % | 1.457203 %\n",
      "165 | [1, 164, 3, 1] | 38.771629 % | 1.443372 %\n",
      "166 | [1, 165, 3, 1] | 39.008035 % | 1.429493 %\n",
      "167 | [1, 166, 3, 1] | 39.244442 % | 1.415910 %\n",
      "168 | [1, 167, 3, 1] | 39.480849 % | 1.402587 %\n",
      "169 | [1, 168, 3, 1] | 39.717255 % | 1.389195 %\n",
      "170 | [1, 169, 3, 1] | 39.953662 % | 1.375768 %\n",
      "171 | [1, 170, 3, 1] | 40.190068 % | 1.362429 %\n",
      "172 | [1, 171, 3, 1] | 40.426475 % | 1.349226 %\n",
      "173 | [1, 172, 3, 1] | 40.662882 % | 1.336143 %\n",
      "174 | [1, 173, 3, 1] | 40.899288 % | 1.323020 %\n",
      "175 | [1, 174, 3, 1] | 41.135695 % | 1.310470 %\n",
      "176 | [1, 175, 3, 1] | 41.372102 % | 1.298042 %\n",
      "177 | [1, 176, 3, 1] | 41.608508 % | 1.285600 %\n",
      "178 | [1, 177, 3, 1] | 41.844915 % | 1.273299 %\n",
      "179 | [1, 178, 3, 1] | 42.081321 % | 1.260965 %\n",
      "180 | [1, 179, 3, 1] | 42.317728 % | 1.248753 %\n",
      "181 | [1, 180, 3, 1] | 42.554135 % | 1.236511 %\n",
      "182 | [1, 181, 3, 1] | 42.790541 % | 1.224495 %\n",
      "183 | [1, 182, 3, 1] | 43.026948 % | 1.212492 %\n",
      "184 | [1, 183, 3, 1] | 43.263354 % | 1.200502 %\n",
      "185 | [1, 184, 3, 1] | 43.499761 % | 1.188849 %\n",
      "186 | [1, 185, 3, 1] | 43.736168 % | 1.177194 %\n",
      "187 | [1, 186, 3, 1] | 43.972574 % | 1.165556 %\n",
      "188 | [1, 187, 3, 1] | 44.208981 % | 1.153887 %\n",
      "189 | [1, 188, 3, 1] | 44.445388 % | 1.142438 %\n",
      "190 | [1, 189, 3, 1] | 44.681794 % | 1.130899 %\n",
      "191 | [1, 190, 3, 1] | 44.918201 % | 1.119726 %\n",
      "192 | [1, 191, 3, 1] | 45.154607 % | 1.108554 %\n",
      "193 | [1, 192, 3, 1] | 45.391014 % | 1.097420 %\n",
      "194 | [1, 193, 3, 1] | 45.627421 % | 1.086402 %\n",
      "195 | [1, 194, 3, 1] | 45.863827 % | 1.075462 %\n",
      "196 | [1, 195, 3, 1] | 46.100234 % | 1.064673 %\n",
      "197 | [1, 196, 3, 1] | 46.336641 % | 1.054027 %\n",
      "198 | [1, 197, 3, 1] | 46.573047 % | 1.043398 %\n",
      "199 | [1, 198, 3, 1] | 46.809454 % | 1.032840 %\n",
      "200 | [1, 199, 3, 1] | 47.045860 % | 1.022447 %\n",
      "201 | [1, 200, 3, 1] | 47.282267 % | 1.012052 %\n",
      "202 | [1, 201, 3, 1] | 47.518674 % | 1.001734 %\n",
      "203 | [1, 202, 3, 1] | 47.755080 % | 0.991437 %\n",
      "204 | [1, 203, 3, 1] | 47.991487 % | 0.981268 %\n",
      "205 | [1, 204, 3, 1] | 48.227893 % | 0.971142 %\n",
      "206 | [1, 205, 3, 1] | 48.464300 % | 0.961249 %\n",
      "207 | [1, 206, 3, 1] | 48.700707 % | 0.951293 %\n",
      "208 | [1, 207, 3, 1] | 48.937113 % | 0.941385 %\n",
      "209 | [1, 208, 3, 1] | 49.173520 % | 0.931477 %\n",
      "210 | [1, 209, 3, 1] | 49.409927 % | 0.921773 %\n",
      "211 | [1, 210, 3, 1] | 49.646333 % | 0.912099 %\n",
      "212 | [1, 211, 3, 1] | 49.882740 % | 0.902451 %\n",
      "213 | [1, 212, 3, 1] | 50.119146 % | 0.893109 %\n",
      "Target compression ratio reached. Stopping search.\n",
      "Optimal rank: [1, 212, 3, 1], Compression: 50.119146421206175%, Error: 0.893108919262886%\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:45.247574Z",
     "start_time": "2025-06-05T20:10:45.210714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Tensor shape = {list(example_tensor.shape)}\",\n",
    "    f\"Best Rank = {best_rank}\",\n",
    "    f\"Frobenius Error = {frobenius_error:.6f}%\",\n",
    "    f\"Compression Ratio = {compression_ratio:.6f}%\",\n",
    "    f\"Elapsed Time = {elapsed_time:.6f} seconds\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "# Tensor shape = [412, 620, 3]\n",
    "# Best Rank = [1, 169, 3, 1]\n",
    "# Frobenius Error = 3.607338%\n",
    "# Compression Ratio = 50.106613%\n",
    "# Elapsed Time = 46.066486 seconds"
   ],
   "id": "19e9ab98a4dbd989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape = [564, 564, 3]\n",
      "Best Rank = [1, 212, 3, 1]\n",
      "Frobenius Error = 0.893109%\n",
      "Compression Ratio = 50.119146%\n",
      "Elapsed Time = 55.211325 seconds\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### calculate metrics",
   "id": "e27ef03b7ea9bc9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:45.391887Z",
     "start_time": "2025-06-05T20:10:45.356026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_compression_ratio_for_graphs_percent = 50.0\n",
    "frobenius_error_coef_for_graphs = 1.0\n",
    "compression_ratio_coef_for_graphs = 10.0\n",
    "\n",
    "(\n",
    "    custom_alg_compression_ratios,\n",
    "    custom_alg_frobenius_errors,\n",
    "    custom_alg_compression_penalties,\n",
    "    custom_alg_loss_function_results,\n",
    ") = [], [], [], []\n",
    "for element in find_rank_logs:\n",
    "    compression_ratio = element[\"compression_ratio\"] / 100.0\n",
    "    frobenius_error = element[\"frobenius_error\"] / 100.0\n",
    "    target_compression_ratio_for_graphs = target_compression_ratio_for_graphs_percent / 100.0\n",
    "\n",
    "    custom_alg_compression_ratios.append(compression_ratio)\n",
    "    custom_alg_frobenius_errors.append(frobenius_error)\n",
    "\n",
    "    compression_penalty = (target_compression_ratio_for_graphs - compression_ratio) ** 2\n",
    "    loss_function_result = (\n",
    "            frobenius_error_coef_for_graphs * frobenius_error + compression_ratio_coef_for_graphs * compression_penalty\n",
    "    )\n",
    "\n",
    "    custom_alg_compression_penalties.append(compression_penalty)\n",
    "    custom_alg_loss_function_results.append(loss_function_result)"
   ],
   "id": "fecad357155d054",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph of metrics path in search area",
   "id": "54942db9f2e6ff13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.052336Z",
     "start_time": "2025-06-05T20:10:45.433382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figs = []\n",
    "\n",
    "for metric, metric_data in zip(\n",
    "        metrics,\n",
    "        [\n",
    "            custom_alg_frobenius_errors,\n",
    "            custom_alg_compression_ratios,\n",
    "            custom_alg_compression_penalties,\n",
    "            custom_alg_loss_function_results,\n",
    "        ],\n",
    "        strict=False,\n",
    "):\n",
    "    z_values = np.array([search_area_example_results[key].get(metric, np.nan) for key in search_area_example_results])\n",
    "    x_indices = internal_indices[:, 0]\n",
    "    y_indices = internal_indices[:, 1]\n",
    "\n",
    "    # Поиск локальных минимумов\n",
    "    local_min_points = []\n",
    "\n",
    "    metric_dict = {tuple(idx): search_area_example_results[idx] for idx in search_area_example_results}\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_indices, y_indices)):\n",
    "        z = z_values[i]\n",
    "\n",
    "        neighbors = [\n",
    "            (x - 1, y), (x + 1, y),  # По оси X\n",
    "            (x, y - 1), (x, y + 1)  # По оси Y\n",
    "        ]\n",
    "\n",
    "        is_local_min = all(\n",
    "            (neighbor not in metric_dict or metric_dict[neighbor].get(metric, np.inf) >= z)\n",
    "            for neighbor in neighbors\n",
    "        )\n",
    "\n",
    "        if is_local_min:\n",
    "            local_min_points.append((x, y, z))\n",
    "\n",
    "    x_min, y_min, z_min = zip(*local_min_points) if local_min_points else ([], [], [])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Основные точки\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_indices,\n",
    "        y=y_indices,\n",
    "        z=z_values,\n",
    "        mode=\"markers\",\n",
    "        marker={\"size\": 5, \"color\": z_values, \"colorscale\": \"Viridis\", \"opacity\": 0.8},\n",
    "    ))\n",
    "\n",
    "    path_x = []\n",
    "    path_y = []\n",
    "    path_z = []\n",
    "\n",
    "    for i, log in enumerate(find_rank_logs):\n",
    "        rank = log[\"rank\"]\n",
    "        if metric == \"frobenius_error\":\n",
    "            z_value = custom_alg_frobenius_errors[i]\n",
    "        elif metric == \"compression_ratio\":\n",
    "            z_value = custom_alg_compression_ratios[i]\n",
    "        elif metric == \"compression_penalty\":\n",
    "            z_value = custom_alg_compression_penalties[i]\n",
    "        elif metric == \"loss_function_result\":\n",
    "            z_value = custom_alg_loss_function_results[i]\n",
    "\n",
    "        path_x.append(rank[1])\n",
    "        path_y.append(rank[2])\n",
    "        path_z.append(z_value)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[rank[1]],\n",
    "                y=[rank[2]],\n",
    "                z=[z_value],\n",
    "                mode=\"markers\",\n",
    "                marker={\n",
    "                    \"size\": 10 if i == 0 or i == len(find_rank_logs) - 1 else 5,\n",
    "                    \"color\": \"yellow\" if i == 0 or i == len(find_rank_logs) - 1 else \"red\",\n",
    "                    \"opacity\": 0.8,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Добавляем локальные минимумы (выделенные точки)\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_min,\n",
    "        y=y_min,\n",
    "        z=z_min,\n",
    "        mode=\"markers+text\",\n",
    "        marker={\"size\": 8, \"color\": \"blue\", \"symbol\": \"diamond\"},\n",
    "        text=[f\"min: {val:.6f}\" for val in z_min],\n",
    "        textposition=\"top center\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=path_x,\n",
    "            y=path_y,\n",
    "            z=path_z,\n",
    "            mode=\"lines+markers\",\n",
    "            marker={\"size\": 5, \"color\": \"red\", \"opacity\": 0.8},\n",
    "            line={\"color\": \"red\", \"width\": 3},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Search area for example tensor {tensor_name} of {metric.replace('_', ' ').title()} with custom alg path\",\n",
    "        height=800,\n",
    "        scene={\n",
    "            \"xaxis_title\": \"Rank Index 1\",\n",
    "            \"yaxis_title\": \"Rank Index 2\",\n",
    "            \"zaxis_title\": metric.replace(\"_\", \" \").title(),\n",
    "            \"yaxis\": {\"tickmode\": \"array\", \"tickvals\": list(set(y_indices.astype(int)))},\n",
    "        },\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 0},\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    figs.append(fig)\n",
    "\n",
    "html_str = \"\"\n",
    "for fig in figs:\n",
    "    html_str += go.Figure(fig).to_html(full_html=False, include_plotlyjs=False)\n",
    "\n",
    "html_file = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Search area by some metrics</h1>\n",
    "{html_str}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "output_path = f\"../.cache/data_analyze/optimization_algs_for_tensor_train_search_area_{tensor_name}_with_custom_alg.html\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:  # noqa: PTH123\n",
    "    f.write(html_file)"
   ],
   "id": "9d1b32609db91a71",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## scipy algs",
   "id": "2651d78941d7d00d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### args for algs",
   "id": "4d64c4af94a47035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.101687Z",
     "start_time": "2025-06-05T20:10:46.065006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    f\"Initial rank: {example_tensor_initial_rank}\",\n",
    "    f\"TT args: {tensor_train_args}\",\n",
    "    sep='\\n',\n",
    ")\n",
    "\n",
    "frobenius_error_coef_algs = 1.0\n",
    "compression_ratio_coef_algs = 10.0\n",
    "\n",
    "target_compression_ratio_algs = 50.0"
   ],
   "id": "7fcbae2bed1a303b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: (564, 564, 3)\n",
      "Initial rank: [1, 1, 1, 1]\n",
      "TT args: {'svd': 'truncated_svd'}\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.232915Z",
     "start_time": "2025-06-05T20:10:46.196981Z"
    }
   },
   "cell_type": "code",
   "source": "scipy_algs_results = {}",
   "id": "b9d65348e1010483",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funcs for check scipy algs",
   "id": "e00607b46194cb5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### loss function",
   "id": "b01d6e26d0f4bd81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.335209Z",
     "start_time": "2025-06-05T20:10:46.299912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_function(\n",
    "        rank: list,\n",
    "        tensor: np.ndarray,\n",
    "        target_compression_ratio: float,\n",
    "        tensor_train_args: dict[str, str],\n",
    "        frobenius_error_coef: float = 1.0,\n",
    "        compression_ratio_coef: float = 10.0,\n",
    "):\n",
    "    try:\n",
    "        with tl.backend_context(\"pytorch\"):\n",
    "            tensor_cuda = tl.tensor(tensor).to(\"cuda\")\n",
    "            \n",
    "            tt_factors = tl.decomposition.tensor_train(tensor_cuda, rank=rank, **tensor_train_args)\n",
    "            reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "    \n",
    "            frobenius_error = (tl.norm(reconstructed_tensor - tensor_cuda) / tl.norm(tensor_cuda)).item()\n",
    "            compression_ratio = IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(tensor_cuda)\n",
    "    \n",
    "            target_compression_ratio /= 100\n",
    "    \n",
    "            compression_penalty = (target_compression_ratio - compression_ratio) ** 2\n",
    "    \n",
    "            # compression_penalty = target_compression_ratio - compression_ratio\n",
    "            #\n",
    "            # if compression_ratio > 1.0 or compression_ratio < 0.0 or compression_penalty < 0.0 or compression_penalty > 1.0:\n",
    "            #     compression_penalty = float(\"inf\")\n",
    "    \n",
    "        return frobenius_error_coef * frobenius_error + compression_ratio_coef * compression_penalty\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return float(\"inf\")\n",
    "    finally:\n",
    "        torch.cuda.synchronize()\n",
    "        del tensor, tensor_cuda, tt_factors, reconstructed_tensor, compression_ratio, compression_penalty\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ],
   "id": "765c541ff7f4fa73",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### local optimization",
   "id": "4138a6c01de98a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.403919Z",
     "start_time": "2025-06-05T20:10:46.360127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def local_optimize_rank(\n",
    "        tensor: np.ndarray,\n",
    "        target_compression_ratio: float,\n",
    "        tensor_train_args: dict[str, str],\n",
    "        initial_rank: list[int],\n",
    "        frobenius_error_coef: float = 1.0,\n",
    "        compression_ratio_coef: float = 10.0,\n",
    "        optimization_method: str = \"nelder-mead\",\n",
    "        jac: str | None = None,\n",
    "        hess: str | None = None,\n",
    "):\n",
    "    def loss_wrapper(free_rank: list):\n",
    "        full_rank = [1] + list(np.clip(np.round(free_rank).astype(int), 1, None)) + [1]  # noqa: RUF005\n",
    "        return loss_function(\n",
    "            rank=full_rank,\n",
    "            tensor=tensor,\n",
    "            target_compression_ratio=target_compression_ratio,\n",
    "            tensor_train_args=tensor_train_args,\n",
    "            frobenius_error_coef=frobenius_error_coef,\n",
    "            compression_ratio_coef=compression_ratio_coef,\n",
    "        )\n",
    "\n",
    "    def calculate_tt_bounds(tensor_shape: tuple | list) -> list:\n",
    "        \"\"\"\n",
    "        Calculates the bounds for TT-ranks of a tensor based on its shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tensor_shape : tuple or list\n",
    "            List or tuple of tensor dimensions. Each element represents the size of the tensor along that dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of rank bounds in the format [(1, 1), (1, r1_max), ..., (1, 1)].\n",
    "\n",
    "        \"\"\"\n",
    "        d = len(tensor_shape)\n",
    "        bounds = [(1, 1)]\n",
    "\n",
    "        for k in range(1, d):\n",
    "            prod_left = 1\n",
    "            for i in range(k):\n",
    "                prod_left *= tensor_shape[i]\n",
    "\n",
    "            prod_right = 1\n",
    "            for j in range(k, d):\n",
    "                prod_right *= tensor_shape[j]\n",
    "\n",
    "            rk_max = min(prod_left, prod_right)\n",
    "            bounds.append((1, rk_max))\n",
    "\n",
    "        bounds.append((1, 1))\n",
    "        return bounds\n",
    "\n",
    "    def calculate_metrics(\n",
    "            tensor: np.ndarray,\n",
    "            rank: list,\n",
    "            tensor_train_args: dict[str, str],\n",
    "            target_compression_ratio_percent: float = 50.0,\n",
    "            frobenius_error_coef: float = 1.0,\n",
    "            compression_ratio_coef: float = 10.0,\n",
    "    ):\n",
    "\n",
    "        with tl.backend_context(\"pytorch\"):\n",
    "            tensor_cuda = tl.tensor(tensor).to(\"cuda\")\n",
    "            \n",
    "            tt_factors = tl.decomposition.tensor_train(tensor_cuda, rank=rank, **tensor_train_args)\n",
    "            reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "    \n",
    "            target_compression_ratio_percent /= 100\n",
    "    \n",
    "            frobenius_error = (tl.norm(reconstructed_tensor - tensor_cuda) / tl.norm(tensor_cuda)).item()\n",
    "            compression_ratio = IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(tensor_cuda)\n",
    "            compression_penalty = (target_compression_ratio_percent - compression_ratio) ** 2\n",
    "            loss = frobenius_error_coef * frobenius_error + compression_ratio_coef * compression_penalty\n",
    "    \n",
    "            metrics = {\n",
    "                \"frobenius_error\": frobenius_error,\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"compression_penalty\": compression_penalty,\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        del tensor_cuda, tensor, tt_factors, reconstructed_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    class OptimizationLogger:\n",
    "        def __init__(\n",
    "                self,\n",
    "                tensor: np.ndarray,\n",
    "                tensor_train_args: dict[str, str],\n",
    "                target_compression_ratio: float = 50.0,\n",
    "                frobenius_error_coef: float = 1.0,\n",
    "                compression_ratio_coef: float = 10.0,\n",
    "        ):\n",
    "            self.logs = []\n",
    "            self.current_iteration = -1\n",
    "\n",
    "            self.tensor = tensor\n",
    "            self.tensor_train_args = tensor_train_args\n",
    "            self.target_compression_ratio = target_compression_ratio\n",
    "            self.frobenius_error_coef = frobenius_error_coef\n",
    "            self.compression_ratio_coef = compression_ratio_coef\n",
    "\n",
    "        def calculate_metrics(\n",
    "                self,\n",
    "                rank: list,\n",
    "        ) -> dict[str, float]:\n",
    "            return calculate_metrics(\n",
    "                tensor=self.tensor,\n",
    "                rank=rank,\n",
    "                tensor_train_args=self.tensor_train_args,\n",
    "                target_compression_ratio_percent=self.target_compression_ratio,\n",
    "                frobenius_error_coef=self.frobenius_error_coef,\n",
    "                compression_ratio_coef=self.compression_ratio_coef,\n",
    "            )\n",
    "\n",
    "        def callback(self, xk):\n",
    "            self.current_iteration += 1\n",
    "\n",
    "            rank = [1] + list(np.round(xk).astype(int)) + [1]  # noqa: RUF005\n",
    "            metrics = self.calculate_metrics(rank=rank)\n",
    "\n",
    "            self.logs.append(\n",
    "                {\n",
    "                    \"step\": self.current_iteration,\n",
    "                    \"raw_xk\": xk,\n",
    "                    \"rank\": rank,\n",
    "                    \"metrics\": metrics,\n",
    "                }\n",
    "            )\n",
    "            print(f\"\\n=== Iteration {self.current_iteration} complete ===\", f\"New rank estimate: {rank}\\n\", sep=\"\\n\")\n",
    "\n",
    "    optimization_logger = OptimizationLogger(\n",
    "        tensor=tensor,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        target_compression_ratio=target_compression_ratio,\n",
    "        frobenius_error_coef=frobenius_error_coef,\n",
    "        compression_ratio_coef=compression_ratio_coef,\n",
    "    )\n",
    "\n",
    "    free_rank = initial_rank[1:-1]\n",
    "\n",
    "    # params\n",
    "    is_bounds_variable_usable = [\n",
    "        \"nelder-mead\",\n",
    "        \"l-bfgs-b\",\n",
    "        \"tnc\",\n",
    "        \"slsqp\",\n",
    "        \"powell\",\n",
    "        \"trust-constr\",\n",
    "        \"cobyla\",\n",
    "        \"cobyqa\",\n",
    "    ]\n",
    "\n",
    "    is_adaptive_variable_usable = [\"nelder-mead\"]\n",
    "\n",
    "    is_jac_variable_usable = [\n",
    "        \"cg\",\n",
    "        \"bfgs\",\n",
    "        \"newton-cg\",\n",
    "        \"l-bfgs-b\",\n",
    "        \"tnc\",\n",
    "        \"slsqp\",\n",
    "        \"trust-ncg\",\n",
    "        \"trust-krylov\",\n",
    "        \"trust-exact\",\n",
    "        \"trust-constr\",\n",
    "    ]\n",
    "\n",
    "    is_hess_variable_usable = [\"newton-cg\", \"dogleg\", \"trust-ncg\", \"trust-krylov\", \"trust-exact\", \" trust-constr\"]\n",
    "\n",
    "    is_callback_variable_not_usable = [\"tnc\", \"slsqp\", \"cobyla\"]\n",
    "\n",
    "    free_bounds = calculate_tt_bounds(tensor.shape)[1:-1] if optimization_method in is_bounds_variable_usable else None\n",
    "\n",
    "    callback_param = (\n",
    "        optimization_logger.callback if optimization_method not in is_callback_variable_not_usable else None\n",
    "    )\n",
    "\n",
    "    adaptive = optimization_method.lower() in is_adaptive_variable_usable\n",
    "\n",
    "    jac = jac if optimization_method.lower() in is_jac_variable_usable else None\n",
    "    jac = None\n",
    "\n",
    "    hess = hess if optimization_method.lower() in is_hess_variable_usable else None\n",
    "\n",
    "    minimize_kwargs = {\n",
    "        \"fun\": loss_wrapper,\n",
    "        \"x0\": free_rank,\n",
    "        \"method\": optimization_method,\n",
    "        \"jac\": jac,\n",
    "        \"hess\": hess,\n",
    "        \"bounds\": free_bounds,\n",
    "        \"callback\": callback_param,\n",
    "        \"options\": {\n",
    "            \"disp\": True,\n",
    "            # \"maxiter\": 1000,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if adaptive:\n",
    "        minimize_kwargs[\"options\"][\"adaptive\"] = adaptive\n",
    "\n",
    "    if jac:\n",
    "        minimize_kwargs[\"jac\"] = jac\n",
    "\n",
    "    if hess:\n",
    "        minimize_kwargs[\"hess\"] = hess\n",
    "\n",
    "    # params\n",
    "\n",
    "    result = minimize(**minimize_kwargs)\n",
    "\n",
    "    optimal_rank = [1] + list(np.clip(np.round(result.x).astype(int), 1, None)) + [1]  # noqa: RUF005\n",
    "    final_loss = result.fun\n",
    "\n",
    "    return optimal_rank, final_loss, result, optimization_logger.logs"
   ],
   "id": "fd7c39a887c6e830",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### global optimization",
   "id": "24729bb13807d588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.464465Z",
     "start_time": "2025-06-05T20:10:46.427428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_wrapper(\n",
    "        free_rank: list,\n",
    "        tensor: np.ndarray,\n",
    "        target_compression_ratio: float,\n",
    "        frobenius_error_coef: float,\n",
    "        compression_ratio_coef: float,\n",
    "        tensor_train_args: dict[str, str],\n",
    ") -> float:\n",
    "    full_rank = [1] + list(np.clip(np.round(free_rank).astype(int), 1, None)) + [1]  # noqa: RUF005\n",
    "    try:\n",
    "        loss = loss_function(\n",
    "            rank=full_rank,\n",
    "            tensor=tensor,\n",
    "            target_compression_ratio=target_compression_ratio,\n",
    "            tensor_train_args=tensor_train_args,\n",
    "            frobenius_error_coef=frobenius_error_coef,\n",
    "            compression_ratio_coef=compression_ratio_coef,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        loss = float(\"inf\")\n",
    "    finally:\n",
    "        return loss"
   ],
   "id": "a5b90531f88bb212",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:46.535736Z",
     "start_time": "2025-06-05T20:10:46.489151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def global_optimize_rank(\n",
    "        tensor: np.ndarray,\n",
    "        target_compression_ratio: float,\n",
    "        tensor_train_args: dict[str, str],\n",
    "        initial_rank: list[int],\n",
    "        frobenius_error_coef: float = 1.0,\n",
    "        compression_ratio_coef: float = 10.0,\n",
    "        optimization_method: str = \"differential_evolution\",\n",
    "        loss_function_fixed: Callable | None = None,\n",
    "):\n",
    "    def calculate_tt_bounds(tensor_shape: tuple | list) -> list:\n",
    "        \"\"\"\n",
    "        Calculates the bounds for TT-ranks of a tensor based on its shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tensor_shape : tuple or list\n",
    "            List or tuple of tensor dimensions. Each element represents the size of the tensor along that dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of rank bounds in the format [(1, 1), (1, r1_max), ..., (1, 1)].\n",
    "\n",
    "        \"\"\"\n",
    "        d = len(tensor_shape)\n",
    "        bounds = [(1, 1)]\n",
    "\n",
    "        for k in range(1, d):\n",
    "            prod_left = 1\n",
    "            for i in range(k):\n",
    "                prod_left *= tensor_shape[i]\n",
    "\n",
    "            prod_right = 1\n",
    "            for j in range(k, d):\n",
    "                prod_right *= tensor_shape[j]\n",
    "\n",
    "            rk_max = min(prod_left, prod_right)\n",
    "            bounds.append((1, rk_max))\n",
    "\n",
    "        bounds.append((1, 1))\n",
    "        return bounds\n",
    "\n",
    "    def calculate_metrics(\n",
    "            tensor: np.ndarray,\n",
    "            rank: list,\n",
    "            tensor_train_args: dict[str, str],\n",
    "            target_compression_ratio_percent: float = 50.0,\n",
    "            frobenius_error_coef: float = 1.0,\n",
    "            compression_ratio_coef: float = 10.0,\n",
    "    ):\n",
    "\n",
    "        with tl.backend_context(\"pytorch\"):\n",
    "            tensor_cuda = tl.tensor(tensor).to(\"cuda\")\n",
    "\n",
    "            tt_factors = tl.decomposition.tensor_train(tensor_cuda, rank=rank, **tensor_train_args)\n",
    "            reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "    \n",
    "            target_compression_ratio_percent /= 100\n",
    "    \n",
    "            frobenius_error = (tl.norm(reconstructed_tensor - tensor_cuda) / tl.norm(tensor_cuda)).item()\n",
    "            compression_ratio = IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(tensor_cuda)\n",
    "            compression_penalty = (target_compression_ratio_percent - compression_ratio) ** 2\n",
    "            loss = frobenius_error_coef * frobenius_error + compression_ratio_coef * compression_penalty\n",
    "    \n",
    "            metrics = {\n",
    "                \"frobenius_error\": frobenius_error,\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"compression_penalty\": compression_penalty,\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        del tensor_cuda, tensor, tt_factors, reconstructed_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    class OptimizationLogger:\n",
    "        def __init__(\n",
    "                self,\n",
    "                tensor: np.ndarray,\n",
    "                tensor_train_args: dict[str, str],\n",
    "                target_compression_ratio: float = 50.0,\n",
    "                frobenius_error_coef: float = 1.0,\n",
    "                compression_ratio_coef: float = 10.0,\n",
    "        ):\n",
    "            self.logs = []\n",
    "            self.current_iteration = -1\n",
    "\n",
    "            self.tensor = tensor\n",
    "            self.tensor_train_args = tensor_train_args\n",
    "            self.target_compression_ratio = target_compression_ratio\n",
    "            self.frobenius_error_coef = frobenius_error_coef\n",
    "            self.compression_ratio_coef = compression_ratio_coef\n",
    "\n",
    "        def calculate_metrics(\n",
    "                self,\n",
    "                rank: list,\n",
    "        ) -> dict[str, float]:\n",
    "            return calculate_metrics(\n",
    "                tensor=self.tensor,\n",
    "                rank=rank,\n",
    "                tensor_train_args=self.tensor_train_args,\n",
    "                target_compression_ratio_percent=self.target_compression_ratio,\n",
    "                frobenius_error_coef=self.frobenius_error_coef,\n",
    "                compression_ratio_coef=self.compression_ratio_coef,\n",
    "            )\n",
    "\n",
    "        def callback(self, intermediate_result: OptimizeResult):\n",
    "            self.current_iteration += 1\n",
    "\n",
    "            rank = [1] + list(np.round(intermediate_result.x).astype(int)) + [1]  # noqa: RUF005\n",
    "            metrics = self.calculate_metrics(rank=rank)\n",
    "\n",
    "            self.logs.append(\n",
    "                {\n",
    "                    \"step\": self.current_iteration,\n",
    "                    \"rank\": rank,\n",
    "                    \"metrics\": metrics,\n",
    "                    \"raw_results\": intermediate_result,\n",
    "                }\n",
    "            )\n",
    "            print(f\"\\n=== Iteration {self.current_iteration} complete ===\", f\"New rank estimate: {rank}\\n\", sep=\"\\n\")\n",
    "\n",
    "    optimization_logger = OptimizationLogger(\n",
    "        tensor=tensor,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        target_compression_ratio=target_compression_ratio,\n",
    "        frobenius_error_coef=frobenius_error_coef,\n",
    "        compression_ratio_coef=compression_ratio_coef,\n",
    "    )\n",
    "\n",
    "    loss_function_fixed = partial(\n",
    "        loss_wrapper,\n",
    "        tensor=tensor,\n",
    "        target_compression_ratio=target_compression_ratio,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        frobenius_error_coef=frobenius_error_coef,\n",
    "        compression_ratio_coef=compression_ratio_coef,\n",
    "    )\n",
    "\n",
    "    # params\n",
    "    is_bounds_variable_usable = [\n",
    "        \"differential_evolution\",\n",
    "    ]\n",
    "\n",
    "    is_callback_variable_not_usable = []\n",
    "\n",
    "    free_bounds = calculate_tt_bounds(tensor.shape)[1:-1] if optimization_method in is_bounds_variable_usable else None\n",
    "\n",
    "    callback_param = (\n",
    "        optimization_logger.callback if optimization_method not in is_callback_variable_not_usable else None\n",
    "    )\n",
    "\n",
    "    if optimization_method == \"differential_evolution\":\n",
    "        optimization_kwargs_differential_evolution = {\n",
    "\n",
    "            \"func\": loss_function_fixed,\n",
    "            \"bounds\": free_bounds,\n",
    "\n",
    "            \"strategy\": \"best1bin\",\n",
    "            \"maxiter\": 50,\n",
    "            \"popsize\": 10,\n",
    "            \"tol\": 0.01,\n",
    "            \"atol\": 0.001,\n",
    "            \"mutation\": (0.3, 0.7),\n",
    "            \"recombination\": 0.9,\n",
    "            \"init\": \"latinhypercube\",\n",
    "            \"polish\": True,\n",
    "\n",
    "            \"workers\": 1,\n",
    "            \"updating\": \"immediate\", # {‘immediate’ - when 1 worker, ‘deferred’ - when more than 1 worker}\n",
    "\n",
    "            \"callback\": callback_param,\n",
    "            \"disp\": True,\n",
    "        }\n",
    "\n",
    "        result = differential_evolution(**optimization_kwargs_differential_evolution)\n",
    "\n",
    "        optimal_rank = [1] + list(np.clip(np.round(result.x).astype(int), 1, None)) + [1]  # noqa: RUF005\n",
    "        final_loss = result.fun\n",
    "\n",
    "    return optimal_rank, final_loss, result, optimization_logger.logs"
   ],
   "id": "a2515442ebd2fc67",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### check algs",
   "id": "3f1f59256c467672"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://docs.scipy.org/doc/scipy-1.15.0/tutorial/optimize.html#",
   "id": "2b3a14c9ca8891aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### nelder-mead",
   "id": "47302bb20af108a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:10:59.225489Z",
     "start_time": "2025-06-05T20:10:46.562511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method = \"nelder-mead\"\n",
    "\n",
    "print(\n",
    "    f\"Testing optimization method: {method}\",\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    f\"Initial rank: {example_tensor_initial_rank}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "try:\n",
    "    # check optimizer method\n",
    "    start_time = time.perf_counter()\n",
    "    optimal_rank, final_loss, minimize_result_nelder_mead, iteration_logs_nelder_mead = local_optimize_rank(\n",
    "        tensor=example_tensor,\n",
    "        target_compression_ratio=target_compression_ratio_algs,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        initial_rank=example_tensor_initial_rank,\n",
    "        optimization_method=method,\n",
    "        frobenius_error_coef=frobenius_error_coef_algs,\n",
    "        compression_ratio_coef=compression_ratio_coef_algs,\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    # check final frobenius error and compression ratio\n",
    "    tt_factors = tl.decomposition.tensor_train(example_tensor, rank=optimal_rank, **tensor_train_args)\n",
    "    reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "\n",
    "    frobenius_error = 100.0 * (tl.norm(reconstructed_tensor - example_tensor) / tl.norm(example_tensor)).item()\n",
    "    compression_ratio = (\n",
    "            100.0 * IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(example_tensor)\n",
    "    )\n",
    "\n",
    "    scipy_algs_results[method] = {\n",
    "        \"final_results\": minimize_result_nelder_mead,\n",
    "        \"steps_results\": iteration_logs_nelder_mead,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Optimal rank: {optimal_rank}\",\n",
    "        f\"Elapsed time: {elapsed_time:.6f} seconds\",\n",
    "        f\"Frobenius Error: {frobenius_error:.6f}%\",\n",
    "        f\"Compression Ratio: {compression_ratio:.6f}%\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error with method {method}: {e}\")"
   ],
   "id": "6e659a5cf1fc85f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimization method: nelder-mead\n",
      "Tensor shape: (564, 564, 3)\n",
      "Initial rank: [1, 1, 1, 1]\n",
      "\n",
      "=== Iteration 0 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 1 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 2 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 3 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 4 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 5 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 6 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 7 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 8 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "\n",
      "=== Iteration 9 complete ===\n",
      "New rank estimate: [1, 1, 1, 1]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.741255\n",
      "         Iterations: 10\n",
      "         Function evaluations: 39\n",
      "Optimal rank: [1, 1, 1, 1]\n",
      "Elapsed time: 12.206945 seconds\n",
      "Frobenius Error: 25.309318%\n",
      "Compression Ratio: 0.118518%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### powell",
   "id": "22b4dc9da590e2db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:11:27.864595Z",
     "start_time": "2025-06-05T20:10:59.284997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method = \"powell\"\n",
    "\n",
    "print(\n",
    "    f\"Testing optimization method: {method}\",\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    f\"Initial rank: {example_tensor_initial_rank}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "try:\n",
    "    # check optimizer method\n",
    "    start_time = time.perf_counter()\n",
    "    optimal_rank, final_loss, minimize_result_powell, iteration_logs_powell = local_optimize_rank(\n",
    "        tensor=example_tensor,\n",
    "        target_compression_ratio=target_compression_ratio_algs,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        initial_rank=example_tensor_initial_rank,\n",
    "        optimization_method=method,\n",
    "        frobenius_error_coef=frobenius_error_coef_algs,\n",
    "        compression_ratio_coef=compression_ratio_coef_algs,\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    # check final frobenius error and compression ratio\n",
    "    tt_factors = tl.decomposition.tensor_train(example_tensor, rank=optimal_rank, **tensor_train_args)\n",
    "    reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "\n",
    "    frobenius_error = 100.0 * (tl.norm(reconstructed_tensor - example_tensor) / tl.norm(example_tensor)).item()\n",
    "    compression_ratio = (\n",
    "            100.0 * IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(example_tensor)\n",
    "    )\n",
    "\n",
    "    scipy_algs_results[method] = {\n",
    "        \"final_results\": minimize_result_powell,\n",
    "        \"steps_results\": iteration_logs_powell,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Optimal rank: {optimal_rank}\",\n",
    "        f\"Elapsed time: {elapsed_time:.6f} seconds\",\n",
    "        f\"Frobenius Error: {frobenius_error:.6f}%\",\n",
    "        f\"Compression Ratio: {compression_ratio:.6f}%\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error with method {method}: {e}\")"
   ],
   "id": "332b57f9b01dbdc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimization method: powell\n",
      "Tensor shape: (564, 564, 3)\n",
      "Initial rank: [1, 1, 1, 1]\n",
      "\n",
      "=== Iteration 0 complete ===\n",
      "New rank estimate: [1, 423, 2, 1]\n",
      "\n",
      "\n",
      "=== Iteration 1 complete ===\n",
      "New rank estimate: [1, 282, 2, 1]\n",
      "\n",
      "\n",
      "=== Iteration 2 complete ===\n",
      "New rank estimate: [1, 282, 2, 1]\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.021653\n",
      "         Iterations: 3\n",
      "         Function evaluations: 105\n",
      "Optimal rank: [1, 282, 2, 1]\n",
      "Elapsed time: 28.314864 seconds\n",
      "Frobenius Error: 2.165262%\n",
      "Compression Ratio: 50.000629%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SLSQP",
   "id": "bc618dc9bd894791"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:11:28.946638Z",
     "start_time": "2025-06-05T20:11:27.918010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method = \"slsqp\"\n",
    "\n",
    "print(\n",
    "    f\"Testing optimization method: {method}\",\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    f\"Initial rank: {example_tensor_initial_rank}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "try:\n",
    "    # check optimizer method\n",
    "    start_time = time.perf_counter()\n",
    "    optimal_rank, final_loss, minimize_result_slsqp, iteration_logs_slsqp = local_optimize_rank(\n",
    "        tensor=example_tensor,\n",
    "        target_compression_ratio=target_compression_ratio_algs,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        initial_rank=example_tensor_initial_rank,\n",
    "        optimization_method=method,\n",
    "        frobenius_error_coef=frobenius_error_coef_algs,\n",
    "        compression_ratio_coef=compression_ratio_coef_algs,\n",
    "        jac=None,\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    # check final frobenius error and compression ratio\n",
    "    tt_factors = tl.decomposition.tensor_train(example_tensor, rank=optimal_rank, **tensor_train_args)\n",
    "    reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "\n",
    "    frobenius_error = 100.0 * (tl.norm(reconstructed_tensor - example_tensor) / tl.norm(example_tensor)).item()\n",
    "    compression_ratio = (\n",
    "            100.0 * IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(example_tensor)\n",
    "    )\n",
    "\n",
    "    scipy_algs_results[method] = {\n",
    "        \"final_results\": minimize_result_slsqp,\n",
    "        \"steps_results\": iteration_logs_slsqp,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Optimal rank: {optimal_rank}\",\n",
    "        f\"Elapsed time: {elapsed_time:.6f} seconds\",\n",
    "        f\"Frobenius Error: {frobenius_error:.6f}%\",\n",
    "        f\"Compression Ratio: {compression_ratio:.6f}%\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error with method {method}: {e}\")"
   ],
   "id": "1438dbd56e34fb3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimization method: slsqp\n",
      "Tensor shape: (564, 564, 3)\n",
      "Initial rank: [1, 1, 1, 1]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 2.7412554316590985\n",
      "            Iterations: 1\n",
      "            Function evaluations: 3\n",
      "            Gradient evaluations: 1\n",
      "Optimal rank: [1, 1, 1, 1]\n",
      "Elapsed time: 0.687272 seconds\n",
      "Frobenius Error: 25.309318%\n",
      "Compression Ratio: 0.118518%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### differential_evolution",
   "id": "c043c213b8d9a1c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.116542Z",
     "start_time": "2025-06-05T20:11:29.005046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method = \"differential_evolution\"\n",
    "\n",
    "print(\n",
    "    f\"Testing optimization method: {method}\",\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "try:\n",
    "    # check optimizer method\n",
    "    start_time = time.perf_counter()\n",
    "    optimal_rank, final_loss, minimize_result_differential_evolution, iteration_logs_differential_evolution = global_optimize_rank(\n",
    "        tensor=example_tensor,\n",
    "        target_compression_ratio=target_compression_ratio_algs,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        initial_rank=example_tensor_initial_rank,\n",
    "        optimization_method=method,\n",
    "        frobenius_error_coef=frobenius_error_coef_algs,\n",
    "        compression_ratio_coef=compression_ratio_coef_algs,\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    # check final frobenius error and compression ratio\n",
    "    with tl.backend_context(\"pytorch\"):\n",
    "        tensor_cuda = tl.tensor(example_tensor).to(\"cuda\")\n",
    "        tt_factors = tl.decomposition.tensor_train(tensor_cuda, rank=optimal_rank, **tensor_train_args)\n",
    "        reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "    \n",
    "        frobenius_error = 100.0 * (tl.norm(reconstructed_tensor - tensor_cuda) / tl.norm(tensor_cuda)).item()\n",
    "        compression_ratio = (\n",
    "                100.0 * IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(tensor_cuda)\n",
    "        )\n",
    "\n",
    "    scipy_algs_results[method] = {\n",
    "        \"final_results\": minimize_result_differential_evolution,\n",
    "        \"steps_results\": iteration_logs_differential_evolution,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Optimal rank: {optimal_rank}\",\n",
    "        f\"Elapsed time: {elapsed_time:.6f} seconds\",\n",
    "        f\"Frobenius Error: {frobenius_error:.6f}%\",\n",
    "        f\"Compression Ratio: {compression_ratio:.6f}%\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error with method {method}: {e}\")"
   ],
   "id": "2527c08e405ac8cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimization method: differential_evolution\n",
      "Tensor shape: (564, 564, 3)\n",
      "differential_evolution step 1: f(x)= 0.02488813178839536\n",
      "\n",
      "=== Iteration 0 complete ===\n",
      "New rank estimate: [1, 272, 2, 1]\n",
      "\n",
      "differential_evolution step 2: f(x)= 0.009246066083439163\n",
      "\n",
      "=== Iteration 1 complete ===\n",
      "New rank estimate: [1, 210, 3, 1]\n",
      "\n",
      "differential_evolution step 3: f(x)= 0.009099131982462427\n",
      "\n",
      "=== Iteration 2 complete ===\n",
      "New rank estimate: [1, 214, 3, 1]\n",
      "\n",
      "differential_evolution step 4: f(x)= 0.0089452850623151\n",
      "\n",
      "=== Iteration 3 complete ===\n",
      "New rank estimate: [1, 212, 3, 1]\n",
      "\n",
      "differential_evolution step 5: f(x)= 0.0089452850623151\n",
      "\n",
      "=== Iteration 4 complete ===\n",
      "New rank estimate: [1, 212, 3, 1]\n",
      "\n",
      "differential_evolution step 6: f(x)= 0.0089452850623151\n",
      "\n",
      "=== Iteration 5 complete ===\n",
      "New rank estimate: [1, 212, 3, 1]\n",
      "\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "Optimal rank: [1, 212, 3, 1]\n",
      "Elapsed time: 38.966592 seconds\n",
      "Frobenius Error: 0.893109%\n",
      "Compression Ratio: 50.119146%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SHGO",
   "id": "78ac863bc4f17d4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.179544Z",
     "start_time": "2025-06-05T20:12:08.141953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method = \"shgo\"\n",
    "\n",
    "print(\n",
    "    f\"Testing optimization method: {method}\",\n",
    "    f\"Tensor shape: {example_tensor.shape}\",\n",
    "    f\"Initial rank: {example_tensor_initial_rank}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "try:\n",
    "    # check optimizer method\n",
    "    start_time = time.perf_counter()\n",
    "    optimal_rank, final_loss, minimize_result_shgo, iteration_logs_shgo = global_optimize_rank(\n",
    "        tensor=example_tensor,\n",
    "        target_compression_ratio=target_compression_ratio_algs,\n",
    "        tensor_train_args=tensor_train_args,\n",
    "        initial_rank=example_tensor_initial_rank,\n",
    "        optimization_method=method,\n",
    "        frobenius_error_coef=frobenius_error_coef_algs,\n",
    "        compression_ratio_coef=compression_ratio_coef_algs,\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    # check final frobenius error and compression ratio\n",
    "    tt_factors = tl.decomposition.tensor_train(example_tensor, rank=optimal_rank, **tensor_train_args)\n",
    "    reconstructed_tensor = tl.tt_to_tensor(tt_factors)\n",
    "\n",
    "    frobenius_error = 100.0 * (tl.norm(reconstructed_tensor - example_tensor) / tl.norm(example_tensor)).item()\n",
    "    compression_ratio = (\n",
    "            100.0 * IMetricCalculator.get_tensors_size(*tt_factors) / IMetricCalculator.get_tensors_size(example_tensor)\n",
    "    )\n",
    "\n",
    "    scipy_algs_results[method] = {\n",
    "        \"final_results\": minimize_result_shgo,\n",
    "        \"steps_results\": iteration_logs_shgo,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Optimal rank: {optimal_rank}\",\n",
    "        f\"Elapsed time: {elapsed_time:.6f} seconds\",\n",
    "        f\"Frobenius Error: {frobenius_error:.6f}%\",\n",
    "        f\"Compression Ratio: {compression_ratio:.6f}%\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error with method {method}: {e}\")"
   ],
   "id": "b0197d8d2ffac164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimization method: shgo\n",
      "Tensor shape: (564, 564, 3)\n",
      "Initial rank: [1, 1, 1, 1]\n",
      "Error with method shgo: cannot access local variable 'optimal_rank' where it is not associated with a value\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Path of metrics in search area",
   "id": "c7a58c6bb73cc10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.812283Z",
     "start_time": "2025-06-05T20:12:08.252762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for method_name, method_results in scipy_algs_results.items():\n",
    "\n",
    "    metric_names = [\"frobenius_error\", \"compression_ratio\", \"compression_penalty\", \"loss_function_result\"]\n",
    "    frobenius_error_from_method, compression_ratio_from_method, compression_penalty_from_method, loss_from_method = [], [], [], []\n",
    "    for method_steps_logs in method_results['steps_results']:\n",
    "        frobenius_error_from_method.append(method_steps_logs[\"metrics\"][\"frobenius_error\"])\n",
    "        compression_ratio_from_method.append(method_steps_logs[\"metrics\"][\"compression_ratio\"])\n",
    "        compression_penalty_from_method.append(method_steps_logs[\"metrics\"][\"compression_penalty\"])\n",
    "        loss_from_method.append(method_steps_logs[\"metrics\"][\"loss\"])\n",
    "\n",
    "    figs = []\n",
    "\n",
    "    for metric, metric_data in zip(\n",
    "            metric_names,\n",
    "            [\n",
    "                frobenius_error_from_method,\n",
    "                compression_ratio_from_method,\n",
    "                compression_penalty_from_method,\n",
    "                loss_from_method,\n",
    "            ],\n",
    "            strict=False,\n",
    "    ):\n",
    "        z_values = np.array(\n",
    "            [search_area_example_results[key].get(metric, np.nan) for key in search_area_example_results])\n",
    "        x_indices = internal_indices[:, 0]\n",
    "        y_indices = internal_indices[:, 1]\n",
    "\n",
    "        # Поиск локальных минимумов\n",
    "        local_min_points = []\n",
    "\n",
    "        metric_dict = {tuple(idx): search_area_example_results[idx] for idx in search_area_example_results}\n",
    "\n",
    "        for i, (x, y) in enumerate(zip(x_indices, y_indices)):\n",
    "            z = z_values[i]\n",
    "\n",
    "            neighbors = [\n",
    "                (x - 1, y), (x + 1, y),  # По оси X\n",
    "                (x, y - 1), (x, y + 1)  # По оси Y\n",
    "            ]\n",
    "\n",
    "            is_local_min = all(\n",
    "                (neighbor not in metric_dict or metric_dict[neighbor].get(metric, np.inf) >= z)\n",
    "                for neighbor in neighbors\n",
    "            )\n",
    "\n",
    "            if is_local_min:\n",
    "                local_min_points.append((x, y, z))\n",
    "\n",
    "        x_min, y_min, z_min = zip(*local_min_points) if local_min_points else ([], [], [])\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Основные точки\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x_indices,\n",
    "            y=y_indices,\n",
    "            z=z_values,\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 5, \"color\": z_values, \"colorscale\": \"Viridis\", \"opacity\": 0.8},\n",
    "        ))\n",
    "\n",
    "        path_x = []\n",
    "        path_y = []\n",
    "        path_z = []\n",
    "\n",
    "        for i, log in enumerate(method_results['steps_results']):\n",
    "            rank = log[\"rank\"]\n",
    "            if metric == \"frobenius_error\":\n",
    "                z_value = frobenius_error_from_method[i]\n",
    "            elif metric == \"compression_ratio\":\n",
    "                z_value = compression_ratio_from_method[i]\n",
    "            elif metric == \"compression_penalty\":\n",
    "                z_value = compression_penalty_from_method[i]\n",
    "            elif metric == \"loss_function_result\":\n",
    "                z_value = loss_from_method[i]\n",
    "\n",
    "            path_x.append(rank[1])\n",
    "            path_y.append(rank[2])\n",
    "            path_z.append(z_value)\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[rank[1]],\n",
    "                    y=[rank[2]],\n",
    "                    z=[z_value],\n",
    "                    mode=\"markers\",\n",
    "                    marker={\n",
    "                        \"size\": 10 if i == 0 or i == len(method_results['steps_results']) - 1 else 5,\n",
    "                        \"color\": \"yellow\" if i == 0 or i == len(method_results['steps_results']) - 1 else \"red\",\n",
    "                        \"opacity\": 0.8,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Добавляем локальные минимумы (выделенные точки)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x_min,\n",
    "            y=y_min,\n",
    "            z=z_min,\n",
    "            mode=\"markers+text\",\n",
    "            marker={\"size\": 8, \"color\": \"blue\", \"symbol\": \"diamond\"},\n",
    "            text=[f\"min: {val:.6f}\" for val in z_min],\n",
    "            textposition=\"top center\",\n",
    "        ))\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=path_x,\n",
    "                y=path_y,\n",
    "                z=path_z,\n",
    "                mode=\"lines+markers\",\n",
    "                marker={\"size\": 5, \"color\": \"red\", \"opacity\": 0.8},\n",
    "                line={\"color\": \"red\", \"width\": 3},\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Search area for example tensor {tensor_name} of {metric.replace('_', ' ').title()} with {method_name} alg path\",\n",
    "            height=800,\n",
    "            scene={\n",
    "                \"xaxis_title\": \"Rank Index 1\",\n",
    "                \"yaxis_title\": \"Rank Index 2\",\n",
    "                \"zaxis_title\": metric.replace(\"_\", \" \").title(),\n",
    "                \"yaxis\": {\"tickmode\": \"array\", \"tickvals\": list(set(y_indices.astype(int)))},\n",
    "            },\n",
    "            margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 0},\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "    html_str = \"\"\n",
    "    for fig in figs:\n",
    "        html_str += go.Figure(fig).to_html(full_html=False, include_plotlyjs=False)\n",
    "\n",
    "    html_file = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Search area by some metrics</h1>\n",
    "    {html_str}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = f\"../.cache/data_analyze/optimization_algs_for_tensor_train_search_area_{tensor_name}_with_{method_name}_alg.html\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:  # noqa: PTH123\n",
    "        f.write(html_file)"
   ],
   "id": "8a3a2d6ab43e06f1",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimization alg for target tensor",
   "id": "9b5475496caef09d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## calculation",
   "id": "fa4ab92e29e43433"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.869878Z",
     "start_time": "2025-06-05T20:12:08.835695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from src.utils.eeg_controller import create_eeg_limo_data_tensor\n",
    "# \n",
    "# cache_dir_eeg = \"../.cache/eeg\"\n",
    "# \n",
    "# target_tensor = create_eeg_limo_data_tensor(cache_dir_eeg=cache_dir_eeg)\n",
    "# target_initial_rank = calculate_tensor_train_initial_rank(calculate_tt_bounds(target_tensor.shape))"
   ],
   "id": "51bab3e1cd07fc74",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.929210Z",
     "start_time": "2025-06-05T20:12:08.895210Z"
    }
   },
   "cell_type": "code",
   "source": "# calculate_tt_bounds(target_tensor.shape)",
   "id": "979a4ee264cb50b7",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T20:12:08.990026Z",
     "start_time": "2025-06-05T20:12:08.953876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# method = \"differential_evolution\"\n",
    "# \n",
    "# scipy_target_tensor_results = {}\n",
    "# \n",
    "# print(\n",
    "#     f\"Testing optimization method: {method}\",\n",
    "#     f\"Tensor shape: {target_tensor.shape}\",\n",
    "#     sep=\"\\n\",\n",
    "# )\n",
    "# try:\n",
    "#     # check optimizer method\n",
    "#     start_time = time.perf_counter()\n",
    "#     optimal_rank, final_loss, minimize_result_differential_evolution, iteration_logs_differential_evolution = global_optimize_rank(\n",
    "#         tensor=target_tensor,\n",
    "#         target_compression_ratio=target_compression_ratio_algs,\n",
    "#         tensor_train_args=tensor_train_args,\n",
    "#         initial_rank=target_initial_rank,\n",
    "#         optimization_method=method,\n",
    "#         frobenius_error_coef=frobenius_error_coef_algs,\n",
    "#         compression_ratio_coef=compression_ratio_coef_algs,\n",
    "#     )\n",
    "#     elapsed_time = time.perf_counter() - start_time\n",
    "# \n",
    "#     scipy_target_tensor_results[method] = {\n",
    "#         \"final_results\": minimize_result_differential_evolution,\n",
    "#         \"steps_results\": iteration_logs_differential_evolution,\n",
    "#     }\n",
    "# except Exception as e:\n",
    "#     print(f\"Error with method {method}: {e}\")"
   ],
   "id": "1de98a8c72a9839d",
   "outputs": [],
   "execution_count": 113
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
