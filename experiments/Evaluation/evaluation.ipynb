{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:39:50.315536Z",
     "start_time": "2025-03-27T14:39:48.746038Z"
    }
   },
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "from timeit import timeit\n",
    "from flopco import FlopCo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from src.model_compressor.model_compressor import compress_model\n",
    "\n",
    "from torchvision.models import (resnet18, ResNet18_Weights,\n",
    "                                resnet34, ResNet34_Weights,\n",
    "                                resnet50, ResNet50_Weights,\n",
    "                                resnet101, ResNet101_Weights,\n",
    "                                resnet152, ResNet152_Weights,\n",
    "                                vgg11, VGG11_Weights,\n",
    "                                vgg13, VGG13_Weights,\n",
    "                                vgg16, VGG16_Weights,\n",
    "                                vgg19, VGG19_Weights,\n",
    "                                mobilenet_v2, MobileNet_V2_Weights,\n",
    "                                mobilenet_v3_large, MobileNet_V3_Large_Weights,\n",
    "                                mobilenet_v3_small, MobileNet_V3_Small_Weights,\n",
    "                                efficientnet_b0, EfficientNet_B0_Weights,\n",
    "                                efficientnet_b1, EfficientNet_B1_Weights,\n",
    "                                efficientnet_b2, EfficientNet_B2_Weights,\n",
    "                                efficientnet_b3, EfficientNet_B3_Weights,\n",
    "                                efficientnet_b4, EfficientNet_B4_Weights\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:39:50.350646Z",
     "start_time": "2025-03-27T14:39:50.318799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parameters for compression\n",
    "compression_params = {\n",
    "    'conv_compression_method': 'TKDCPD',\n",
    "    'conv_transpose_compression_method': 'TKDCPD',\n",
    "    'finetune': True,\n",
    "    'optimizer': AdamW,\n",
    "    'data_size': [3, 224, 224],\n",
    "    'lr': 0.001,\n",
    "    'loss_function': CrossEntropyLoss,\n",
    "    'finetune_device': device,\n",
    "    'task': 'classification',\n",
    "}\n",
    "\n",
    "# Functions to load models\n",
    "models = [\n",
    "    (resnet18, ResNet18_Weights.IMAGENET1K_V1),\n",
    "    (resnet34, ResNet34_Weights.IMAGENET1K_V1),\n",
    "    (resnet50, ResNet50_Weights.IMAGENET1K_V1),\n",
    "    (resnet101, ResNet101_Weights.IMAGENET1K_V1),\n",
    "    (resnet152, ResNet152_Weights.IMAGENET1K_V1),\n",
    "    (vgg11, VGG11_Weights.IMAGENET1K_V1),\n",
    "    (vgg13, VGG13_Weights.IMAGENET1K_V1),\n",
    "    (vgg16, VGG16_Weights.IMAGENET1K_V1),\n",
    "    (vgg19, VGG19_Weights.IMAGENET1K_V1),\n",
    "    (mobilenet_v2, MobileNet_V2_Weights.IMAGENET1K_V1),\n",
    "    (mobilenet_v3_large, MobileNet_V3_Large_Weights.IMAGENET1K_V1),\n",
    "    (mobilenet_v3_small, MobileNet_V3_Small_Weights.IMAGENET1K_V1),\n",
    "    (efficientnet_b0, EfficientNet_B0_Weights.IMAGENET1K_V1),\n",
    "    (efficientnet_b1, EfficientNet_B1_Weights.IMAGENET1K_V1),\n",
    "    (efficientnet_b2, EfficientNet_B2_Weights.IMAGENET1K_V1),\n",
    "    (efficientnet_b3, EfficientNet_B3_Weights.IMAGENET1K_V1),\n",
    "    (efficientnet_b4, EfficientNet_B4_Weights.IMAGENET1K_V1),\n",
    "]"
   ],
   "id": "36ad6d490d15b379",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:39:50.400723Z",
     "start_time": "2025-03-27T14:39:50.397145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def performance_test(model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=True):\n",
    "            model(input_tensor)\n",
    "\n",
    "def evaluate_compressed_model(original_model, compressed_model):\n",
    "    # Set models to evaluation mode\n",
    "    model1 = original_model.to(device).eval()\n",
    "    model2 = compressed_model.to(device).eval()\n",
    "\n",
    "    # Initialize counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(1000), desc=\"Evaluating models\"):\n",
    "            # Move data to device\n",
    "            images = torch.randn(64, 3, 224, 224).to(device)\n",
    "\n",
    "            # Get predictions from both models\n",
    "            outputs1 = model1(images)\n",
    "            outputs2 = model2(images)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds1 = torch.max(outputs1, 1)\n",
    "            _, preds2 = torch.max(outputs2, 1)\n",
    "\n",
    "            correct += (preds1 == preds2).sum().item()\n",
    "            total += preds1.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    flopco_stats_original = FlopCo(model1, device=device)\n",
    "    flopco_stats_compressed = FlopCo(model2, device=device)\n",
    "\n",
    "    print(f'FLOPs: {flopco_stats_original.total_flops}')\n",
    "    print(f'Parameters: {flopco_stats_original.total_params}')\n",
    "    print(f'Total macs: {flopco_stats_original.total_macs}')\n",
    "\n",
    "    print(f'FLOPs (compressed): {flopco_stats_compressed.total_flops}')\n",
    "    print(f'Parameters (compressed): {flopco_stats_compressed.total_params}')\n",
    "    print(f'Total macs (compressed): {flopco_stats_compressed.total_macs}')\n",
    "\n",
    "    print(f'Compression Ratio: {flopco_stats_original.total_params / flopco_stats_compressed.total_params:.2f}')\n",
    "    print(f'Compression Ratio (FLOPs): {flopco_stats_original.total_flops / flopco_stats_compressed.total_flops:.2f}')\n",
    "    print(f'Compression Ratio (macs): {flopco_stats_original.total_macs / flopco_stats_compressed.total_macs:.2f}')\n",
    "\n",
    "\n",
    "    # Measure performance\n",
    "    input_tensor = torch.randn(64, 3, 224, 224).to(device)\n",
    "    time_original = timeit(lambda: performance_test(model1, input_tensor), number=1000)\n",
    "    time_compressed = timeit(lambda: performance_test(model2, input_tensor), number=1000)\n",
    "\n",
    "    return accuracy, flopco_stats_original, flopco_stats_compressed, time_original, time_compressed"
   ],
   "id": "d6d62a773ffa9920",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:46:53.504740Z",
     "start_time": "2025-03-27T14:40:28.297716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model_func, weights in models:\n",
    "    print(f\"Loading model: {model_func.__name__}\")\n",
    "    original_model = model_func(weights=weights)\n",
    "    compressed_model = deepcopy(original_model)\n",
    "    compress_model(compressed_model, **compression_params)\n",
    "\n",
    "    accuracy, stats_original, stats_compressed, time_original, time_compressed = evaluate_compressed_model(original_model, compressed_model)\n",
    "    results = {\n",
    "        'model': model_func.__name__,\n",
    "        'accuracy': accuracy,\n",
    "        'time_original': time_original,\n",
    "        'time_compressed': time_compressed,\n",
    "        'flopco_stats_original': {\n",
    "            'total_flops': stats_original.total_flops,\n",
    "            'total_params': stats_original.total_params,\n",
    "            'total_macs': stats_original.total_macs,\n",
    "            'flops': stats_original.flops,\n",
    "            'macs': stats_original.macs,\n",
    "            'params': stats_original.params,\n",
    "            'relative_flops': stats_original.relative_flops,\n",
    "            'relative_macs': stats_original.relative_macs,\n",
    "            'relative_params': stats_original.relative_params,\n",
    "        },\n",
    "        'flopco_stats_compressed': {\n",
    "            'total_flops': stats_compressed.total_flops,\n",
    "            'total_params': stats_compressed.total_params,\n",
    "            'total_macs': stats_compressed.total_macs,\n",
    "            'flops': stats_compressed.flops,\n",
    "            'macs': stats_compressed.macs,\n",
    "            'params': stats_compressed.params,\n",
    "            'relative_flops': stats_compressed.relative_flops,\n",
    "            'relative_macs': stats_compressed.relative_macs,\n",
    "            'relative_params': stats_compressed.relative_params,\n",
    "        },\n",
    "        'compression_ratio': {\n",
    "            'params': stats_original.total_params / stats_compressed.total_params,\n",
    "            'flops': stats_original.total_flops / stats_compressed.total_flops,\n",
    "            'macs': stats_original.total_macs / stats_compressed.total_macs,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Append results to a JSON file\n",
    "    try:\n",
    "        with open('./results2.json', 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        existing_results = []\n",
    "\n",
    "    existing_results.append(results)\n",
    "\n",
    "    with open('./results2.json', 'w+') as f:\n",
    "        json.dump(existing_results, f, indent=4)\n",
    "\n",
    "    del compressed_model, original_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "41b35665d663a217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mobilenet_v3_large\n",
      "differential_evolution step 1: f(x)= 0.18263319027386715\n",
      "differential_evolution step 2: f(x)= 0.18263319027386715\n",
      "differential_evolution step 3: f(x)= 0.18263319027386715\n",
      "differential_evolution step 4: f(x)= 0.18263319027386715\n",
      "differential_evolution step 5: f(x)= 0.18263319027386715\n",
      "differential_evolution step 6: f(x)= 0.18263319027386715\n",
      "differential_evolution step 7: f(x)= 0.18263319027386715\n",
      "Polishing solution with 'L-BFGS-B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksandr-vashchenko/Desktop/Thesis/tensor-compression-methods/src/model_compressor/model_compressor.py:693: UserWarning: Depthwise convolution is not supported. This layer will not be compressed.\n",
      "  warn(\"Depthwise convolution is not supported. This layer will not be compressed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= 0.5754669308662415\n",
      "differential_evolution step 2: f(x)= 0.5754669308662415\n",
      "differential_evolution step 3: f(x)= 0.5754669308662415\n",
      "differential_evolution step 4: f(x)= 0.5754669308662415\n",
      "differential_evolution step 5: f(x)= 0.5754669308662415\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6084460020065308\n",
      "differential_evolution step 2: f(x)= 0.5342286825180054\n",
      "differential_evolution step 3: f(x)= 0.5342286825180054\n",
      "differential_evolution step 4: f(x)= 0.5342286825180054\n",
      "differential_evolution step 5: f(x)= 0.5342286825180054\n",
      "differential_evolution step 6: f(x)= 0.5342286825180054\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6732354164123535\n",
      "differential_evolution step 2: f(x)= 0.6732354164123535\n",
      "differential_evolution step 3: f(x)= 0.6259453892707825\n",
      "differential_evolution step 4: f(x)= 0.6259453892707825\n",
      "differential_evolution step 5: f(x)= 0.6259453892707825\n",
      "differential_evolution step 6: f(x)= 0.6259453892707825\n",
      "differential_evolution step 7: f(x)= 0.6259453892707825\n",
      "differential_evolution step 8: f(x)= 0.6259453892707825\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7382282842824488\n",
      "differential_evolution step 2: f(x)= 0.6806642345276716\n",
      "differential_evolution step 3: f(x)= 0.5049373209476471\n",
      "differential_evolution step 4: f(x)= 0.5049373209476471\n",
      "differential_evolution step 5: f(x)= 0.5049373209476471\n",
      "differential_evolution step 6: f(x)= 0.5049373209476471\n",
      "differential_evolution step 7: f(x)= 0.5049373209476471\n",
      "differential_evolution step 8: f(x)= 0.5049373209476471\n",
      "differential_evolution step 9: f(x)= 0.5049373209476471\n",
      "differential_evolution step 10: f(x)= 0.5049373209476471\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6002081057008238\n",
      "differential_evolution step 2: f(x)= 0.6002081057008238\n",
      "differential_evolution step 3: f(x)= 0.5967787504196167\n",
      "differential_evolution step 4: f(x)= 0.5720537900924683\n",
      "differential_evolution step 5: f(x)= 0.5720537900924683\n",
      "differential_evolution step 6: f(x)= 0.5720537900924683\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6805979197407946\n",
      "differential_evolution step 2: f(x)= 0.6325178146362305\n",
      "differential_evolution step 3: f(x)= 0.6325178146362305\n",
      "differential_evolution step 4: f(x)= 0.6325178146362305\n",
      "differential_evolution step 5: f(x)= 0.6325178146362305\n",
      "differential_evolution step 6: f(x)= 0.6325178146362305\n",
      "differential_evolution step 7: f(x)= 0.6325178146362305\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 2: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 3: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 4: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 5: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 6: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 7: f(x)= 0.0004825115773195457\n",
      "differential_evolution step 8: f(x)= 0.0004101307263856825\n",
      "differential_evolution step 9: f(x)= 0.0004101307263856825\n",
      "differential_evolution step 10: f(x)= 0.0004101307263856825\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.0034298109885012654\n",
      "differential_evolution step 2: f(x)= 0.001209414868730609\n",
      "differential_evolution step 3: f(x)= 0.001209414868730609\n",
      "differential_evolution step 4: f(x)= 0.00048268854245009515\n",
      "differential_evolution step 5: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 6: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 7: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 8: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 9: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 10: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 11: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 12: f(x)= 4.351668110302853e-07\n",
      "differential_evolution step 13: f(x)= 4.351668110302853e-07\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6793035493956672\n",
      "differential_evolution step 2: f(x)= 0.6754297918743557\n",
      "differential_evolution step 3: f(x)= 0.6540172460638447\n",
      "differential_evolution step 4: f(x)= 0.6540172460638447\n",
      "differential_evolution step 5: f(x)= 0.6447386966811286\n",
      "differential_evolution step 6: f(x)= 0.6447386966811286\n",
      "differential_evolution step 7: f(x)= 0.6447386966811286\n",
      "differential_evolution step 8: f(x)= 0.6447386966811286\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5492504841751522\n",
      "differential_evolution step 2: f(x)= 0.5492504841751522\n",
      "differential_evolution step 3: f(x)= 0.41007976741260954\n",
      "differential_evolution step 4: f(x)= 0.3896441996097565\n",
      "differential_evolution step 5: f(x)= 0.3896441996097565\n",
      "differential_evolution step 6: f(x)= 0.3821057379245758\n",
      "differential_evolution step 7: f(x)= 0.3821057379245758\n",
      "differential_evolution step 8: f(x)= 0.3821057379245758\n",
      "differential_evolution step 9: f(x)= 0.3821057379245758\n",
      "differential_evolution step 10: f(x)= 0.3821057379245758\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.49529472258355883\n",
      "differential_evolution step 2: f(x)= 0.49529472258355883\n",
      "differential_evolution step 3: f(x)= 0.32230400443077084\n",
      "differential_evolution step 4: f(x)= 0.32230400443077084\n",
      "differential_evolution step 5: f(x)= 0.32230400443077084\n",
      "differential_evolution step 6: f(x)= 0.32230400443077084\n",
      "differential_evolution step 7: f(x)= 0.32230400443077084\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.37884771890110447\n",
      "differential_evolution step 2: f(x)= 0.3521116077899933\n",
      "differential_evolution step 3: f(x)= 0.3422631025314331\n",
      "differential_evolution step 4: f(x)= 0.3422631025314331\n",
      "differential_evolution step 5: f(x)= 0.3422631025314331\n",
      "differential_evolution step 6: f(x)= 0.3422631025314331\n",
      "differential_evolution step 7: f(x)= 0.3422631025314331\n",
      "differential_evolution step 8: f(x)= 0.3422631025314331\n",
      "differential_evolution step 9: f(x)= 0.3422631025314331\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.4970022704601288\n",
      "differential_evolution step 2: f(x)= 0.4937805758582221\n",
      "differential_evolution step 3: f(x)= 0.48172545433044434\n",
      "differential_evolution step 4: f(x)= 0.48172545433044434\n",
      "differential_evolution step 5: f(x)= 0.48172545433044434\n",
      "differential_evolution step 6: f(x)= 0.48172545433044434\n",
      "differential_evolution step 7: f(x)= 0.48172545433044434\n",
      "differential_evolution step 8: f(x)= 0.48172545433044434\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5597969651222229\n",
      "differential_evolution step 2: f(x)= 0.530942440032959\n",
      "differential_evolution step 3: f(x)= 0.530942440032959\n",
      "differential_evolution step 4: f(x)= 0.530942440032959\n",
      "differential_evolution step 5: f(x)= 0.530942440032959\n",
      "differential_evolution step 6: f(x)= 0.530942440032959\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.26816102431880107\n",
      "differential_evolution step 2: f(x)= 0.19775444521928875\n",
      "differential_evolution step 3: f(x)= 0.04943890120526362\n",
      "differential_evolution step 4: f(x)= 0.04943890120526362\n",
      "differential_evolution step 5: f(x)= 0.03433270003338862\n",
      "differential_evolution step 6: f(x)= 0.03433270003338862\n",
      "differential_evolution step 7: f(x)= 0.03433270003338862\n",
      "differential_evolution step 8: f(x)= 0.03433270003338862\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.2448401004076004\n",
      "differential_evolution step 2: f(x)= 0.17666901648044586\n",
      "differential_evolution step 3: f(x)= 0.08789104219724209\n",
      "differential_evolution step 4: f(x)= 0.08691719243176096\n",
      "differential_evolution step 5: f(x)= 0.034332692587867086\n",
      "differential_evolution step 6: f(x)= 0.034332692587867086\n",
      "differential_evolution step 7: f(x)= 0.034332692587867086\n",
      "differential_evolution step 8: f(x)= 0.034332692587867086\n",
      "differential_evolution step 9: f(x)= 0.034332692587867086\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6836752201186287\n",
      "differential_evolution step 2: f(x)= 0.650043331252204\n",
      "differential_evolution step 3: f(x)= 0.6423379805352952\n",
      "differential_evolution step 4: f(x)= 0.6423379805352952\n",
      "differential_evolution step 5: f(x)= 0.6423379805352952\n",
      "differential_evolution step 6: f(x)= 0.6423379805352952\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6663279237747193\n",
      "differential_evolution step 2: f(x)= 0.6663279237747193\n",
      "differential_evolution step 3: f(x)= 0.6395208266046312\n",
      "differential_evolution step 4: f(x)= 0.6395208266046312\n",
      "differential_evolution step 5: f(x)= 0.6395208266046312\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7654786477088928\n",
      "differential_evolution step 2: f(x)= 0.7247061598565844\n",
      "differential_evolution step 3: f(x)= 0.6704551049338446\n",
      "differential_evolution step 4: f(x)= 0.6396792969173855\n",
      "differential_evolution step 5: f(x)= 0.6390109286308289\n",
      "differential_evolution step 6: f(x)= 0.6322281250423855\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6199557328605652\n",
      "differential_evolution step 2: f(x)= 0.6199557328605652\n",
      "differential_evolution step 3: f(x)= 0.5423188481330872\n",
      "differential_evolution step 4: f(x)= 0.5422116527557372\n",
      "differential_evolution step 5: f(x)= 0.5298591527557374\n",
      "differential_evolution step 6: f(x)= 0.5298591527557374\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.608122444152832\n",
      "differential_evolution step 2: f(x)= 0.5210957894706726\n",
      "differential_evolution step 3: f(x)= 0.5210957894706726\n",
      "differential_evolution step 4: f(x)= 0.5153751644706727\n",
      "differential_evolution step 5: f(x)= 0.5153751644706727\n",
      "differential_evolution step 6: f(x)= 0.5153751644706727\n",
      "differential_evolution step 7: f(x)= 0.5153751644706727\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7738402866010179\n",
      "differential_evolution step 2: f(x)= 0.5859792549127892\n",
      "differential_evolution step 3: f(x)= 0.537280063548025\n",
      "differential_evolution step 4: f(x)= 0.5340354392569097\n",
      "differential_evolution step 5: f(x)= 0.5317557473263803\n",
      "differential_evolution step 6: f(x)= 0.5317557473263803\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5599654670023062\n",
      "differential_evolution step 2: f(x)= 0.5464079426907862\n",
      "differential_evolution step 3: f(x)= 0.5239793012182744\n",
      "differential_evolution step 4: f(x)= 0.5239793012182744\n",
      "differential_evolution step 5: f(x)= 0.5188301130594062\n",
      "differential_evolution step 6: f(x)= 0.5188301130594062\n",
      "differential_evolution step 7: f(x)= 0.5188301130594062\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6420254811447375\n",
      "differential_evolution step 2: f(x)= 0.5673023743980999\n",
      "differential_evolution step 3: f(x)= 0.5607868205847497\n",
      "differential_evolution step 4: f(x)= 0.5179004092297617\n",
      "differential_evolution step 5: f(x)= 0.5154713317216681\n",
      "differential_evolution step 6: f(x)= 0.5154713317216681\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5480465166284817\n",
      "differential_evolution step 2: f(x)= 0.5480465166284817\n",
      "differential_evolution step 3: f(x)= 0.54724681568957\n",
      "differential_evolution step 4: f(x)= 0.54724681568957\n",
      "differential_evolution step 5: f(x)= 0.54724681568957\n",
      "differential_evolution step 6: f(x)= 0.54724681568957\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5918577966690064\n",
      "differential_evolution step 2: f(x)= 0.5918577966690064\n",
      "differential_evolution step 3: f(x)= 0.5918577966690064\n",
      "differential_evolution step 4: f(x)= 0.5875517354011536\n",
      "differential_evolution step 5: f(x)= 0.5875517354011536\n",
      "differential_evolution step 6: f(x)= 0.5875517354011536\n",
      "differential_evolution step 7: f(x)= 0.5875517354011536\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5693386269675361\n",
      "differential_evolution step 2: f(x)= 0.5655215792420469\n",
      "differential_evolution step 3: f(x)= 0.4230973422527313\n",
      "differential_evolution step 4: f(x)= 0.40944962817945596\n",
      "differential_evolution step 5: f(x)= 0.40944962817945596\n",
      "differential_evolution step 6: f(x)= 0.4035885903835297\n",
      "differential_evolution step 7: f(x)= 0.4035885903835297\n",
      "differential_evolution step 8: f(x)= 0.4035885903835297\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.539186162289278\n",
      "differential_evolution step 2: f(x)= 0.45773903009037914\n",
      "differential_evolution step 3: f(x)= 0.4356395330693986\n",
      "differential_evolution step 4: f(x)= 0.4329690668347441\n",
      "differential_evolution step 5: f(x)= 0.42975016109443004\n",
      "differential_evolution step 6: f(x)= 0.42975016109443004\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6292940178695989\n",
      "differential_evolution step 2: f(x)= 0.6292940178695989\n",
      "differential_evolution step 3: f(x)= 0.6292940178695989\n",
      "differential_evolution step 4: f(x)= 0.6212718564636853\n",
      "differential_evolution step 5: f(x)= 0.6207072734832764\n",
      "differential_evolution step 6: f(x)= 0.6207072734832764\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5888881165005176\n",
      "differential_evolution step 2: f(x)= 0.5888881165005176\n",
      "differential_evolution step 3: f(x)= 0.5488838200666467\n",
      "differential_evolution step 4: f(x)= 0.5444756283555514\n",
      "differential_evolution step 5: f(x)= 0.5366037423190649\n",
      "differential_evolution step 6: f(x)= 0.5366037423190649\n",
      "differential_evolution step 7: f(x)= 0.5366037423190649\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6329725865846848\n",
      "differential_evolution step 2: f(x)= 0.49663298564968084\n",
      "differential_evolution step 3: f(x)= 0.4453683318023332\n",
      "differential_evolution step 4: f(x)= 0.4382315237574865\n",
      "differential_evolution step 5: f(x)= 0.4298694793177509\n",
      "differential_evolution step 6: f(x)= 0.4146842479407911\n",
      "differential_evolution step 7: f(x)= 0.4146842479407911\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.4469816202857316\n",
      "differential_evolution step 2: f(x)= 0.4285555247259998\n",
      "differential_evolution step 3: f(x)= 0.4113737040474056\n",
      "differential_evolution step 4: f(x)= 0.40968368646503733\n",
      "differential_evolution step 5: f(x)= 0.407246142731851\n",
      "differential_evolution step 6: f(x)= 0.407246142731851\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5944830083252352\n",
      "differential_evolution step 2: f(x)= 0.5711641357599202\n",
      "differential_evolution step 3: f(x)= 0.5711641357599202\n",
      "differential_evolution step 4: f(x)= 0.5586994225558813\n",
      "differential_evolution step 5: f(x)= 0.5586994225558813\n",
      "differential_evolution step 6: f(x)= 0.5586994225558813\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5605143776285946\n",
      "differential_evolution step 2: f(x)= 0.5555299882688208\n",
      "differential_evolution step 3: f(x)= 0.5555299882688208\n",
      "differential_evolution step 4: f(x)= 0.5555299882688208\n",
      "differential_evolution step 5: f(x)= 0.5555299882688208\n",
      "differential_evolution step 6: f(x)= 0.5555299882688208\n",
      "differential_evolution step 7: f(x)= 0.5555299882688208\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5784898437850569\n",
      "differential_evolution step 2: f(x)= 0.563262839030787\n",
      "differential_evolution step 3: f(x)= 0.563262839030787\n",
      "differential_evolution step 4: f(x)= 0.5532908918040117\n",
      "differential_evolution step 5: f(x)= 0.5515675040786664\n",
      "differential_evolution step 6: f(x)= 0.5499308339477926\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7384689511575412\n",
      "differential_evolution step 2: f(x)= 0.6225172531252253\n",
      "differential_evolution step 3: f(x)= 0.5859266179307121\n",
      "differential_evolution step 4: f(x)= 0.5514830230735477\n",
      "differential_evolution step 5: f(x)= 0.5460637148744312\n",
      "differential_evolution step 6: f(x)= 0.5447914076814647\n",
      "differential_evolution step 7: f(x)= 0.5447914076814647\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7285322004164698\n",
      "differential_evolution step 2: f(x)= 0.7285322004164698\n",
      "differential_evolution step 3: f(x)= 0.6725738219393084\n",
      "differential_evolution step 4: f(x)= 0.6648101594983316\n",
      "differential_evolution step 5: f(x)= 0.6648101594983316\n",
      "differential_evolution step 6: f(x)= 0.6613848805427551\n",
      "differential_evolution step 7: f(x)= 0.6613848805427551\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.668493902683258\n",
      "differential_evolution step 2: f(x)= 0.6069936990737914\n",
      "differential_evolution step 3: f(x)= 0.6069936990737914\n",
      "differential_evolution step 4: f(x)= 0.586544105052948\n",
      "differential_evolution step 5: f(x)= 0.586544105052948\n",
      "differential_evolution step 6: f(x)= 0.5854064572122362\n",
      "differential_evolution step 7: f(x)= 0.5854064572122362\n",
      "differential_evolution step 8: f(x)= 0.5854064572122362\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.5978671369552612\n",
      "differential_evolution step 2: f(x)= 0.5798732508847743\n",
      "differential_evolution step 3: f(x)= 0.573656165258384\n",
      "differential_evolution step 4: f(x)= 0.548403094874488\n",
      "differential_evolution step 5: f(x)= 0.5477270310307726\n",
      "differential_evolution step 6: f(x)= 0.5458373170546543\n",
      "differential_evolution step 7: f(x)= 0.5418394479751587\n",
      "differential_evolution step 8: f(x)= 0.5416345356423178\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6171041418181525\n",
      "differential_evolution step 2: f(x)= 0.6171041418181525\n",
      "differential_evolution step 3: f(x)= 0.6083480358123778\n",
      "differential_evolution step 4: f(x)= 0.5769039245475958\n",
      "differential_evolution step 5: f(x)= 0.5405791537261303\n",
      "differential_evolution step 6: f(x)= 0.5377474187332907\n",
      "differential_evolution step 7: f(x)= 0.5377474187332907\n",
      "differential_evolution step 8: f(x)= 0.536916510982278\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7142866334385342\n",
      "differential_evolution step 2: f(x)= 0.6582958031760321\n",
      "differential_evolution step 3: f(x)= 0.6464220523834229\n",
      "differential_evolution step 4: f(x)= 0.6266165018081665\n",
      "differential_evolution step 5: f(x)= 0.6134363946914673\n",
      "differential_evolution step 6: f(x)= 0.6109352672364977\n",
      "differential_evolution step 7: f(x)= 0.6088115320205689\n",
      "differential_evolution step 8: f(x)= 0.6088115320205689\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7834477659331427\n",
      "differential_evolution step 2: f(x)= 0.5959821343421936\n",
      "differential_evolution step 3: f(x)= 0.5959821343421936\n",
      "differential_evolution step 4: f(x)= 0.5819805345535278\n",
      "differential_evolution step 5: f(x)= 0.5785479173660278\n",
      "differential_evolution step 6: f(x)= 0.5785479173660278\n",
      "differential_evolution step 7: f(x)= 0.5785479173660278\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7059771409858892\n",
      "differential_evolution step 2: f(x)= 0.6660011895968591\n",
      "differential_evolution step 3: f(x)= 0.6467905464172363\n",
      "differential_evolution step 4: f(x)= 0.6015075227360667\n",
      "differential_evolution step 5: f(x)= 0.5830023884773254\n",
      "differential_evolution step 6: f(x)= 0.5803293896781074\n",
      "differential_evolution step 7: f(x)= 0.5768805153575944\n",
      "differential_evolution step 8: f(x)= 0.5768805153575944\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.564993492603302\n",
      "differential_evolution step 2: f(x)= 0.557662785789113\n",
      "differential_evolution step 3: f(x)= 0.557662785789113\n",
      "differential_evolution step 4: f(x)= 0.557662785789113\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.6624116606182522\n",
      "differential_evolution step 2: f(x)= 0.6624116606182522\n",
      "differential_evolution step 3: f(x)= 0.6518140435218811\n",
      "differential_evolution step 4: f(x)= 0.6518140435218811\n",
      "differential_evolution step 5: f(x)= 0.651642382144928\n",
      "differential_evolution step 6: f(x)= 0.651642382144928\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7070848021507263\n",
      "differential_evolution step 2: f(x)= 0.6627515463299222\n",
      "differential_evolution step 3: f(x)= 0.6379230699009365\n",
      "differential_evolution step 4: f(x)= 0.6379230699009365\n",
      "differential_evolution step 5: f(x)= 0.6347092137336732\n",
      "differential_evolution step 6: f(x)= 0.6344170794487\n",
      "Polishing solution with 'L-BFGS-B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models: 100%|██████████| 1000/1000 [01:45<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.16%\n",
      "FLOPs: 433186840\n",
      "Parameters: 5458632\n",
      "Total macs: 216589760\n",
      "FLOPs (compressed): 526543384\n",
      "Parameters (compressed): 6105181\n",
      "Total macs (compressed): 263267008\n",
      "Compression Ratio: 0.89\n",
      "Compression Ratio (FLOPs): 0.82\n",
      "Compression Ratio (macs): 0.82\n",
      "Loading model: mobilenet_v3_small\n",
      "differential_evolution step 1: f(x)= 0.2091097671981912\n",
      "differential_evolution step 2: f(x)= 0.2091097671981912\n",
      "differential_evolution step 3: f(x)= 0.2091097671981912\n",
      "differential_evolution step 4: f(x)= 0.2091097671981912\n",
      "differential_evolution step 5: f(x)= 0.2091097671981912\n",
      "differential_evolution step 6: f(x)= 0.2091097671981912\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.0024475371824337344\n",
      "differential_evolution step 2: f(x)= 0.0024475371824337344\n",
      "differential_evolution step 3: f(x)= 0.0024475371824337344\n",
      "differential_evolution step 4: f(x)= 0.0024475371824337344\n",
      "differential_evolution step 5: f(x)= 0.0024475371824337344\n",
      "differential_evolution step 6: f(x)= 0.0024475371824337344\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 2: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 3: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 4: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 5: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 6: f(x)= 0.0024446676000025036\n",
      "differential_evolution step 7: f(x)= 0.0024446676000025036\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.8143400549888611\n",
      "differential_evolution step 2: f(x)= 0.8143400549888611\n",
      "differential_evolution step 3: f(x)= 0.8143400549888611\n",
      "differential_evolution step 4: f(x)= 0.8143400549888611\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "differential_evolution step 1: f(x)= 0.7782817482948303\n",
      "differential_evolution step 2: f(x)= 0.7141948342323303\n",
      "differential_evolution step 3: f(x)= 0.649594869878557\n",
      "differential_evolution step 4: f(x)= 0.6492633208816434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m original_model \u001B[38;5;241m=\u001B[39m model_func(weights\u001B[38;5;241m=\u001B[39mweights)\n\u001B[1;32m      4\u001B[0m compressed_model \u001B[38;5;241m=\u001B[39m deepcopy(original_model)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mcompress_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompressed_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcompression_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m accuracy, stats_original, stats_compressed, time_original, time_compressed \u001B[38;5;241m=\u001B[39m evaluate_compressed_model(original_model, compressed_model)\n\u001B[1;32m      8\u001B[0m results \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model_func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m: accuracy,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m     }\n\u001B[1;32m     40\u001B[0m }\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/model_compressor.py:754\u001B[0m, in \u001B[0;36mcompress_model\u001B[0;34m(model, layers, conv_compression_method, conv_transpose_compression_method, linear_compress_method, target_compression_ratio, frobenius_error_coef, compression_ratio_coef, rank_cpd, rank_tkd, finetune, epochs, optimizer, data_size, lr, loss_function, batch_size, number_of_iterations, finetune_device, task)\u001B[0m\n\u001B[1;32m    752\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(model, name, FactorizedLinear\u001B[38;5;241m.\u001B[39mfrom_linear(child, factorization\u001B[38;5;241m=\u001B[39mlinear_compress_method))\n\u001B[1;32m    753\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 754\u001B[0m         \u001B[43mcompress_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    756\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    757\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    758\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_transpose_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    759\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlinear_compress_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    760\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    761\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    763\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_cpd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_tkd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m finetune:\n\u001B[1;32m    768\u001B[0m     optim \u001B[38;5;241m=\u001B[39m optimizer(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/model_compressor.py:754\u001B[0m, in \u001B[0;36mcompress_model\u001B[0;34m(model, layers, conv_compression_method, conv_transpose_compression_method, linear_compress_method, target_compression_ratio, frobenius_error_coef, compression_ratio_coef, rank_cpd, rank_tkd, finetune, epochs, optimizer, data_size, lr, loss_function, batch_size, number_of_iterations, finetune_device, task)\u001B[0m\n\u001B[1;32m    752\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(model, name, FactorizedLinear\u001B[38;5;241m.\u001B[39mfrom_linear(child, factorization\u001B[38;5;241m=\u001B[39mlinear_compress_method))\n\u001B[1;32m    753\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 754\u001B[0m         \u001B[43mcompress_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    756\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    757\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    758\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_transpose_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    759\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlinear_compress_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    760\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    761\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    763\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_cpd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_tkd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m finetune:\n\u001B[1;32m    768\u001B[0m     optim \u001B[38;5;241m=\u001B[39m optimizer(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "    \u001B[0;31m[... skipping similar frames: compress_model at line 754 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/model_compressor.py:754\u001B[0m, in \u001B[0;36mcompress_model\u001B[0;34m(model, layers, conv_compression_method, conv_transpose_compression_method, linear_compress_method, target_compression_ratio, frobenius_error_coef, compression_ratio_coef, rank_cpd, rank_tkd, finetune, epochs, optimizer, data_size, lr, loss_function, batch_size, number_of_iterations, finetune_device, task)\u001B[0m\n\u001B[1;32m    752\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(model, name, FactorizedLinear\u001B[38;5;241m.\u001B[39mfrom_linear(child, factorization\u001B[38;5;241m=\u001B[39mlinear_compress_method))\n\u001B[1;32m    753\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 754\u001B[0m         \u001B[43mcompress_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    756\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    757\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    758\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconv_transpose_compression_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    759\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlinear_compress_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    760\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    761\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    763\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_cpd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrank_tkd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m finetune:\n\u001B[1;32m    768\u001B[0m     optim \u001B[38;5;241m=\u001B[39m optimizer(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/model_compressor.py:704\u001B[0m, in \u001B[0;36mcompress_model\u001B[0;34m(model, layers, conv_compression_method, conv_transpose_compression_method, linear_compress_method, target_compression_ratio, frobenius_error_coef, compression_ratio_coef, rank_cpd, rank_tkd, finetune, epochs, optimizer, data_size, lr, loss_function, batch_size, number_of_iterations, finetune_device, task)\u001B[0m\n\u001B[1;32m    700\u001B[0m method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferential_evolution\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     reconstructed_tensor, weight, factors, optimal_rank, final_loss_value, optimize_result, iteration_logs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 704\u001B[0m         \u001B[43mglobal_optimize_tucker_rank\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m            \u001B[49m\u001B[43moptimization_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    706\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    707\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    708\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    710\u001B[0m \u001B[43m            \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m     )\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child\u001B[38;5;241m.\u001B[39mgroups \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    714\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/calculate_optimized_rank_for_nn_layer.py:300\u001B[0m, in \u001B[0;36mglobal_optimize_tucker_rank\u001B[0;34m(tensor, target_compression_ratio, tucker_args, frobenius_error_coef, compression_ratio_coef, optimization_method, loss_function_fixed, verbose)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m tl\u001B[38;5;241m.\u001B[39mget_backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    293\u001B[0m     optimization_kwargs_differential_evolution\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m    294\u001B[0m         {\n\u001B[1;32m    295\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworkers\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m    296\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdating\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeferred\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    297\u001B[0m         }\n\u001B[1;32m    298\u001B[0m     )\n\u001B[0;32m--> 300\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mdifferential_evolution\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptimization_kwargs_differential_evolution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m optimal_rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39mclip(np\u001B[38;5;241m.\u001B[39mround(result\u001B[38;5;241m.\u001B[39mx)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m), \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    303\u001B[0m final_loss \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mfun\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/scipy/optimize/_differentialevolution.py:503\u001B[0m, in \u001B[0;36mdifferential_evolution\u001B[0;34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, seed, callback, disp, polish, init, atol, updating, workers, constraints, x0, integrality, vectorized)\u001B[0m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;66;03m# using a context manager means that any created Pool objects are\u001B[39;00m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;66;03m# cleared up.\u001B[39;00m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m DifferentialEvolutionSolver(func, bounds, args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m    489\u001B[0m                                  strategy\u001B[38;5;241m=\u001B[39mstrategy,\n\u001B[1;32m    490\u001B[0m                                  maxiter\u001B[38;5;241m=\u001B[39mmaxiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    501\u001B[0m                                  integrality\u001B[38;5;241m=\u001B[39mintegrality,\n\u001B[1;32m    502\u001B[0m                                  vectorized\u001B[38;5;241m=\u001B[39mvectorized) \u001B[38;5;28;01mas\u001B[39;00m solver:\n\u001B[0;32m--> 503\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43msolver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/scipy/optimize/_differentialevolution.py:1168\u001B[0m, in \u001B[0;36mDifferentialEvolutionSolver.solve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1165\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m nit \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxiter \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1166\u001B[0m     \u001B[38;5;66;03m# evolve the population by a generation\u001B[39;00m\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1168\u001B[0m         \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1169\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m   1170\u001B[0m         warning_flag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/scipy/optimize/_differentialevolution.py:1582\u001B[0m, in \u001B[0;36mDifferentialEvolutionSolver.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1580\u001B[0m     feasible \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1581\u001B[0m     cv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39matleast_2d([\u001B[38;5;241m0.\u001B[39m])\n\u001B[0;32m-> 1582\u001B[0m     energy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1583\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nfev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1585\u001B[0m \u001B[38;5;66;03m# compare trial and population member\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/scipy/_lib/_util.py:441\u001B[0m, in \u001B[0;36m_FunctionWrapper.__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 441\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/calculate_optimized_rank_for_nn_layer.py:106\u001B[0m, in \u001B[0;36mloss_tucker_wrapper\u001B[0;34m(tucker_rank, tensor, target_compression_ratio, frobenius_error_coef, compression_ratio_coef, tucker_args)\u001B[0m\n\u001B[1;32m    104\u001B[0m tucker_rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39mround(tucker_rank)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m))\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 106\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_function_tucker\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtucker_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_compression_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtucker_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtucker_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrobenius_error_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression_ratio_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28mprint\u001B[39m(e)\n",
      "File \u001B[0;32m~/Desktop/Thesis/tensor-compression-methods/src/model_compressor/calculate_optimized_rank_for_nn_layer.py:54\u001B[0m, in \u001B[0;36mloss_function_tucker\u001B[0;34m(rank, tensor, target_compression_ratio, tucker_args, frobenius_error_coef, compression_ratio_coef)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m tl\u001B[38;5;241m.\u001B[39mtensor(tensor)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m tl\u001B[38;5;241m.\u001B[39mget_backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m tl\u001B[38;5;241m.\u001B[39mtensor(tensor)\n\u001B[0;32m---> 54\u001B[0m     weight, factors \u001B[38;5;241m=\u001B[39m \u001B[43mtl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecomposition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtucker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtucker_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     reconstructed_tensor \u001B[38;5;241m=\u001B[39m tl\u001B[38;5;241m.\u001B[39mtucker_to_tensor((weight, factors))\n\u001B[1;32m     57\u001B[0m     frobenius_error \u001B[38;5;241m=\u001B[39m (tl\u001B[38;5;241m.\u001B[39mnorm(reconstructed_tensor \u001B[38;5;241m-\u001B[39m tensor) \u001B[38;5;241m/\u001B[39m tl\u001B[38;5;241m.\u001B[39mnorm(tensor))\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/tensorly/decomposition/_tucker.py:329\u001B[0m, in \u001B[0;36mtucker\u001B[0;34m(tensor, rank, fixed_factors, n_iter_max, init, return_errors, svd, tol, random_state, mask, verbose)\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;66;03m# TO-DO validate rank for partial tucker as well\u001B[39;00m\n\u001B[1;32m    327\u001B[0m rank \u001B[38;5;241m=\u001B[39m validate_tucker_rank(tl\u001B[38;5;241m.\u001B[39mshape(tensor), rank\u001B[38;5;241m=\u001B[39mrank)\n\u001B[0;32m--> 329\u001B[0m (core, factors), rec_errors \u001B[38;5;241m=\u001B[39m \u001B[43mpartial_tucker\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_iter_max\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_iter_max\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43msvd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msvd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    341\u001B[0m tensor \u001B[38;5;241m=\u001B[39m TuckerTensor((core, factors))\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_errors:\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/tensorly/decomposition/_tucker.py:197\u001B[0m, in \u001B[0;36mpartial_tucker\u001B[0;34m(tensor, rank, modes, n_iter_max, init, tol, svd, random_state, verbose, mask, svd_mask_repeats)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, mode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(modes):\n\u001B[1;32m    194\u001B[0m     core_approximation \u001B[38;5;241m=\u001B[39m multi_mode_dot(\n\u001B[1;32m    195\u001B[0m         tensor, factors, modes\u001B[38;5;241m=\u001B[39mmodes, skip\u001B[38;5;241m=\u001B[39mindex, transpose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    196\u001B[0m     )\n\u001B[0;32m--> 197\u001B[0m     eigenvecs, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43msvd_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43munfold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcore_approximation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_eigenvecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrank\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m     factors[index] \u001B[38;5;241m=\u001B[39m eigenvecs\n\u001B[1;32m    204\u001B[0m core \u001B[38;5;241m=\u001B[39m multi_mode_dot(tensor, factors, modes\u001B[38;5;241m=\u001B[39mmodes, transpose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/tensorly/tenalg/svd.py:427\u001B[0m, in \u001B[0;36msvd_interface\u001B[0;34m(matrix, method, n_eigenvecs, flip_sign, u_based_flip_sign, non_negative, mask, n_iter_mask_imputation, **kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    424\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot svd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmethod\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. However, the possible choices are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSVD_FUNS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or to pass a callable.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    425\u001B[0m     )\n\u001B[0;32m--> 427\u001B[0m U, S, V \u001B[38;5;241m=\u001B[39m \u001B[43msvd_fun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_eigenvecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_eigenvecs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m n_eigenvecs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_iter_mask_imputation):\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;66;03m# Workaround to avoid needing fill_diagonal\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/tensorly/tenalg/svd.py:232\u001B[0m, in \u001B[0;36mtruncated_svd\u001B[0;34m(matrix, n_eigenvecs, **kwargs)\u001B[0m\n\u001B[1;32m    230\u001B[0m n_eigenvecs, min_dim, _ \u001B[38;5;241m=\u001B[39m svd_checks(matrix, n_eigenvecs\u001B[38;5;241m=\u001B[39mn_eigenvecs)\n\u001B[1;32m    231\u001B[0m full_matrices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m n_eigenvecs \u001B[38;5;241m>\u001B[39m min_dim \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 232\u001B[0m U, S, V \u001B[38;5;241m=\u001B[39m \u001B[43mtl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msvd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_matrices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull_matrices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m U[:, :n_eigenvecs], S[:n_eigenvecs], V[:n_eigenvecs, :]\n",
      "File \u001B[0;32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/tensorly/backend/__init__.py:202\u001B[0m, in \u001B[0;36mBackendManager.dispatch_backend_method.<locals>.wrapped_backend_method\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_backend_method\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    199\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"A dynamically dispatched method\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m    Returns the queried method from the currently set backend\"\"\"\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_THREAD_LOCAL_DATA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbackend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:36:09.445176Z",
     "start_time": "2025-03-27T14:36:09.441167Z"
    }
   },
   "cell_type": "code",
   "source": "print(compressed_model)",
   "id": "b0255b3d7f0e9d68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=3, bias=False)\n",
      "          (2): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Sequential(\n",
      "            (0): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (fc2): Sequential(\n",
      "            (0): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a6b388bc87402907"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
