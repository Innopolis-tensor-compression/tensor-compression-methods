{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:06.656959Z",
     "start_time": "2024-10-02T23:50:06.619576Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pyvenv.cfg file\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T15:37:37.831777Z",
     "start_time": "2024-10-05T15:37:36.652050Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\89123\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\89123\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\89123\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\89123\\AppData\\Local\\Temp\\ipykernel_13804\\1114448699.py\", line 11, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 25, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\", line 28, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_utils\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.distribute import values as value_lib\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py\", line 23, in <module>\n",
      "    from tensorflow.python.distribute import distribute_lib\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 205, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 34, in <module>\n",
      "    from tensorflow.python.data.ops import iterator_ops\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 45, in <module>\n",
      "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 50, in <module>\n",
      "    from tensorflow.python.training import py_checkpoint_reader\n",
      "  File \"E:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 19, in <module>\n",
      "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson5\u001b[39;00m\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_autograph\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug_mode\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iterator_ops\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m options \u001b[38;5;28;01mas\u001b[39;00m options_lib\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSaverBuilder\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collections_abc\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\training\\saver.py:50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_saved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_checkpoint_reader\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_util\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saveable_object\n",
      "File \u001b[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors_impl\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_checkpoint_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckpointReader\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_translator\u001b[39m(e):\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import json5\n",
    "import numpy as np\n",
    "import t3f\n",
    "import tensorflow as tf\n",
    "import tensorly as tl\n",
    "import torch\n",
    "import yt_dlp\n",
    "from dotenv import load_dotenv\n",
    "from memory_profiler import memory_usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext memory_profiler\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.313859Z",
     "start_time": "2024-10-02T23:50:14.283539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Some class and methods for logging of compression metrics of some methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.485727Z",
     "start_time": "2024-10-02T23:50:14.474209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MethodLogger:\n",
    "    experiments_count = 1\n",
    "\n",
    "    def __init__(self, method_name: str, method_input_tensor: np.ndarray, qualitative_metrics: dict[str, str], func, method_args: dict[str, Any]):\n",
    "        self.name = method_name\n",
    "        self.method_args = method_args\n",
    "        self.qualitative_metrics = qualitative_metrics\n",
    "\n",
    "        self.method_input_tensor = method_input_tensor\n",
    "\n",
    "        self.func = func\n",
    "\n",
    "        gpu_allocated_mem_usages, gpu_cached_mem_usages, ram_mem_usages, durations, result = self._run_method_with_tracking(func, **method_args)\n",
    "\n",
    "        self.method_result = result\n",
    "\n",
    "        self.quantitative_metrics = {\n",
    "            \"gpu_allocated_mem_usage\": gpu_allocated_mem_usages,\n",
    "            \"gpu_cached_mem_usages\": gpu_cached_mem_usages,\n",
    "            \"ram_mem_usage\": ram_mem_usages,\n",
    "            \"duration\": durations,\n",
    "        }\n",
    "        self.logs = {}\n",
    "\n",
    "        gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "    def _run_method_with_tracking(self, func, *args, **kwargs):\n",
    "        gpu_allocated_mem_usages, gpu_cached_mem_usages, ram_mem_usages, durations = [], [], [], []\n",
    "\n",
    "        def wrapper():\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_allocated_memory_before = torch.cuda.memory_allocated()\n",
    "            gpu_cached_memory_before = torch.cuda.memory_reserved()\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "\n",
    "            gpu_allocated_memory_after = torch.cuda.memory_allocated()\n",
    "            gpu_cached_memory_after = torch.cuda.memory_reserved()\n",
    "\n",
    "            gpu_allocated_memory_used = gpu_allocated_memory_after - gpu_allocated_memory_before\n",
    "            gpu_cached_memory_used = gpu_cached_memory_after - gpu_cached_memory_before\n",
    "            ram_memory_used = memory_usage((func, args, kwargs))[-1]\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            return gpu_allocated_memory_used, gpu_cached_memory_used, ram_memory_used, duration, result\n",
    "\n",
    "        for _ in tqdm(range(MethodLogger.experiments_count), desc=\"Эксперимент набора параметров\"):\n",
    "            gpu_allocated_memory_used, gpu_cached_memory_used, ram_memory_used, duration, result = wrapper()\n",
    "\n",
    "            gpu_allocated_mem_usages.append(gpu_allocated_memory_used)\n",
    "            gpu_cached_mem_usages.append(gpu_cached_memory_used)\n",
    "            ram_mem_usages.append(ram_memory_used)\n",
    "            durations.append(duration)\n",
    "\n",
    "        resulted_gpu_allocated_mem_usages = MethodLogger._compute_stats(gpu_allocated_mem_usages)\n",
    "\n",
    "        resulted_gpu_cached_mem_usages = MethodLogger._compute_stats(gpu_cached_mem_usages)\n",
    "\n",
    "        resulted_gpu_allocated_mem_usages[\"mean\"] /= 1024**2\n",
    "        resulted_gpu_allocated_mem_usages[\"min\"] /= 1024**2\n",
    "        resulted_gpu_allocated_mem_usages[\"max\"] /= 1024**2\n",
    "\n",
    "        resulted_gpu_cached_mem_usages[\"mean\"] /= 1024**2\n",
    "        resulted_gpu_cached_mem_usages[\"min\"] /= 1024**2\n",
    "        resulted_gpu_cached_mem_usages[\"max\"] /= 1024**2\n",
    "\n",
    "        resulted_ram_mem_usages = MethodLogger._compute_stats(ram_mem_usages)\n",
    "        resulted_durations = MethodLogger._compute_stats(durations)\n",
    "\n",
    "        return resulted_gpu_allocated_mem_usages, resulted_gpu_cached_mem_usages, resulted_ram_mem_usages, resulted_durations, result\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_stats(data):\n",
    "        data = np.array(data)\n",
    "\n",
    "        if len(data) > 1:\n",
    "            lower_bound, upper_bound = np.percentile(data, [5, 95])\n",
    "            filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "        else:\n",
    "            filtered_data = data\n",
    "\n",
    "        return {\n",
    "            \"mean\": np.mean(filtered_data),\n",
    "            \"min\": np.min(filtered_data),\n",
    "            \"max\": np.max(filtered_data),\n",
    "        }\n",
    "\n",
    "\n",
    "method_logs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:51:45.934029Z",
     "start_time": "2024-10-02T23:51:45.922931Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_logs_to_file(method_logs: MethodLogger, is_test: bool = False):\n",
    "    log_entry = {\n",
    "        \"method_name\": method_logs.name,\n",
    "        \"method_args\": method_logs.method_args.copy(),\n",
    "        \"qualitative_metrics\": method_logs.qualitative_metrics.copy(),\n",
    "        \"quantitative_metrics\": method_logs.quantitative_metrics.copy(),\n",
    "    }\n",
    "    excluded_data = [\"tensor\", \"input_tensor\", \"tens\", \"sites\"]\n",
    "    for arg in excluded_data:\n",
    "        log_entry[\"method_args\"].pop(arg, None)\n",
    "\n",
    "    if is_test:\n",
    "        pprint(log_entry)\n",
    "    else:\n",
    "        log_dir_path = Path(\"../.cache\")\n",
    "        log_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        log_file_path = log_dir_path / \"method_logs.json\"\n",
    "\n",
    "        if not log_file_path.exists() or log_file_path.stat().st_size == 0:\n",
    "            with log_file_path.open(\"r+\", encoding=\"utf-8\") as f:\n",
    "                json5.dump([log_entry], f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            with log_file_path.open(\"r+\", encoding=\"utf-8\") as f:\n",
    "                logs = json5.load(f)\n",
    "\n",
    "                existing_log = next(\n",
    "                    (log for log in logs if log[\"method_name\"] == log_entry[\"method_name\"] and log[\"method_args\"] == log_entry[\"method_args\"]), None\n",
    "                )\n",
    "\n",
    "                if existing_log:\n",
    "                    existing_log.update(log_entry)\n",
    "                else:\n",
    "                    logs.append(log_entry)\n",
    "\n",
    "                f.seek(0)\n",
    "                json5.dump(logs, f, ensure_ascii=False, indent=4)\n",
    "                f.truncate()\n",
    "\n",
    "\n",
    "def get_tensors_size(*args):\n",
    "    total = 0\n",
    "    for arg in args:\n",
    "        partial = 1\n",
    "        for d in arg.shape:\n",
    "            partial *= d\n",
    "        total += partial\n",
    "    return total\n",
    "\n",
    "\n",
    "def load_logs_from_file(log_file_path):\n",
    "    log_file = Path(log_file_path)\n",
    "    if log_file.exists():\n",
    "        with log_file.open(encoding=\"utf-8\") as f:\n",
    "            logs = json5.load(f)\n",
    "            # print(f\"Загруженные логи:\")\n",
    "            # pprint(logs, indent=4, width=100)\n",
    "            return logs\n",
    "    else:\n",
    "        print(f\"Файл {log_file_path} не найден.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_frame_tensorly_tensortrain(frame):\n",
    "    frame = (frame - np.min(frame)) / (np.max(frame) - np.min(frame))\n",
    "    frame = np.clip(frame * 255, 0, 255)\n",
    "    return frame.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Download video and extract frames from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Some functions to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.622137Z",
     "start_time": "2024-10-02T23:50:14.610993Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_progress_hook(d):\n",
    "    if d[\"status\"] == \"downloading\":\n",
    "        print(f\"Downloading: {d['_percent_str']} at {d['_speed_str']} ETA: {d['_eta_str']}\")\n",
    "    elif d[\"status\"] == \"finished\":\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "\n",
    "def extract_video_id(video_url):\n",
    "    video_id_match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", video_url)\n",
    "    if video_id_match:\n",
    "        return video_id_match.group(1)\n",
    "    error_message = \"Не удалось извлечь ID видео из URL\"\n",
    "    raise ValueError(error_message)\n",
    "\n",
    "\n",
    "def download_youtube_video(video_url, cache_dir=None, proxy_url=None):\n",
    "    if cache_dir:\n",
    "        Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "        video_id = extract_video_id(video_url)\n",
    "        cache_video_path = Path(cache_dir) / f\"{video_id}.mp4\"\n",
    "    else:\n",
    "        cache_video_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\").name\n",
    "\n",
    "    if Path(cache_video_path).exists():\n",
    "        print(f\"Видео уже загружено и закешировано: {cache_video_path}\")\n",
    "        return cache_video_path\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"best\",\n",
    "        \"outtmpl\": str(cache_video_path),\n",
    "        \"progress_hooks\": [download_progress_hook],\n",
    "    }\n",
    "\n",
    "    if proxy_url:\n",
    "        ydl_opts[\"proxy\"] = proxy_url\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])\n",
    "\n",
    "    print(f\"Видео загружено и сохранено: {cache_video_path}\")\n",
    "    return cache_video_path\n",
    "\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return np.array(frames), fps, (width, height)\n",
    "\n",
    "\n",
    "def process_frames(frames):\n",
    "    processed_frames = []\n",
    "\n",
    "    for frame in frames:\n",
    "        b_channel, g_channel, r_channel = cv2.split(frame)\n",
    "\n",
    "        merged_frame = cv2.merge((b_channel, g_channel, r_channel))\n",
    "\n",
    "        processed_frames.append(merged_frame)\n",
    "\n",
    "    return np.array(processed_frames)\n",
    "\n",
    "\n",
    "def show_frames_as_video(frames):\n",
    "    for frame in frames:\n",
    "        cv2.imshow(\"Downloaded Video\", frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def save_frames_as_video(name, frames, fps, frame_size):\n",
    "    output_path = f\"../.cache/output_videos/{name}.mp4\"\n",
    "\n",
    "    Path(\"../.cache/output_videos\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    width, height = frame_size\n",
    "    size = (width, height)\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, size)\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Видео сохранено как {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.636685Z",
     "start_time": "2024-10-02T23:50:14.632175Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=eSKe2Vx-rpY\"\n",
    "proxy_url = os.getenv(\"PROXY_URL\")\n",
    "cache_dir = \"../.cache/youtube\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check how functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.665127Z",
     "start_time": "2024-10-02T23:50:14.660146Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео уже загружено и закешировано: ..\\.cache\\youtube\\eSKe2Vx-rpY.mp4\n"
     ]
    }
   ],
   "source": [
    "video_path = download_youtube_video(video_url, cache_dir=cache_dir, proxy_url=proxy_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:14.988914Z",
     "start_time": "2024-10-02T23:50:14.798835Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_frames, original_fps, frame_size = extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:15.010977Z",
     "start_time": "2024-10-02T23:50:15.005457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 360, 202, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:50:15.146749Z",
     "start_time": "2024-10-02T23:50:15.141024Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processed_video_frames = process_frames(video_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:11.951399Z",
     "start_time": "2024-10-02T16:33:11.948222Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show_frames_as_video(processed_video_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Implementations of Decompositions methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some packages which can decompose some dense type of tensors, from [this](https://arxiv.org/pdf/2103.13756) paper\n",
    "\n",
    "\n",
    "Decomposition methods which used:\n",
    "1. Canonical Polyadic Decomposition as PARAllel FACtors analysis (aka PARAFAC aka CPD aka CP)\n",
    "2. Tucker Decomposition\n",
    "3. Tensor Train\n",
    "4. some variants of its (Other)\n",
    "\n",
    "Tensor types:\n",
    "1. Dense (D)\n",
    "2. Sparse (S)\n",
    "3. BlockSparse (BS)\n",
    "4. Symmetric\n",
    "5. Supersymmetric\n",
    "\n",
    "Target system:\n",
    "1. CPU (C)\n",
    "2. GPU(G)\n",
    "3. Distributed Memory (D)\n",
    "\n",
    "\n",
    "\n",
    "| Method name                                                                             | Decomposition methods implemented | Tensor Type | Platform | Language         | Git | PyPI | Want to check | Checked     |\n",
    "|-----------------------------------------------------------------------------------------|-----------------------------------|------------|----------|------------------|------|------|--------------|-------------|\n",
    "| [AdaTM](https://github.com/hpcgarage/AdaTM)                                             | CP                                | S          | C        | C                | +    | ?    |              |             |\n",
    "| [BTAS](https://github.com/ValeevGroup/BTAS)                                             | CP, Tucker                        | nan        | C        | C++              | +    | ?    |              |             |\n",
    "| [CP-CALS](https://github.com/HPAC/CP-CALS)                                              | CP, Other                         | D          | C, G     | C++, Mat         | +    |      | +            |             |\n",
    "| [CSTF](https://github.com/ZacBlanco/cstf)                                               | Other                             | S          | D        | Scala            | +    | ?    |              |             |\n",
    "| [D-Tucker](https://datalab.snu.ac.kr/dtucker/resources/DTucker-v1.0.tar.gz)             | Tucker, Other                     | D          | C        | Matlab           |      | ?    |              |             |\n",
    "| [DFacTo](http://www.joonheechoi.com/research.)                                          | CP                                | S          | C, D     | C++              |      | ?    |              |             |\n",
    "| [EXATN](https://github.com/ORNL-QCI/exatn)                                              | TensorTrain                       | D          | C, D, G  | C++, Py          | +    |      | +            |             |\n",
    "| [Genten](https://gitlab.com/tensors/genten)                                             | CP                                | D, S       | C, G     | C++              | +    |      | +            |             |\n",
    "| GigaTensor                                                                              | CP                                | D          | C        | C++, Python      |      | ?    |              |             |\n",
    "| [ITensor](https://github.com/ITensor/ITensor)                                           | TensorTrain                       | D, BS      | C, G     | C++, Julia       | +    |      | +            |             |\n",
    "| [multiway](https://cran.r-project.org/web/packages/multiway/index.html)                 | CP, Tucker, Other                 | D          | C        | R                |      | ?    |              |             |\n",
    "| [N-way toolbox](http://www.models.life.ku.dk/nwaytoolbox/download)                      | CP, Tucker, Other                 | D          | C        | Matlab           |      | ?    |              |             |\n",
    "| [ParCube](https://www.cs.ucr.edu/~epapalex/src/parCube.zip)                             | CP                                | S          | C        | Matlab           |      | ?    |              |             |\n",
    "| [ParTensor](https://github.com/neurocom/partensor-toolbox)                              | CP                                | D          | C, G     | C++              | +    |      | +            |             |\n",
    "| [ParTI!](https://github.com/hpcgarage/ParTI)                                            | CP, Tucker                        | S          | C, G     | C, CUDA, Mat     | +    | ?    |              |             |\n",
    "| [PLANC](https://github.com/ramkikannan/planc)                                           | CP                                | S          | C, D     | C++              | +    | ?    |              |             |\n",
    "| [PLS toolbox](https://eigenvector.com/software/pls-toolbox/)                            | CP          , Tucker              | D          | C        | Matlab           |      | ?    |              |             |\n",
    "| [Pytensor](https://code.google.com/archive/p/pytensor/source/default/source)            | Tucker                            | D, S       | C        | Python           |      | ?    |              |             |\n",
    "| [rTensor](https://github.com/jamesyili/rTensor)                                         | CP, Tucker, Other                 | D          | C        | R                | +    |      | +            |             |\n",
    "| [rTensor (randomized)](https://github.com/erichson/rTensor)                             | CP                                | D          | C        | Python           | +    |      | +       +    |             |\n",
    "| [scikit-tensor](https://github.com/mnick/scikit-tensor)                                 | CP, Tucker, Other                 | D, S       | C        | Python           | +    | +    | +    +   +   | too old     |\n",
    "| [Scikit-TT](https://github.com/PGelss/scikit_tt)                                        | TensorTrain                       | D          | C        | Python           | +    |      |     +   +    |             |\n",
    "| [SPALS](https://github.com/dehuacheng/SpAls)                                            | CP                                | S          | C        | C++              | +    | ?    |              |             |\n",
    "| [SPARTan](https://github.com/kperros/SPARTan)                                           | Other                             | S          | C        | Matlab           | +    | ?    |              |             |\n",
    "| [SPLATT](https://github.com/ShadenSmith/splatt)                                         | CP                                | S          | C, D     | C, C++, Oct, Mat | +    | ?    |              |             |\n",
    "| [SuSMoST](https://susmost.com/downloads.html)                                           | TensorTrain, Other                | D          | C        | Python           |      | ?    |              |             |\n",
    "| [T3F](https://github.com/Bihaqo/t3f)                                                    | TensorTrain                       | D          | C, G     | Python           | +    | +    | +    + +     | in progress |\n",
    "| [TDALAB](https://github.com/andrewssobral/TDALAB)                                       | CP                                | D, S       | C        | Python, Matlab   | +    |      | +         +  |             |\n",
    "| [TeNPy](https://github.com/tenpy/tenpy)                                        | TensorTrain                       | D          | C        | Python           | +    | +    | +      + +   | in progress |\n",
    "| [Tensor Fox](https://github.com/felipebottega/Tensor-Fox)                               | CP                                | D, S       | C        | Python, Matlab   | +    | +    | +    + +     | ?           |\n",
    "| [Tensor package](http://www.gipsa-lab.fr/~pierre.comon/TensorPackage/tensorPackage.html) | CP                                | D          | C        | Matlab           |      | ?    |              |             |\n",
    "| [Tensor Toolbox](https://gitlab.com/tensors/tensor_toolbox)                             | CP, Tucker, Other                 | D, S       | C        | Matlab           | +    |      | +            |             |\n",
    "| [tensor_decomposition](https://github.com/cyclops-community/tensor_decomposition)       | CP, Tucker                        | D          | C, D     | Python           | +    |      | +        +   |             |\n",
    "| [TensorBox](https://github.com/phananhhuy/TensorBox)                                    | CP, Tucker, Other                 | D, S       | C        | Matlab           | +    |      | +            |             |\n",
    "| [TensorD](https://github.com/Large-Scale-Tensor-Decomposition/tensorD)                  | CP, Tucker                        | D          | C, G     | Python           | ?    | ?    |              |             |\n",
    "| [TensorLab](https://www.tensorlab.net)                                                  | CP, Tucker, Other                 | D, S       | C        | Matlab           |      | ?    |              |             |\n",
    "| [TensorLab+](https://www.tensorlabplus.net)                                             | CP, Other                         | D, S       | C        | Matlab           |      | ?    |              |             |\n",
    "| [TensorLy](https://github.com/tensorly/tensorly)                                        | CP, Tucker, TensorTrain, Other    | D          | C, G     | Python           | +    | +    | +       + +  | in progress |\n",
    "| [Three-Way](https://github.com/cran/ThreeWay)                                           | CP, Tucker                        | D          | C        | R                | +    |      | +            |             |\n",
    "| [TNR](https://github.com/ycyuustc/matlab)                                               | Other                             | D          | C        | Matlab           | +    |      | +            |             |\n",
    "| [TT-Toolbox](https://github.com/oseledets/TT-Toolbox)                                   | TensorTrain                       | D          | C, D, G  | Matlab, Python   | +    |      | +       +    |             |\n",
    "| [xerus](https://git.hemio.de/xerus/xerus/)                                              | TensorTrain                       | D, S       | C        | C++              | +    |      | +            |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## TensorLy"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.118871Z",
     "start_time": "2024-10-02T16:33:12.114808Z"
    }
   },
   "outputs": [],
   "source": [
    "# {‘numpy’, ‘mxnet’, ‘pytorch’, ‘tensorflow’, ‘cupy’}\n",
    "# backend variants for tensorly\n",
    "# tl.set_backend('pytorch')\n",
    "# with tl.backend_context(‘pytorch’): ... pass\n",
    "\n",
    "# video_frames_cuda = tl.tensor(video_frames.copy()).to(device)\n",
    "# video_frames_cuda = tl.tensor(video_frames.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Params"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.142436Z",
     "start_time": "2024-10-02T16:33:12.133882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         ...,\n",
       "         [ 52,  55,  66],\n",
       "         [ 56,  56,  68],\n",
       "         [ 53,  53,  65]],\n",
       "\n",
       "        [[ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         [ 41,  44,  57],\n",
       "         ...,\n",
       "         [ 60,  63,  74],\n",
       "         [ 60,  60,  72],\n",
       "         [ 50,  50,  62]],\n",
       "\n",
       "        [[ 40,  43,  56],\n",
       "         [ 41,  44,  57],\n",
       "         [ 40,  43,  56],\n",
       "         ...,\n",
       "         [ 71,  74,  85],\n",
       "         [ 69,  69,  81],\n",
       "         [ 47,  47,  59]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[125, 128, 133],\n",
       "         [119, 122, 127],\n",
       "         [127, 130, 135],\n",
       "         ...,\n",
       "         [128, 128, 135],\n",
       "         [128, 129, 139],\n",
       "         [132, 133, 143]],\n",
       "\n",
       "        [[130, 133, 138],\n",
       "         [121, 124, 129],\n",
       "         [126, 129, 134],\n",
       "         ...,\n",
       "         [132, 132, 139],\n",
       "         [125, 126, 136],\n",
       "         [133, 134, 144]],\n",
       "\n",
       "        [[132, 135, 140],\n",
       "         [123, 126, 131],\n",
       "         [125, 128, 133],\n",
       "         ...,\n",
       "         [135, 135, 142],\n",
       "         [125, 126, 136],\n",
       "         [130, 131, 141]]],\n",
       "\n",
       "\n",
       "       [[[ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         ...,\n",
       "         [ 54,  54,  66],\n",
       "         [ 50,  53,  64],\n",
       "         [ 47,  50,  61]],\n",
       "\n",
       "        [[ 40,  43,  56],\n",
       "         [ 41,  44,  57],\n",
       "         [ 41,  44,  57],\n",
       "         ...,\n",
       "         [ 59,  59,  71],\n",
       "         [ 48,  51,  62],\n",
       "         [ 46,  49,  60]],\n",
       "\n",
       "        [[ 41,  44,  57],\n",
       "         [ 41,  44,  57],\n",
       "         [ 40,  43,  56],\n",
       "         ...,\n",
       "         [ 68,  68,  80],\n",
       "         [ 47,  50,  61],\n",
       "         [ 43,  46,  57]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[119, 122, 127],\n",
       "         [127, 130, 135],\n",
       "         [142, 145, 150],\n",
       "         ...,\n",
       "         [130, 129, 139],\n",
       "         [132, 133, 143],\n",
       "         [133, 134, 144]],\n",
       "\n",
       "        [[121, 124, 129],\n",
       "         [126, 129, 134],\n",
       "         [139, 142, 147],\n",
       "         ...,\n",
       "         [127, 126, 136],\n",
       "         [133, 134, 144],\n",
       "         [132, 133, 143]],\n",
       "\n",
       "        [[123, 126, 131],\n",
       "         [125, 128, 133],\n",
       "         [137, 140, 145],\n",
       "         ...,\n",
       "         [127, 126, 136],\n",
       "         [130, 131, 141],\n",
       "         [131, 132, 142]]],\n",
       "\n",
       "\n",
       "       [[[ 40,  43,  56],\n",
       "         [ 40,  43,  56],\n",
       "         [ 41,  44,  57],\n",
       "         ...,\n",
       "         [ 52,  52,  64],\n",
       "         [ 50,  50,  62],\n",
       "         [ 49,  49,  61]],\n",
       "\n",
       "        [[ 40,  43,  56],\n",
       "         [ 41,  44,  57],\n",
       "         [ 41,  44,  57],\n",
       "         ...,\n",
       "         [ 55,  55,  67],\n",
       "         [ 55,  55,  67],\n",
       "         [ 56,  56,  68]],\n",
       "\n",
       "        [[ 41,  44,  57],\n",
       "         [ 40,  43,  56],\n",
       "         [ 44,  47,  60],\n",
       "         ...,\n",
       "         [ 52,  52,  64],\n",
       "         [ 55,  55,  67],\n",
       "         [ 56,  56,  68]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[133, 136, 141],\n",
       "         [139, 142, 147],\n",
       "         [140, 143, 148],\n",
       "         ...,\n",
       "         [134, 136, 143],\n",
       "         [135, 136, 146],\n",
       "         [135, 136, 146]],\n",
       "\n",
       "        [[132, 135, 140],\n",
       "         [134, 137, 142],\n",
       "         [135, 138, 143],\n",
       "         ...,\n",
       "         [135, 137, 144],\n",
       "         [135, 137, 144],\n",
       "         [135, 137, 144]],\n",
       "\n",
       "        [[130, 133, 138],\n",
       "         [134, 137, 142],\n",
       "         [132, 135, 140],\n",
       "         ...,\n",
       "         [134, 136, 143],\n",
       "         [135, 137, 144],\n",
       "         [134, 136, 143]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[185, 181, 184],\n",
       "         [176, 172, 175],\n",
       "         [156, 152, 155],\n",
       "         ...,\n",
       "         [ 83,  94, 115],\n",
       "         [ 97, 105, 127],\n",
       "         [114, 122, 144]],\n",
       "\n",
       "        [[180, 176, 179],\n",
       "         [173, 169, 172],\n",
       "         [155, 151, 154],\n",
       "         ...,\n",
       "         [ 80,  91, 112],\n",
       "         [100, 108, 130],\n",
       "         [111, 119, 141]],\n",
       "\n",
       "        [[191, 186, 192],\n",
       "         [192, 187, 193],\n",
       "         [177, 172, 178],\n",
       "         ...,\n",
       "         [ 67,  80, 103],\n",
       "         [ 81,  92, 113],\n",
       "         [ 90, 101, 122]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         ...,\n",
       "         [127, 136, 151],\n",
       "         [152, 161, 176],\n",
       "         [168, 177, 192]],\n",
       "\n",
       "        [[148, 155, 166],\n",
       "         [147, 154, 165],\n",
       "         [147, 154, 165],\n",
       "         ...,\n",
       "         [122, 131, 146],\n",
       "         [125, 134, 149],\n",
       "         [132, 141, 156]],\n",
       "\n",
       "        [[138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         ...,\n",
       "         [134, 143, 158],\n",
       "         [134, 143, 158],\n",
       "         [133, 142, 157]]],\n",
       "\n",
       "\n",
       "       [[[184, 180, 183],\n",
       "         [183, 179, 182],\n",
       "         [166, 162, 165],\n",
       "         ...,\n",
       "         [ 74,  87, 110],\n",
       "         [ 86,  97, 120],\n",
       "         [ 99, 110, 133]],\n",
       "\n",
       "        [[180, 176, 179],\n",
       "         [178, 174, 177],\n",
       "         [165, 161, 164],\n",
       "         ...,\n",
       "         [ 69,  82, 105],\n",
       "         [ 83,  94, 117],\n",
       "         [ 94, 105, 128]],\n",
       "\n",
       "        [[190, 185, 191],\n",
       "         [192, 187, 193],\n",
       "         [190, 185, 191],\n",
       "         ...,\n",
       "         [ 62,  75,  98],\n",
       "         [ 71,  84, 107],\n",
       "         [ 79,  92, 115]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         ...,\n",
       "         [131, 140, 155],\n",
       "         [154, 163, 178],\n",
       "         [161, 170, 185]],\n",
       "\n",
       "        [[148, 155, 166],\n",
       "         [147, 154, 165],\n",
       "         [147, 154, 165],\n",
       "         ...,\n",
       "         [123, 132, 147],\n",
       "         [145, 154, 169],\n",
       "         [167, 176, 191]],\n",
       "\n",
       "        [[138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         ...,\n",
       "         [122, 131, 146],\n",
       "         [123, 132, 147],\n",
       "         [130, 139, 154]]],\n",
       "\n",
       "\n",
       "       [[[182, 178, 181],\n",
       "         [184, 180, 183],\n",
       "         [175, 171, 174],\n",
       "         ...,\n",
       "         [ 69,  82, 105],\n",
       "         [ 81,  92, 115],\n",
       "         [ 93, 104, 127]],\n",
       "\n",
       "        [[177, 173, 176],\n",
       "         [178, 174, 177],\n",
       "         [174, 170, 173],\n",
       "         ...,\n",
       "         [ 64,  77, 100],\n",
       "         [ 73,  84, 107],\n",
       "         [ 81,  92, 115]],\n",
       "\n",
       "        [[196, 191, 197],\n",
       "         [196, 191, 197],\n",
       "         [199, 194, 200],\n",
       "         ...,\n",
       "         [ 60,  73,  96],\n",
       "         [ 66,  79, 102],\n",
       "         [ 72,  85, 108]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         [147, 154, 167],\n",
       "         ...,\n",
       "         [131, 140, 155],\n",
       "         [154, 163, 178],\n",
       "         [161, 170, 185]],\n",
       "\n",
       "        [[148, 155, 166],\n",
       "         [147, 154, 165],\n",
       "         [147, 154, 165],\n",
       "         ...,\n",
       "         [123, 132, 147],\n",
       "         [145, 154, 169],\n",
       "         [167, 176, 191]],\n",
       "\n",
       "        [[138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         [138, 145, 156],\n",
       "         ...,\n",
       "         [122, 131, 146],\n",
       "         [123, 132, 147],\n",
       "         [130, 139, 154]]]], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tucker (tl.decomposition.tucker)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Params"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.252267Z",
     "start_time": "2024-10-02T16:33:12.245946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truncated_svd', 'symeig_svd', 'randomized_svd']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.SVD_FUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.365310Z",
     "start_time": "2024-10-02T16:33:12.360350Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_param = (video_frames.shape[0] // 2, video_frames.shape[1], video_frames.shape[2], video_frames.shape[3])\n",
    "\n",
    "n_iter_max_param = 100\n",
    "\n",
    "svd_params = [\"truncated_svd\", \"symeig_svd\", \"randomized_svd\"]\n",
    "\n",
    "init_params = [\"svd\", \"random\"]\n",
    "\n",
    "backend_params = [\"pytorch\", \"numpy\"]\n",
    "\n",
    "random_state_param = 42\n",
    "\n",
    "total_iterations = len(list(product(svd_params, init_params, backend_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.556676Z",
     "start_time": "2024-10-02T16:33:12.550566Z"
    }
   },
   "outputs": [],
   "source": [
    "log_file_path = \"../.cache/method_logs.json\"\n",
    "logs = load_logs_from_file(log_file_path)\n",
    "\n",
    "for backend, svd_func, init_method in tqdm(product(backend_params, svd_params, init_params), desc=\"Проверка набора параметров\", total=total_iterations):\n",
    "    method_name = f\"TensorLy_Tucker_{backend}_{svd_func}_{init_method}\"\n",
    "\n",
    "    if logs:\n",
    "        existing_log = next(\n",
    "            (\n",
    "                log\n",
    "                for log in logs\n",
    "                if log[\"method_name\"] == method_name\n",
    "                and log[\"method_args\"].get(\"init\") == init_method\n",
    "                and log[\"method_args\"].get(\"svd\") == svd_func\n",
    "                and log[\"qualitative_metrics\"].get(\"TensorLy backend\") == backend\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if existing_log:\n",
    "            print(f\"Пропущена итерация: логи уже существуют для {method_name}\")\n",
    "            continue\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        with tl.backend_context(backend):\n",
    "            if backend == \"pytorch\":\n",
    "                tensor_param = tl.tensor(video_frames.copy()).to(device)\n",
    "            elif backend == \"numpy\":\n",
    "                tensor_param = tl.tensor(video_frames.copy())\n",
    "\n",
    "            method_logs = MethodLogger(\n",
    "                method_name=method_name,\n",
    "                method_input_tensor=tensor_param,\n",
    "                qualitative_metrics={\n",
    "                    \"Language\": \"Python\",\n",
    "                    \"Library\": \"TensorLy\",\n",
    "                    \"TensorLy backend\": f\"{backend}\",\n",
    "                    \"Tensor type\": \"Dense\",\n",
    "                    \"Platform\": \"CPU, GPU\",\n",
    "                    \"Decomposition method\": \"Tucker\",\n",
    "                },\n",
    "                method_args={\n",
    "                    \"tensor\": tensor_param,\n",
    "                    \"rank\": rank_param,\n",
    "                    \"n_iter_max\": n_iter_max_param,\n",
    "                    \"init\": init_method,\n",
    "                    \"svd\": svd_func,\n",
    "                    \"random_state\": random_state_param,\n",
    "                },\n",
    "                func=tl.decomposition.tucker,\n",
    "            )\n",
    "\n",
    "        if backend == \"pytorch\":\n",
    "            tensor_param = tl.tensor(tensor_param.cpu())\n",
    "\n",
    "        method_logs_list.append(method_logs)\n",
    "\n",
    "        core, factors = method_logs.method_result\n",
    "        factors_numpy = []\n",
    "        if backend == \"pytorch\":\n",
    "            core = tl.tensor(core.cpu())\n",
    "            for index, factor in enumerate(factors):\n",
    "                factors[index] = tl.tensor(factor.cpu())\n",
    "\n",
    "        reconstruct_frames_from_tensorly_tt_factors = tl.tucker_tensor.tucker_to_tensor((core, factors))\n",
    "\n",
    "        method_logs.quantitative_metrics[\"compression_ratio\"] = 100.0 * get_tensors_size(core, *factors) / get_tensors_size(tensor_param)\n",
    "\n",
    "        method_logs.quantitative_metrics[\"frobenius_error\"] = (\n",
    "            100.0 * tl.norm(reconstruct_frames_from_tensorly_tt_factors - tensor_param) / tl.norm(tensor_param)\n",
    "        ).item()\n",
    "\n",
    "        save_logs_to_file(method_logs=method_logs, is_test=False)\n",
    "\n",
    "        reconstructed_frames = []\n",
    "\n",
    "        reconstructed_frames = [normalize_frame_tensorly_tensortrain(frame) for frame in reconstruct_frames_from_tensorly_tt_factors]\n",
    "\n",
    "        save_frames_as_video(name=method_logs.name, frames=reconstructed_frames, fps=original_fps, frame_size=frame_size)\n",
    "    except (torch.cuda.OutOfMemoryError, MemoryError) as e:\n",
    "        print(f\"Пропущена итерация из-за недостатка памяти: {backend}, {svd_func}, {init_method}. Ошибка: {e!s}\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tensor Train - MPS (tensorly.decomposition.tensor_train)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Params"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:33:12.567253Z",
     "start_time": "2024-10-02T16:33:12.562682Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_param = [1, 500, 302, 500, 1]\n",
    "\n",
    "svd_params = [\"truncated_svd\", \"symeig_svd\", \"randomized_svd\"]\n",
    "\n",
    "backend_params = [\"pytorch\", \"numpy\"]\n",
    "\n",
    "total_iterations = len(list(product(backend_params, svd_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:53:37.566900Z",
     "start_time": "2024-10-02T20:38:14.416549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загруженные логи:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Проверка набора параметров:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущена итерация: логи уже существуют для TensorLy_TensorTrain_pytorch_truncated_svd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Эксперимент набора параметров:   0%|          | 0/5 [00:03<?, ?it/s]\u001B[A\n",
      "Проверка набора параметров:  33%|███▎      | 2/6 [00:03<00:06,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущена итерация из-за недостатка памяти: pytorch, symeig_svd. Ошибка: CUDA out of memory. Tried to allocate 177.30 GiB. GPU 0 has a total capacity of 11.00 GiB of which 8.82 GiB is free. Of the allocated memory 740.47 MiB is allocated by PyTorch, and 365.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Пропущена итерация: логи уже существуют для TensorLy_TensorTrain_pytorch_randomized_svd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Эксперимент набора параметров:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Эксперимент набора параметров:  20%|██        | 1/5 [01:32<06:09, 92.35s/it]\u001B[A\n",
      "Эксперимент набора параметров:  40%|████      | 2/5 [03:00<04:29, 89.98s/it]\u001B[A\n",
      "Эксперимент набора параметров:  60%|██████    | 3/5 [04:28<02:57, 88.77s/it]\u001B[A\n",
      "Эксперимент набора параметров:  80%|████████  | 4/5 [05:58<01:29, 89.36s/it]\u001B[A\n",
      "Эксперимент набора параметров: 100%|██████████| 5/5 [07:24<00:00, 88.95s/it]\u001B[A\n",
      "Проверка набора параметров:  67%|██████▋   | 4/6 [07:32<04:25, 132.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео сохранено как ../.cache/output_videos/TensorLy_TensorTrain_numpy_truncated_svd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Эксперимент набора параметров:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Проверка набора параметров:  83%|████████▎ | 5/6 [07:32<01:33, 93.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущена итерация из-за недостатка памяти: numpy, symeig_svd. Ошибка: Unable to allocate 44.3 GiB for an array with shape (218160, 218160) and data type uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Эксперимент набора параметров:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Эксперимент набора параметров:  20%|██        | 1/5 [13:23<53:33, 803.29s/it]\u001B[A\n",
      "Эксперимент набора параметров:  40%|████      | 2/5 [26:59<40:32, 810.92s/it]\u001B[A\n",
      "Эксперимент набора параметров:  60%|██████    | 3/5 [40:36<27:06, 813.45s/it]\u001B[A\n",
      "Эксперимент набора параметров:  80%|████████  | 4/5 [53:57<13:28, 808.87s/it]\u001B[A\n",
      "Эксперимент набора параметров: 100%|██████████| 5/5 [1:07:43<00:00, 812.69s/it]\u001B[A\n",
      "Проверка набора параметров: 100%|██████████| 6/6 [1:15:22<00:00, 753.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео сохранено как ../.cache/output_videos/TensorLy_TensorTrain_numpy_randomized_svd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "log_file_path = \"../.cache/method_logs.json\"\n",
    "logs = load_logs_from_file(log_file_path)\n",
    "\n",
    "for backend, svd_func in tqdm(product(backend_params, svd_params), desc=\"Проверка набора параметров\", total=total_iterations):\n",
    "    method_name = f\"TensorLy_TensorTrain_{backend}_{svd_func}\"\n",
    "\n",
    "    if logs:\n",
    "        existing_log = next(\n",
    "            (\n",
    "                log\n",
    "                for log in logs\n",
    "                if log[\"method_name\"] == method_name\n",
    "                and log[\"method_args\"].get(\"svd\") == svd_func\n",
    "                and log[\"qualitative_metrics\"].get(\"TensorLy backend\") == backend\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if existing_log:\n",
    "            print(f\"Пропущена итерация: логи уже существуют для {method_name}\")\n",
    "            continue\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    try:\n",
    "        with tl.backend_context(backend):\n",
    "            if backend == \"pytorch\":\n",
    "                tensor_param = tl.tensor(video_frames.copy()).to(device)\n",
    "            elif backend == \"numpy\":\n",
    "                tensor_param = tl.tensor(video_frames.copy())\n",
    "\n",
    "            method_logs = MethodLogger(\n",
    "                method_name=method_name,\n",
    "                method_input_tensor=tensor_param,\n",
    "                qualitative_metrics={\n",
    "                    \"Language\": \"Python\",\n",
    "                    \"Library\": \"TensorLy\",\n",
    "                    \"TensorLy backend\": f\"{backend}\",\n",
    "                    \"Tensor type\": \"Dense\",\n",
    "                    \"Platform\": \"CPU, GPU\",\n",
    "                    \"Decomposition method\": \"TensorTrain\",\n",
    "                },\n",
    "                method_args={\n",
    "                    \"input_tensor\": tensor_param,\n",
    "                    \"rank\": rank_param,\n",
    "                    \"svd\": svd_func,\n",
    "                },\n",
    "                func=tl.decomposition.tensor_train,\n",
    "            )\n",
    "\n",
    "        if backend == \"pytorch\":\n",
    "            tensor_param = tl.tensor(tensor_param.cpu())\n",
    "\n",
    "        method_logs_list.append(method_logs)\n",
    "\n",
    "        tt_factors = method_logs.method_result\n",
    "        factors_numpy = []\n",
    "        if backend == \"pytorch\":\n",
    "            for tt_factor in tt_factors:\n",
    "                factors_numpy.append(tt_factor.cpu())\n",
    "        elif backend == \"numpy\":\n",
    "            factors_numpy = tt_factors\n",
    "\n",
    "        reconstruct_frames_from_tensorly_tt_factors = tl.tt_to_tensor(factors_numpy)\n",
    "\n",
    "        method_logs.quantitative_metrics[\"compression_ratio\"] = 100.0 * get_tensors_size(*factors_numpy) / get_tensors_size(tensor_param)\n",
    "\n",
    "        method_logs.quantitative_metrics[\"frobenius_error\"] = (\n",
    "            100.0 * tl.norm(reconstruct_frames_from_tensorly_tt_factors - tensor_param) / tl.norm(tensor_param)\n",
    "        ).item()\n",
    "\n",
    "        save_logs_to_file(method_logs=method_logs)\n",
    "\n",
    "        reconstructed_frames = []\n",
    "\n",
    "        for i in range(len(reconstruct_frames_from_tensorly_tt_factors)):\n",
    "            reconstructed_frames.append(normalize_frame_tensorly_tensortrain(reconstruct_frames_from_tensorly_tt_factors[i]))\n",
    "\n",
    "        save_frames_as_video(name=method_logs.name, frames=reconstructed_frames, fps=original_fps, frame_size=frame_size)\n",
    "\n",
    "    except (torch.cuda.OutOfMemoryError, MemoryError) as e:\n",
    "        print(f\"Пропущена итерация из-за недостатка памяти: {backend}, {svd_func}. Ошибка: {e!s}\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## T3F"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Params"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T22:47:50.940721Z",
     "start_time": "2024-10-02T22:47:50.805438Z"
    }
   },
   "outputs": [],
   "source": [
    "tensor_param = video_frames.copy().astype(np.float32)\n",
    "\n",
    "rank_param = [1, 500, 302, 500, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:03:56.345742Z",
     "start_time": "2024-10-02T23:00:20.622859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эксперимент набора параметров: 100%|██████████| 5/5 [03:33<00:00, 42.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео сохранено как ../.cache/output_videos/T3F_TensorTrain.mp4\n"
     ]
    }
   ],
   "source": [
    "log_file_path = \"../.cache/method_logs.json\"\n",
    "logs = load_logs_from_file(log_file_path)\n",
    "\n",
    "method_name = \"T3F_TensorTrain\"\n",
    "\n",
    "if logs:\n",
    "    existing_log = next((log for log in logs if log[\"method_name\"] == method_name), None)\n",
    "    if existing_log:\n",
    "        error_message = f\"Пропущена итерация: логи уже существуют для {method_name}\"\n",
    "        raise error_message\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "method_logs = MethodLogger(\n",
    "    method_name=method_name,\n",
    "    method_input_tensor=tensor_param,\n",
    "    qualitative_metrics={\n",
    "        \"Language\": \"Python\",\n",
    "        \"Library\": \"T3F\",\n",
    "        \"Tensor type\": \"Dense\",\n",
    "        \"Platform\": \"CPU, GPU\",\n",
    "        \"Decomposition method\": \"TensorTrain\",\n",
    "    },\n",
    "    method_args={\n",
    "        \"tens\": tensor_param,\n",
    "        \"max_tt_rank\": rank_param,\n",
    "    },\n",
    "    func=t3f.to_tt_tensor,\n",
    ")\n",
    "\n",
    "method_logs_list.append(method_logs)\n",
    "\n",
    "tt_factors = method_logs.method_result\n",
    "\n",
    "reconstruct_frames_from_t3f_tt_factors = t3f.full(tt_factors)\n",
    "\n",
    "method_logs.quantitative_metrics[\"compression_ratio\"] = 100.0 * get_tensors_size(*tt_factors.tt_cores) / get_tensors_size(tensor_param)\n",
    "\n",
    "method_logs.quantitative_metrics[\"frobenius_error\"] = (\n",
    "    100.0 * np.linalg.norm(reconstruct_frames_from_t3f_tt_factors - tensor_param) / np.linalg.norm(tensor_param)\n",
    ")\n",
    "\n",
    "save_logs_to_file(method_logs=method_logs)\n",
    "\n",
    "reconstructed_frames = []\n",
    "\n",
    "for i in range(len(reconstruct_frames_from_t3f_tt_factors)):\n",
    "    reconstructed_frames.append(normalize_frame_tensorly_tensortrain(reconstruct_frames_from_t3f_tt_factors[i]))\n",
    "\n",
    "save_frames_as_video(name=method_logs.name, frames=reconstructed_frames, fps=original_fps, frame_size=frame_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## TeNPy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Params"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:15:40.465998Z",
     "start_time": "2024-10-03T00:15:40.286018Z"
    }
   },
   "outputs": [],
   "source": [
    "# tensor_param = video_frames.copy().astype(np.float32)\n",
    "#\n",
    "# # Размерность физического индекса\n",
    "# d = tensor_param.shape[-1]  # в вашем случае это 3\n",
    "#\n",
    "# # Создаем объект LegCharge\n",
    "# leg = LegCharge.from_trivial(d)\n",
    "#\n",
    "# # Создаем объекты Site для каждого физического индекса, кроме последнего\n",
    "# sites = [Site(leg) for _ in range(tensor_param.ndim - 1)]\n",
    "#\n",
    "# rank_param = [1, 500, 302, 500, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:15:49.936575Z",
     "start_time": "2024-10-03T00:15:49.865401Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'has_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mps \u001B[38;5;241m=\u001B[39m \u001B[43mMPS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43msites\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msites\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpsi\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m reconstructed_tensor \u001B[38;5;241m=\u001B[39m mps\u001B[38;5;241m.\u001B[39mto_full_tensor()\n",
      "File \u001B[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tenpy\\networks\\mps.py:2039\u001B[0m, in \u001B[0;36mMPS.from_full\u001B[1;34m(cls, sites, psi, form, cutoff, normalize, bc, outer_S)\u001B[0m\n\u001B[0;32m   2037\u001B[0m S_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m (L \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2038\u001B[0m norm \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m normalize \u001B[38;5;28;01melse\u001B[39;00m npc\u001B[38;5;241m.\u001B[39mnorm(psi)\n\u001B[1;32m-> 2039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mpsi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_label\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvL\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m   2040\u001B[0m     psi \u001B[38;5;241m=\u001B[39m psi\u001B[38;5;241m.\u001B[39madd_trivial_leg(\u001B[38;5;241m0\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvL\u001B[39m\u001B[38;5;124m'\u001B[39m, qconj\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2041\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m bc \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinite\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m psi\u001B[38;5;241m.\u001B[39mget_leg(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvL\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mind_len \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'has_label'"
     ]
    }
   ],
   "source": [
    "# mps = MPS.from_full(sites=sites, psi=tensor_param, normalize=False)\n",
    "#\n",
    "# reconstructed_tensor = mps.to_full_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:55:08.679426Z",
     "start_time": "2024-10-02T23:55:07.923523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эксперимент набора параметров:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown type of a",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m     17\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n\u001B[1;32m---> 19\u001B[0m method_logs \u001B[38;5;241m=\u001B[39m \u001B[43mMethodLogger\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod_input_tensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_param\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqualitative_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLanguage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPython\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLibrary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTeNPy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTensor type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDense\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPlatform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCPU\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDecomposition method\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTensorTrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msites\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_param\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpsi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank_param\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnormalize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtenpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetworks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMPS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_full\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m method_logs_list\u001B[38;5;241m.\u001B[39mappend(method_logs)\n\u001B[0;32m     39\u001B[0m tt_factors \u001B[38;5;241m=\u001B[39m method_logs\u001B[38;5;241m.\u001B[39mmethod_result\n",
      "Cell \u001B[1;32mIn[4], line 13\u001B[0m, in \u001B[0;36mMethodLogger.__init__\u001B[1;34m(self, method_name, method_input_tensor, qualitative_metrics, func, method_args)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod_input_tensor \u001B[38;5;241m=\u001B[39m method_input_tensor\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc \u001B[38;5;241m=\u001B[39m func\n\u001B[1;32m---> 13\u001B[0m gpu_allocated_mem_usages, gpu_cached_mem_usages, ram_mem_usages, durations, result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_method_with_tracking\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmethod_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod_result \u001B[38;5;241m=\u001B[39m result\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquantitative_metrics \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu_allocated_mem_usage\u001B[39m\u001B[38;5;124m'\u001B[39m: gpu_allocated_mem_usages,\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu_cached_mem_usages\u001B[39m\u001B[38;5;124m'\u001B[39m: gpu_cached_mem_usages,\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mram_mem_usage\u001B[39m\u001B[38;5;124m'\u001B[39m: ram_mem_usages,\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mduration\u001B[39m\u001B[38;5;124m'\u001B[39m: durations,\n\u001B[0;32m     22\u001B[0m }\n",
      "Cell \u001B[1;32mIn[4], line 62\u001B[0m, in \u001B[0;36mMethodLogger._run_method_with_tracking\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gpu_allocated_memory_used, gpu_cached_memory_used, ram_memory_used, duration, result\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(MethodLogger\u001B[38;5;241m.\u001B[39mexperiments_count), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mЭксперимент набора параметров\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m---> 62\u001B[0m     gpu_allocated_memory_used, gpu_cached_memory_used, ram_memory_used, duration, result \u001B[38;5;241m=\u001B[39m \u001B[43mwrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     gpu_allocated_mem_usages\u001B[38;5;241m.\u001B[39mappend(gpu_allocated_memory_used)\n\u001B[0;32m     65\u001B[0m     gpu_cached_mem_usages\u001B[38;5;241m.\u001B[39mappend(gpu_cached_memory_used)\n",
      "Cell \u001B[1;32mIn[4], line 42\u001B[0m, in \u001B[0;36mMethodLogger._run_method_with_tracking.<locals>.wrapper\u001B[1;34m()\u001B[0m\n\u001B[0;32m     38\u001B[0m gpu_cached_memory_before \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmemory_reserved()\n\u001B[0;32m     40\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 42\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n\u001B[0;32m     45\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tenpy\\networks\\mps.py:2038\u001B[0m, in \u001B[0;36mMPS.from_full\u001B[1;34m(cls, sites, psi, form, cutoff, normalize, bc, outer_S)\u001B[0m\n\u001B[0;32m   2036\u001B[0m B_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m L\n\u001B[0;32m   2037\u001B[0m S_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m (L \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m-> 2038\u001B[0m norm \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m normalize \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mnpc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpsi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m psi\u001B[38;5;241m.\u001B[39mhas_label(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvL\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m   2040\u001B[0m     psi \u001B[38;5;241m=\u001B[39m psi\u001B[38;5;241m.\u001B[39madd_trivial_leg(\u001B[38;5;241m0\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvL\u001B[39m\u001B[38;5;124m'\u001B[39m, qconj\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tenpy\\linalg\\np_conserved.py:3719\u001B[0m, in \u001B[0;36mnorm\u001B[1;34m(a, ord, convert_to_float)\u001B[0m\n\u001B[0;32m   3717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(a\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, )), \u001B[38;5;28mord\u001B[39m)\n\u001B[0;32m   3718\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(a, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m-> 3719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(\u001B[43m[\u001B[49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   3720\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown type of a\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tenpy\\linalg\\np_conserved.py:3719\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   3717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(a\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, )), \u001B[38;5;28mord\u001B[39m)\n\u001B[0;32m   3718\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(a, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m-> 3719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm([\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m a] \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   3720\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown type of a\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\__git_projects\\tensor-compression-methods\\.venv\\Lib\\site-packages\\tenpy\\linalg\\np_conserved.py:3721\u001B[0m, in \u001B[0;36mnorm\u001B[1;34m(a, ord, convert_to_float)\u001B[0m\n\u001B[0;32m   3719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm([norm(p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m a] \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   3720\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3721\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown type of a\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: unknown type of a"
     ]
    }
   ],
   "source": [
    "# log_file_path = '../.cache/method_logs.json'\n",
    "# logs = load_logs_from_file(log_file_path)\n",
    "#\n",
    "# method_name = f\"TeNPy_TensorTrain\"\n",
    "#\n",
    "# if logs:\n",
    "#     existing_log = next(\n",
    "#         (log for log in logs if log['method_name'] == method_name),\n",
    "#         None\n",
    "#     )\n",
    "#     if existing_log:\n",
    "#         error_message = f\"Пропущена итерация: логи уже существуют для {method_name}\"\n",
    "#         raise error_message\n",
    "#\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.synchronize()\n",
    "#\n",
    "# method_logs = MethodLogger(\n",
    "#     method_name=method_name,\n",
    "#     method_input_tensor=tensor_param,\n",
    "#     qualitative_metrics={\n",
    "#         \"Language\": \"Python\",\n",
    "#         \"Library\": \"TeNPy\",\n",
    "#         \"Tensor type\": \"Dense\",\n",
    "#         \"Platform\": \"CPU\",\n",
    "#         \"Decomposition method\": \"TensorTrain\",\n",
    "#     },\n",
    "#     method_args={\n",
    "#         \"sites\": tensor_param,\n",
    "#         \"psi\": rank_param,\n",
    "#         \"normalize\": False,\n",
    "#     },\n",
    "#     func=tenpy.networks.mps.MPS.from_full\n",
    "# )\n",
    "#\n",
    "# method_logs_list.append(method_logs)\n",
    "#\n",
    "# tt_factors = method_logs.method_result\n",
    "#\n",
    "# reconstruct_frames_from_tenpy_tt_factors = tt_factors.to_full_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_logs.quantitative_metrics['compression_ratio'] = (100.0 * get_tensors_size(*tt_factors) / get_tensors_size(tensor_param))\n",
    "\n",
    "# method_logs.quantitative_metrics['frobenius_error'] = (\n",
    "#         100.0 * np.linalg.norm(reconstruct_frames_from_tenpy_tt_factors - tensor_param) / tl.norm(\n",
    "#         tensor_param))\n",
    "#\n",
    "# save_logs_to_file(method_logs=method_logs, is_test=True)\n",
    "#\n",
    "# reconstructed_frames = []\n",
    "#\n",
    "# for i in range(len(reconstruct_frames_from_tenpy_tt_factors)):\n",
    "#     reconstructed_frames.append(normalize_frame_tensorly_tensortrain(reconstruct_frames_from_tenpy_tt_factors[i]))\n",
    "#\n",
    "# save_frames_as_video(name=method_logs.name, frames=reconstructed_frames, fps=original_fps, frame_size=frame_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
