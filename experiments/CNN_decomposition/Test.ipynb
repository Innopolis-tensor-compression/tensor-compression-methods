{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:12.954723Z",
     "start_time": "2024-11-21T10:42:09.314330Z"
    }
   },
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from torchvision.models import resnet18\n",
    "from tensorly.decomposition import tucker, parafac\n",
    "from flopco import FlopCo\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "tl.set_backend(\"pytorch\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:12.960861Z",
     "start_time": "2024-11-21T10:42:12.958165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "in_channels = 64\n",
    "out_channels = 128\n",
    "kernel_size = (3, 3)\n",
    "tensor_size = 7\n",
    "# rank_CPD = 9\n",
    "# rank_TKD = (256, 101, 9)\n",
    "number_of_images = 128"
   ],
   "id": "8ec3a4fccab9ae2e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Original conv",
   "id": "2632cd96f97354b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.029429Z",
     "start_time": "2024-11-21T10:42:13.023928Z"
    }
   },
   "cell_type": "code",
   "source": "full_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, dtype=torch.float32)",
   "id": "58fb7c4b8f71ef99",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.037481Z",
     "start_time": "2024-11-21T10:42:13.030821Z"
    }
   },
   "cell_type": "code",
   "source": "random_tensor = torch.rand(number_of_images, in_channels, tensor_size, tensor_size, dtype=torch.float32)",
   "id": "95f188119b444e70",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.055136Z",
     "start_time": "2024-11-21T10:42:13.051495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%timeit -r 10 -n 10000\n",
    "# full_conv(random_tensor)"
   ],
   "id": "a5f829a66d39fbe8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPD-only conv",
   "id": "73eeb2a54e252730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.072506Z",
     "start_time": "2024-11-21T10:42:13.067569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def SVD_conv(conv_layer: torch.nn.Conv2d, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    stride = conv_layer.stride\n",
    "    matrix = conv_layer.weight.squeeze().squeeze()\n",
    "    if rank_CPD is None:\n",
    "        rank_CPD = min(matrix.shape)\n",
    "        \n",
    "    core, factors = parafac(matrix, rank_CPD, init=\"random\")\n",
    "    norm = tl.norm(matrix - tl.cp_to_tensor((core, factors))) / tl.norm(matrix)\n",
    "    print(f\"SVD ({in_channels}, {out_channels}, (1, 1)): {norm}\")\n",
    "\n",
    "    factor_CPD_input = factors[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "    factor_CPD_output = factors[0].unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "\n",
    "    conv1 = torch.nn.Conv2d(in_channels, rank_CPD, 1, stride=stride, dtype=torch.float32)\n",
    "    conv2 = torch.nn.Conv2d(rank_CPD, out_channels, 1, dtype=torch.float32)\n",
    "    conv1.weight = torch.nn.parameter.Parameter(factor_CPD_input)\n",
    "    conv2.weight = torch.nn.parameter.Parameter(factor_CPD_output)\n",
    "    return torch.nn.Sequential(conv1, conv2), norm"
   ],
   "id": "d582b8c41f99710e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.092270Z",
     "start_time": "2024-11-21T10:42:13.086256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def CPD_conv(conv_layer: torch.nn.Conv2d, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, rank_CPD)\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    conv_weight = conv_layer.weight.reshape(out_channels, in_channels, kernel_size_x * kernel_size_y)\n",
    "    \n",
    "    if rank_CPD is None:\n",
    "        rank_CPD = sorted(conv_weight.size())[0]\n",
    "    # elif rank_CPD > sorted(conv_weight.size())[0]:\n",
    "    #     rank_CPD = sorted(conv_weight.size())[0]\n",
    "    #     warnings.warn(\n",
    "    #         f\"\"\"\n",
    "    #         rank_CPD > min(f{conv_weight.size()})\n",
    "    #         rank_CPD is bigger than the smallest size of tensor dimension\n",
    "    #         rank_CPD is set to min(f{conv_weight.size()})\n",
    "    #         \"\"\"\n",
    "    #     )\n",
    "\n",
    "    core_CPD, factors_CPD = parafac(conv_weight, rank_CPD, init=\"random\", svd=\"randomized_svd\")\n",
    "    norm = tl.norm(conv_weight - tl.cp_to_tensor((core_CPD, factors_CPD))) / tl.norm(conv_weight)\n",
    "    print(f\"CPD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_CPD_input = factors_CPD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "    factor_CPD_hidden = factors_CPD[2].permute([1, 0]).unsqueeze(1).reshape(rank_CPD, 1, kernel_size_x, kernel_size_y)\n",
    "    factor_CPD_output = factors_CPD[0].unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    conv1_CPD = torch.nn.Conv2d(in_channels, rank_CPD, 1, dtype=torch.float32)\n",
    "    conv2_CPD = torch.nn.Conv2d(rank_CPD, rank_CPD, (kernel_size_x, kernel_size_y), groups=rank_CPD, stride=stride, padding=padding, dilation=dilation, dtype=torch.float32)\n",
    "    conv3_CPD = torch.nn.Conv2d(rank_CPD, out_channels, 1, dtype=torch.float32)\n",
    "    conv1_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_input)\n",
    "    conv2_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_hidden)\n",
    "    conv3_CPD.weight = torch.nn.parameter.Parameter(factor_CPD_output)\n",
    "    \n",
    "    return torch.nn.Sequential(conv1_CPD, conv2_CPD, conv3_CPD), norm"
   ],
   "id": "f789a24ecd535cad",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.502102Z",
     "start_time": "2024-11-21T10:42:13.106852Z"
    }
   },
   "cell_type": "code",
   "source": "conv_CPD = CPD_conv(full_conv, rank_CPD=64)[0]",
   "id": "9d96b2f2763f8798",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPD (64, 128, (3, 3)): 0.8220816850662231\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.519876Z",
     "start_time": "2024-11-21T10:42:13.517014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%timeit -r 10 -n 10000\n",
    "# conv_CPD(random_tensor)"
   ],
   "id": "395d60af8b37159a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.587980Z",
     "start_time": "2024-11-21T10:42:13.533994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del conv_CPD\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "b998b3f40e2e3d3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TKD-only conv",
   "id": "295e757180b65a91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.612918Z",
     "start_time": "2024-11-21T10:42:13.602800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def TKD_conv(conv_layer: torch.nn.Conv2d, rank_TKD: tuple[int, int, int]=None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, min(rank_TKD))\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    conv_weight = conv_layer.weight.reshape(out_channels, in_channels, kernel_size_x * kernel_size_y)\n",
    "    \n",
    "    if rank_TKD is None:\n",
    "        rank_TKD = (out_channels, in_channels, kernel_size_x * kernel_size_y)\n",
    "    else:\n",
    "        if rank_TKD[0] > out_channels:\n",
    "            rank_TKD = (out_channels, rank_TKD[1], rank_TKD[2])\n",
    "            warnings.warn(\"rank_TKD[0] is bigger then out_channels\")\n",
    "        if rank_TKD[1] > in_channels:\n",
    "            rank_TKD = (rank_TKD[0], in_channels, rank_TKD[2])\n",
    "            warnings.warn(\"rank_TKD[1] is bigger then in_channels\")\n",
    "        if rank_TKD[2] > kernel_size_x * kernel_size_y:\n",
    "            rank_TKD = (rank_TKD[0], rank_TKD[1], kernel_size_x * kernel_size_y)\n",
    "            warnings.warn(\"rank_TKD[2] is bigger then kernel_size_x * kernel_size_y\")\n",
    "    \n",
    "    core_TKD, factors_TKD = tucker(conv_weight, rank_TKD)\n",
    "    norm = tl.norm(conv_weight - tl.tucker_to_tensor((core_TKD, factors_TKD))) / tl.norm(conv_weight)\n",
    "    print(f\"TKD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_TKD_input = factors_TKD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "    factor_TKD_hidden = torch.tensordot(factors_TKD[2], core_TKD, dims=([1], [2])).permute([1, 2, 0]).reshape(rank_TKD[0], rank_TKD[1], kernel_size_x, kernel_size_y)\n",
    "    factor_TKD_output = factors_TKD[0].unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    conv1_TKD = torch.nn.Conv2d(in_channels, rank_TKD[1], 1, dtype=torch.float32)\n",
    "    conv2_TKD = torch.nn.Conv2d(rank_TKD[1], rank_TKD[0], (kernel_size_x, kernel_size_y), stride=stride, padding=padding, dilation=dilation, dtype=torch.float32)\n",
    "    conv3_TKD = torch.nn.Conv2d(rank_TKD[0], out_channels, 1, dtype=torch.float32)\n",
    "    conv1_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_input)\n",
    "    conv2_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_hidden)\n",
    "    conv3_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_output)\n",
    "    \n",
    "    return torch.nn.Sequential(conv1_TKD, conv2_TKD, conv3_TKD), norm"
   ],
   "id": "749545e8bc911909",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.757629Z",
     "start_time": "2024-11-21T10:42:13.628403Z"
    }
   },
   "cell_type": "code",
   "source": "conv_TKD = TKD_conv(full_conv)[0]",
   "id": "536029f3d0eb1dea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TKD (64, 128, (3, 3)): 1.3146224091542535e-06\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.774364Z",
     "start_time": "2024-11-21T10:42:13.771894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%timeit -r 10 -n 10000\n",
    "# conv_TKD(random_tensor)"
   ],
   "id": "76b07880bb3ac725",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.845246Z",
     "start_time": "2024-11-21T10:42:13.788688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del conv_TKD\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "e7e4b16a6d5adf58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TKD-CPD conv",
   "id": "fa38d51e70535fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:13.868311Z",
     "start_time": "2024-11-21T10:42:13.858913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def TKDCPD_conv(conv_layer: torch.nn.Conv2d, rank_TKD:tuple[int, int, int] = None, rank_CPD: int = None) -> (torch.nn.Sequential, float):\n",
    "    if conv_layer.kernel_size == (1, 1):\n",
    "        return SVD_conv(conv_layer, rank_CPD)\n",
    "    # Params of source conv_layer\n",
    "    out_channels = conv_layer.out_channels\n",
    "    in_channels = conv_layer.in_channels\n",
    "    kernel_size_x = conv_layer.kernel_size[0]\n",
    "    kernel_size_y = conv_layer.kernel_size[1]\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    conv_weight = conv_layer.weight.reshape(out_channels, in_channels, kernel_size_x * kernel_size_y)\n",
    "\n",
    "    if rank_TKD is None:\n",
    "        rank_TKD = (out_channels, in_channels, kernel_size_x * kernel_size_y)\n",
    "    else:\n",
    "        if rank_TKD[0] > out_channels:\n",
    "            rank_TKD = (out_channels, rank_TKD[1], rank_TKD[2])\n",
    "            warnings.warn(f\"rank_TKD[0] is bigger then out_channels\\n\\nrank_TKD[0]={rank_TKD[0]}\\nout_channels={out_channels}\")\n",
    "        if rank_TKD[1] > in_channels:\n",
    "            rank_TKD = (rank_TKD[0], in_channels, rank_TKD[2])\n",
    "            warnings.warn(f\"rank_TKD[1] is bigger then in_channels\\n\\nrank_TKD[1]={rank_TKD[1]}\\nin_channels={in_channels}\")\n",
    "        if rank_TKD[2] > kernel_size_x * kernel_size_y:\n",
    "            rank_TKD = (rank_TKD[0], rank_TKD[1], kernel_size_x * kernel_size_y)\n",
    "            warnings.warn(f\"rank_TKD[2] is bigger then kernel_size_x * kernel_size_y\\nrank_TKD[2]={rank_TKD[2]}\\nkernel_size_x * kernel_size_y={kernel_size_x * kernel_size_y}\")\n",
    "    \n",
    "    core_TKD, factors_TKD = tucker(conv_weight, rank_TKD)\n",
    "    norm = tl.norm(conv_weight - tl.tucker_to_tensor((core_TKD, factors_TKD))) / tl.norm(conv_weight)\n",
    "    print(f\"TKDCPD ({in_channels}, {out_channels}, ({kernel_size_x}, {kernel_size_y})): {norm}\")\n",
    "\n",
    "    factor_TKD_input = factors_TKD[1].permute([1, 0]).unsqueeze(2).unsqueeze(3)\n",
    "    factor_TKD_hidden = torch.tensordot(factors_TKD[2], core_TKD, dims=([1], [2])).permute([1, 2, 0]).reshape(rank_TKD[0], rank_TKD[1], kernel_size_x, kernel_size_y)\n",
    "    factor_TKD_output = factors_TKD[0].unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    conv2_TKD = torch.nn.Conv2d(rank_TKD[1], rank_TKD[0], (kernel_size_x, kernel_size_y), stride=stride, padding=padding, dilation=dilation, dtype=torch.float32)\n",
    "    conv2_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_hidden)\n",
    "    conv2_TKD = CPD_conv(conv2_TKD, rank_CPD=rank_CPD)\n",
    "    norm = conv2_TKD[1]\n",
    "    conv2_TKD = conv2_TKD[0]\n",
    "\n",
    "    conv1_TKD = torch.nn.Conv2d(in_channels, rank_TKD[1], 1, dtype=torch.float32)\n",
    "    conv3_TKD = torch.nn.Conv2d(rank_TKD[0], out_channels, 1, dtype=torch.float32)\n",
    "    conv1_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_input)\n",
    "    conv3_TKD.weight = torch.nn.parameter.Parameter(factor_TKD_output)\n",
    "\n",
    "    return torch.nn.Sequential(conv1_TKD, conv2_TKD, conv3_TKD), norm"
   ],
   "id": "eee719b0679f783f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.165173Z",
     "start_time": "2024-11-21T10:42:13.883277Z"
    }
   },
   "cell_type": "code",
   "source": "conv_TKDCPD = TKDCPD_conv(full_conv)",
   "id": "b49f5dd3624b5249",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TKDCPD (64, 128, (3, 3)): 1.3146224091542535e-06\n",
      "CPD (64, 128, (3, 3)): 0.9747400283813477\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.181081Z",
     "start_time": "2024-11-21T10:42:14.178167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%timeit -r 10 -n 10000\n",
    "# conv_TKDCPD(random_tensor)"
   ],
   "id": "c6a4dad5910286ec",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.255243Z",
     "start_time": "2024-11-21T10:42:14.195859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del conv_TKDCPD\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "80824388dbba9eb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## STATS",
   "id": "15c183b7aa323f1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.272725Z",
     "start_time": "2024-11-21T10:42:14.269193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stats_conv = FlopCo(full_conv, img_size=[number_of_images, in_channels, tensor_size, tensor_size], device=device)\n",
    "# stats_CPD = FlopCo(conv_CPD, img_size=[number_of_images, in_channels, tensor_size, tensor_size], device=device)\n",
    "# stats_TKD = FlopCo(conv_TKD, img_size=[number_of_images, in_channels, tensor_size, tensor_size], device=device)\n",
    "# stats_TKDCPD = FlopCo(conv_TKDCPD, img_size=[number_of_images, in_channels, tensor_size, tensor_size], device=device)"
   ],
   "id": "2f006490f3ce86e3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.289209Z",
     "start_time": "2024-11-21T10:42:14.286126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"Normal conv:\")\n",
    "# print(stats_conv)\n",
    "# print(\"\\nCPD:\")\n",
    "# print(stats_CPD)\n",
    "# print(\"\\nTKD:\")\n",
    "# print(stats_TKD)\n",
    "# print(\"\\nTKDCPD:\")\n",
    "# print(stats_TKDCPD)"
   ],
   "id": "e49648e695050cf9",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.306419Z",
     "start_time": "2024-11-21T10:42:14.303427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"FLOPS\")\n",
    "# print(f\"{stats_conv.total_flops/10e6}e6; {stats_CPD.total_flops/10e6}e6; {stats_TKD.total_flops/10e6}e6; {stats_TKDCPD.total_flops/10e6}e6\")\n",
    "# print(\"\\nRelative Flops\")\n",
    "# print(f\"{stats_conv.relative_flops}\\n{stats_CPD.relative_flops}\\n{stats_TKD.relative_flops}\\n{stats_TKDCPD.relative_flops}\")\n",
    "# print(\"\\nParameters\")\n",
    "# print(f\"{stats_conv.total_params}; {stats_CPD.total_params}; {stats_TKD.total_params}; {stats_TKDCPD.total_params}\")\n",
    "# print(\"\\nRelative parameters\")\n",
    "# print(f\"{stats_conv.relative_params}\\n{stats_CPD.relative_params}\\n{stats_TKD.relative_params}\\n{stats_TKDCPD.relative_params}\")"
   ],
   "id": "ae1dff131a0edfca",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert RESNET",
   "id": "e301d434441caecf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:14.324319Z",
     "start_time": "2024-11-21T10:42:14.320479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compress_resnet(resnet, conv_func, rank: int = None):\n",
    "    layer_norms = {}\n",
    "    for name, module in resnet.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "            attr_name = name.split(\".\")[-1]\n",
    "            print(parent_name, end=\": \")\n",
    "            # Access the parent module\n",
    "            parent_module = resnet\n",
    "            if parent_name:\n",
    "                parent_module = dict(resnet.named_modules())[parent_name]\n",
    "            \n",
    "            # Replace the old layer with the new one\n",
    "            result = conv_func(module, rank_CPD=rank)\n",
    "            setattr(parent_module, attr_name, result[0])\n",
    "            layer_norms[parent_name + \".\" + attr_name] = result[1]\n",
    "            del result\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return resnet, layer_norms"
   ],
   "id": "4e7e4c3af219bca",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:42:30.972098Z",
     "start_time": "2024-11-21T10:42:14.340875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_results(conv_func_name, rank, stats, norms):\n",
    "    # Define the directory structure\n",
    "    dir_path = f\"./{conv_func_name}/{rank}/\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Save FLOPS and parameter counts\n",
    "    stats_data = {\n",
    "        \"total_flops\": stats.total_flops,\n",
    "        \"total_params\": stats.total_params,\n",
    "        \"relative_flops\": stats.relative_flops,\n",
    "        \"relative_params\": stats.relative_params\n",
    "    }\n",
    "    with open(f\"{dir_path}flops_params.json\", \"w\") as f:\n",
    "        json.dump(stats_data, f, indent=4)\n",
    "\n",
    "    # Save norms of each layer\n",
    "    with open(f\"{dir_path}layer_norms.txt\", \"w\") as f:\n",
    "        for layer_name, norm_value in norms.items():\n",
    "            f.write(f\"{layer_name}: {norm_value}\\n\")\n",
    "    print(f\"Results saved in {dir_path}\")\n",
    "\n",
    "# Define the range of ranks you want to iterate over\n",
    "rank_range = range(41, 42)\n",
    "\n",
    "# Initialize your ResNet model\n",
    "resnet_model = resnet18(weights='DEFAULT')\n",
    "\n",
    "# Loop over the specified ranks and compress the ResNet with each rank\n",
    "for rank in rank_range:\n",
    "    print(f\"Compressing with rank_CPD = {rank}\")\n",
    "\n",
    "    # Clone the model to avoid overwriting the original\n",
    "    model_copy = resnet18(weights='DEFAULT')\n",
    "    \n",
    "    # Compress the model, storing norms of each layer\n",
    "    compressed_model, layer_norms = compress_resnet(model_copy, CPD_conv, rank)\n",
    "    compressed_model = compressed_model.cpu()\n",
    "\n",
    "    # Capture statistics with FlopCo\n",
    "    stats_CPD = FlopCo(compressed_model, device=\"cpu\")\n",
    "\n",
    "    # Save the results\n",
    "    save_results(\"CPD_conv\", rank, stats_CPD, layer_norms)\n",
    "\n",
    "    # Clear cache\n",
    "    del compressed_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ],
   "id": "dac1adf11de21356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing with rank_CPD = 41\n",
      ": CPD (3, 64, (7, 7)): 0.09475382417440414\n",
      "layer1.0: CPD (64, 64, (3, 3)): 0.48146572709083557\n",
      "layer1.0: CPD (64, 64, (3, 3)): 0.5971982479095459\n",
      "layer1.1: CPD (64, 64, (3, 3)): 0.5880162715911865\n",
      "layer1.1: CPD (64, 64, (3, 3)): 0.6535841226577759\n",
      "layer2.0: CPD (64, 128, (3, 3)): 0.710805356502533\n",
      "layer2.0: CPD (128, 128, (3, 3)): 0.7135886549949646\n",
      "layer2.0.downsample: SVD (64, 128, (1, 1)): 0.1941254884004593\n",
      "layer2.1: CPD (128, 128, (3, 3)): 0.7751623392105103\n",
      "layer2.1: CPD (128, 128, (3, 3)): 0.7718322277069092\n",
      "layer3.0: CPD (128, 256, (3, 3)): 0.7454242706298828\n",
      "layer3.0: CPD (256, 256, (3, 3)): 0.8372140526771545\n",
      "layer3.0.downsample: SVD (128, 256, (1, 1)): 0.5445394515991211\n",
      "layer3.1: CPD (256, 256, (3, 3)): 0.8586596846580505\n",
      "layer3.1: CPD (256, 256, (3, 3)): 0.8351826667785645\n",
      "layer4.0: CPD (256, 512, (3, 3)): 0.8577007055282593\n",
      "layer4.0: CPD (512, 512, (3, 3)): 0.9186574816703796\n",
      "layer4.0.downsample: SVD (256, 512, (1, 1)): 0.6602019667625427\n",
      "layer4.1: CPD (512, 512, (3, 3)): 0.8895819187164307\n",
      "layer4.1: CPD (512, 512, (3, 3)): 0.8611284494400024\n",
      "Results saved in ./CPD_conv/41/\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test compressed",
   "id": "533b687ad8a4f43c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compressed_resnet18 = compress_resnet(resnet18, CPD_conv)[0]",
   "id": "4677081c4ceb8565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_tensor = torch.rand(128, 3, 224, 224).to(device)",
   "id": "88f357d39fadf5f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "uncompressed_resnet18 = resnet18(weights='DEFAULT').to(device).eval()\n",
    "compressed_resnet18 = compressed_resnet18.to(device).eval()"
   ],
   "id": "68ac53856c03959f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%timeit -r 10 -n 100\n",
    "with torch.cuda.amp.autocast():\n",
    "    uncompressed_resnet18(test_tensor)"
   ],
   "id": "4b6003e6bcc08782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%timeit -r 10 -n 100\n",
    "with torch.cuda.amp.autocast():\n",
    "    compressed_resnet18(test_tensor)"
   ],
   "id": "4ae3f4e0866051ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stats_uncompressed = FlopCo(uncompressed_resnet18, device=device)\n",
    "stats_compressed = FlopCo(compressed_resnet18, device=device)"
   ],
   "id": "eadf673556ffe5cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"FLOPS\")\n",
    "print(f\"{stats_uncompressed.total_flops/10e6}e6; {stats_compressed.total_flops/10e6}e6\")\n",
    "print(\"\\nParameters\")\n",
    "print(f\"{stats_uncompressed.total_params}; {stats_compressed.total_params}\")"
   ],
   "id": "de5eac3e6c22e03e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dict(sorted(stats_uncompressed.relative_params.items(), key=lambda item: item[1], reverse=True))",
   "id": "d5bc58e30501a63c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dict(sorted(stats_compressed.relative_params.items(), key=lambda item: item[1], reverse=True))",
   "id": "26c8b85881cc6ba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Try to finetune model",
   "id": "fef3dc3c02f6c43c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Instantiate models\n",
    "reference_model = uncompressed_resnet18.train()\n",
    "trainable_model = compressed_resnet18\n",
    "\n",
    "# Loss function: CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for the trainable model\n",
    "optimizer = optim.Adam(trainable_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop without a dataset\n",
    "epochs = 90\n",
    "batch_size = 256\n",
    "input_shape = (3, 224, 224)  # Image-like input (channels, height, width)\n",
    "batches = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    # Use tqdm to track progress\n",
    "    with tqdm(total=batches, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n",
    "        for _ in range(batches):  # Simulate 100 batches per epoch\n",
    "            # Generate random input data (like random images)\n",
    "            inputs = torch.randn(batch_size, *input_shape, device=device)\n",
    "\n",
    "            # Get the reference model's output (detach to avoid backprop on reference model)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    reference_outputs = reference_model(inputs)\n",
    "\n",
    "                # Forward pass of the trainable model\n",
    "                model_outputs = trainable_model(inputs)\n",
    "\n",
    "            # Compute the loss between the two model outputs\n",
    "            # We need to find the class with maximum probability in the reference outputs for CrossEntropy\n",
    "            # Assuming reference outputs are logits (not probabilities)\n",
    "            _, target_indices = torch.max(reference_outputs, dim=1)  # Get class indices for the targets\n",
    "\n",
    "            # Compute the Cross-Entropy loss\n",
    "            loss = criterion(model_outputs, target_indices)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = running_loss / batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "67b62bed14234b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eee1babeb1263956",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
